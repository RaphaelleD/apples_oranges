<!DOCTYPE html> <html lang=en class=js style><!--
 Page saved with SingleFile 
 url: https://www.nature.com/articles/s41599-020-0499-z 
 saved date: Mon Aug 29 2022 09:10:35 GMT+0300 (Eastern European Summer Time)
--><meta charset=utf-8>
<title>Emotion recognition and confidence ratings predicted by vocal stimulus type and prosodic parameters | Humanities and Social Sciences Communications</title>
<meta http-equiv=X-UA-Compatible content="IE=edge">
<meta name=applicable-device content=pc,mobile>
<meta name=viewport content="width=device-width,initial-scale=1.0,maximum-scale=5,user-scalable=yes">
<style>@media only print,only all and (prefers-color-scheme:no-preference),only all and (prefers-color-scheme:light),only all and (prefers-color-scheme:dark){html{text-size-adjust:100%;box-sizing:border-box;font-family:sans-serif;font-size:100%;height:100%;line-height:1.15;overflow-y:scroll}body{color:#222;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;line-height:1.76;min-height:100%}article,aside,header,main,section{display:block}a{vertical-align:baseline}a{background-color:transparent;color:#069;overflow-wrap:break-word;text-decoration:underline;word-break:break-word}b{font-weight:bolder}img{border:0;height:auto;max-width:100%;vertical-align:middle}svg:not(:root){overflow:hidden}button{font-size:100%;line-height:1.15}button{overflow:visible}button{text-transform:none}button{-webkit-appearance:button}button{border-radius:0;cursor:pointer}button,h1{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif}h1{letter-spacing:-.0390625rem}.u-h3,h2{letter-spacing:-.0117156rem}h2{font-size:1.5rem;line-height:1.6rem}.u-h3{font-size:1.25rem;margin-bottom:8px}.u-h3,h2{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-weight:700}.u-h3{line-height:1.4rem}h3{font-size:1.25rem}h3{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-weight:700;letter-spacing:-.0117156rem;line-height:1.4rem}body,button,div,h1,h2,h3{margin:0;padding:0}p{padding:0}body{font-size:1.125rem}p{margin:0 0 28px}ol,ul{margin-bottom:28px;margin-top:0}p:empty{display:none}.article-page{background:#fff}.c-article-header{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;margin-bottom:40px}.c-article-identifiers{color:#6f6f6f;display:flex;flex-wrap:wrap;font-size:1rem;line-height:1.3;list-style:none;margin:0 0 8px;padding:0}.c-article-identifiers__item{border-right:1px solid #6f6f6f;list-style:none;margin-right:8px;padding-right:8px}.c-article-identifiers__item:last-child{border-right:0;margin-right:0;padding-right:0}.c-article-identifiers__open{color:#b74616}.c-article-title{font-size:1.5rem;line-height:1.25;margin-bottom:16px}@media only screen and (min-width:768px){.c-article-title{font-size:1.875rem;line-height:1.2}}.c-article-author-list{display:inline;font-size:1rem;list-style:none;margin:0 8px 0 0;padding:0;width:100%}.c-article-author-list__item{margin-left:0}.c-article-author-list li{display:inline;padding-right:0}.c-article-info-details{font-size:1rem;margin-bottom:8px;margin-top:16px}.c-article-info-details__cite-as{border-left:1px solid #6f6f6f;margin-left:8px;padding-left:8px}.c-article-metrics-bar{display:flex;flex-wrap:wrap;font-size:1rem;line-height:1.3}.c-article-metrics-bar__wrapper{margin:0 0 16px}.c-article-metrics-bar__item{align-items:baseline;border-right:1px solid #6f6f6f;margin-right:8px}.c-article-metrics-bar__item:last-child{border-right:0}.c-article-metrics-bar__count{font-weight:700;margin:0}.c-article-metrics-bar__label{color:#626262;font-style:normal;font-weight:400;margin:0 10px 0 5px}.c-article-metrics-bar__details{margin:0}.c-article-main-column{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;margin-right:8.6%;width:60.2%}@media only screen and (max-width:1023px){.c-article-main-column{margin-right:0;width:100%}}.c-article-extras{float:left;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;width:31.2%}@media only screen and (max-width:1023px){.c-article-extras{display:none}}.c-article-section__title{border-bottom:2px solid #d5d5d5;font-size:1.25rem;line-height:1.3;margin:0;padding-bottom:8px}@media only screen and (min-width:768px){.c-article-section__title{font-size:1.5rem;line-height:1.24}}.c-article-section{clear:both}.c-article-section__content{margin-bottom:40px;padding-top:8px}@media only screen and (max-width:1023px){.c-article-section__content{padding-left:0}}.c-context-bar{box-shadow:0 0 10px 0 rgba(51,51,51,.2);position:relative;width:100%}.c-context-bar__title{display:none}.c-reading-companion{clear:both}.c-reading-companion__sticky{max-width:389px}.c-reading-companion__scroll-pane{margin:0;min-height:200px;overflow:hidden auto}.c-reading-companion__tabs{display:flex;flex-flow:row nowrap;font-size:1rem;list-style:none;margin:0 0 8px;padding:0}.c-reading-companion__tabs>li{flex-grow:1}.c-reading-companion__tab{background-color:#eee;border:1px solid #d5d5d5;border-image:initial;border-left-width:0;color:#069;font-size:1rem;padding:8px 8px 8px 15px;text-align:left;width:100%}.c-reading-companion__tabs li:first-child .c-reading-companion__tab{border-left-width:1px}.c-reading-companion__tab--active{background-color:#fff;border-bottom:1px solid #fff;color:#222;font-weight:700}.c-reading-companion__references-list{list-style:none;padding:0}.c-header__menu--global .c-header__item svg{display:none}.c-reading-companion__reference-item{border-top:1px solid #d5d5d5;font-size:1rem;padding:8px 8px 8px 16px}.c-reading-companion__reference-item:first-child{border-top:none}.c-reading-companion__reference-item a{word-break:break-word}.c-reading-companion__reference-citation{display:inline}.c-reading-companion__reference-links{font-size:.813rem;font-weight:700;list-style:none;margin:8px 0 0;padding:0;text-align:right}.c-reading-companion__reference-links>a{display:inline-block;padding-left:8px}.c-reading-companion__reference-links>a:first-child{display:inline-block;padding-left:0}.c-reading-companion__panel{border-top:none;margin-top:0;padding-top:0}.c-reading-companion__panel--active{display:block}.c-pdf-download__link .u-icon{padding-top:2px}.c-pdf-download{display:flex;margin-bottom:16px;max-height:48px}@media only screen and (min-width:540px){.c-pdf-download{max-height:none}}@media only screen and (min-width:1024px){.c-pdf-download{max-height:48px}}.c-pdf-download__link{flex:1 1 0%;padding:13px 24px!important}.c-pdf-download__link:hover{text-decoration:none}.c-pdf-download__text{padding-right:4px}@media only screen and (max-width:539px){.c-pdf-download__text{text-transform:capitalize}}@media only screen and (min-width:540px){.c-pdf-download__text{padding-right:8px}}.c-context-bar--sticky .c-pdf-download{display:block;margin-bottom:0;white-space:nowrap}@media only screen and (min-width:540px){.c-context-bar--sticky .c-pdf-download__link{align-items:center;flex:1 1 183px}}@media only screen and (max-width:320px){.c-context-bar--sticky .c-pdf-download__link{padding:16px}}p{overflow-wrap:break-word;word-break:break-word}.c-ad{text-align:center}@media only screen and (min-width:320px){.c-ad{padding:8px}}.c-ad--728x90{background-color:#ccc;display:none}.c-ad--728x90 .c-ad__inner{min-height:calc(1.5em + 94px)}@media only screen and (min-width:768px){.js .c-ad--728x90{display:none}}.c-ad__label{color:#333;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:.875rem;font-weight:400;line-height:1.5;margin-bottom:4px}.c-author-list{color:#6f6f6f;font-family:inherit;list-style:none;margin:0;padding:0}.c-author-list>li,.c-breadcrumbs>li,.js .c-author-list{display:inline}.c-author-list>li:not(:first-child):not(:last-child):before{content:", "}.c-author-list>li:not(:only-child):last-child:before{content:" & "}.c-author-list--compact{font-size:.875rem;line-height:1.4}.c-author-list--truncated>li:not(:only-child):last-child:before{content:" ... "}.c-skip-link{background:#069;bottom:auto;color:#fff;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:.875rem;padding:8px;position:absolute;text-align:center;transform:translateY(-100%);z-index:9999}@media (prefers-reduced-motion:reduce){.c-skip-link{transition:top .3s ease-in-out 0s}}.c-skip-link:link{color:#fff}.c-breadcrumbs{color:#000;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1rem;list-style:none;margin:0;padding:0}.c-breadcrumbs__link{color:#666}.c-breadcrumbs__chevron{fill:#888;margin:4px 4px 0}.c-header{background-color:#fff;border-bottom:5px solid #000;font-size:1rem;line-height:1.4;margin-bottom:16px}.c-header__row{padding:8px 0;position:relative}.c-header__row:not(:last-child){border-bottom:1px solid #eee}.c-header__row--flush{padding:0}.c-header__split{align-items:center;display:flex;justify-content:space-between}.c-header__logo-container{flex:1 1 0px;line-height:0;margin-right:24px}.c-header__logo{transform:translateZ(0)}.c-header__logo img{max-height:32px}.c-header__container{margin:0 auto;max-width:1280px;padding:0 16px}.c-header__menu{align-items:center;display:flex;flex:0 1 auto;flex-wrap:wrap;font-weight:700;line-height:1.4;list-style:none;margin:0 -8px;padding:0}.c-header__menu--global{font-weight:400;justify-content:flex-end}@media only screen and (min-width:540px){.c-header__menu--global .c-header__item svg{display:block}}.c-header__menu--journal{font-size:.875rem}@media only screen and (min-width:540px){.c-header__menu--journal{flex-wrap:nowrap;font-size:1rem}}@media only screen and (max-width:1023px){.c-header__menu--tools{display:none}}.c-header__item{display:flex}@media only screen and (min-width:540px){.c-header__item:not(:first-child){margin-left:8px}}.c-header__item--pipe{border-left:2px solid #eee;padding-left:8px}@media only screen and (max-width:767px){.c-header__item--nature-research{display:none}}.c-header__row--flush .c-header__item{padding-bottom:8px;padding-top:8px}.c-header__link{align-items:center;color:inherit;display:flex;padding:8px;white-space:nowrap}.c-header__link>svg{margin-left:2px;transition-duration:.2s}@media only screen and (min-width:540px){.c-header__link>svg{margin-left:4px}}.c-header__show-text{display:none}@media only screen and (min-width:540px){.c-header__show-text{display:inline}.c-header__item--dropdown-menu{position:relative}}.u-button--primary svg{fill:currentcolor}.u-button{align-items:center;border-radius:2px;cursor:pointer;font-family:sans-serif;font-size:1rem;line-height:1.3;margin:0;padding:8px;position:relative;text-decoration:none;transition:all .25s ease 0s,color .25s ease 0s,border-color .25s ease 0s}.u-button--primary{background-color:#069;background-image:none;border:1px solid #069;color:#fff}.u-button--full-width{display:flex;width:100%}.u-hide{display:none;visibility:hidden}.u-visually-hidden{clip:rect(0,0,0,0);border:0;height:1px;margin:-100%;overflow:hidden;padding:0;position:absolute!important;width:1px}@media only screen and (min-width:768px){.u-show-at-md{display:block;visibility:visible}}.u-clearfix:after,.u-clearfix:before{content:"";display:table}.u-clearfix:after{clear:both}.u-float-left{float:left}.u-icon{fill:currentcolor;display:inline-block;height:1em;transform:translate(0);vertical-align:text-top;width:1em}.u-list-reset{list-style:none;margin:0;padding:0}.u-container{margin:0 auto;max-width:1280px;padding:0 16px}.u-justify-content-space-between{justify-content:space-between}.u-ma-0{margin:0}.u-mt-32{margin-top:32px}.u-mr-2{margin-right:2px}.u-mb-16{margin-bottom:16px}.u-mb-32{margin-bottom:32px}html *,html :after,html :before{box-sizing:inherit}.c-article-section__title,.c-article-title{font-weight:700}.c-header__link{text-decoration:inherit}.grade-c-hide{display:block}.u-lazy-ad-wrapper{background-color:#ccc;display:none;min-height:137px}@media only screen and (min-width:768px){.u-lazy-ad-wrapper{display:block}}}</style>
<style media="only print, only all and (prefers-color-scheme: no-preference), only all and (prefers-color-scheme: light), only all and (prefers-color-scheme: dark)">@-webkit-keyframes transparent-to-yellow{0%,to{background:0 0}25%,75%{background:#ff9}}@keyframes transparent-to-yellow{0%,to{background:0 0}25%,75%{background:#ff9}}@-webkit-keyframes highlight{0%,60%{background-color:#ff9}to{background-color:#fff}}@keyframes highlight{0%,60%{background-color:#ff9}to{background-color:#fff}}html{font-family:sans-serif;line-height:1.15;-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%;height:100%;overflow-y:scroll;font-size:100%;box-sizing:border-box}body{min-height:100%;font-family:-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Oxygen-Sans,Ubuntu,Cantarell,"Helvetica Neue",sans-serif;line-height:1.76;color:#222}article,aside,figcaption,figure,footer,header,main,section{display:block}figure{margin:0}a,sub{vertical-align:baseline}a{background-color:transparent;-webkit-text-decoration-skip:objects;color:#069;word-wrap:break-word;overflow-wrap:break-word;word-break:break-word;text-decoration:underline;text-decoration-thickness:.0625rem;text-underline-offset:.08em}a:active,a:hover{outline-width:0}abbr[title]{border-bottom:none;-webkit-text-decoration:underline dotted}b{font-weight:bolder}sub{font-size:75%;line-height:0;position:relative}sub{bottom:-.25em}img{border:0;max-width:100%;height:auto;vertical-align:middle}svg:not(:root){overflow:hidden}button{line-height:1.15}button{overflow:visible}button{text-transform:none}button{-webkit-appearance:button}::-webkit-file-upload-button{-webkit-appearance:button;font:inherit}.c-article-box__controls button:focus,.c-article-box__controls button:hover,.c-pan-zoom-controls button:focus,.c-pan-zoom-controls button:hover,a:focus,a:hover{text-decoration:underline}button{cursor:pointer;font-family:-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Oxygen-Sans,Ubuntu,Cantarell,"Helvetica Neue",sans-serif;border-radius:0}abbr[title]{text-decoration:none}b:focus,dd:focus,div:focus,dl:focus,dt:focus,em:focus,h1:focus,h2:focus,h3:focus,h4:focus,h5:focus,i:focus,li:focus,ol:focus,p:focus,span:focus,strong:focus,ul:focus{outline:0}h1{font-family:-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Oxygen-Sans,Ubuntu,Cantarell,"Helvetica Neue",sans-serif}h1{letter-spacing:min(max(-.011715625rem,4vw),-.0390625rem)}.u-h3,h2{letter-spacing:-.011715625rem}h2{font-size:min(max(1.25rem,3.5vw),1.5rem);line-height:min(max(1.4rem,3.5vw),1.6rem)}.u-h3{font-size:min(max(1.125rem,3vw),1.25rem);margin-bottom:8px}.u-h3,h2{font-weight:700;font-family:-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Oxygen-Sans,Ubuntu,Cantarell,"Helvetica Neue",sans-serif}h3{font-size:min(max(1.125rem,3vw),1.25rem)}h3{font-weight:700;font-family:-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Oxygen-Sans,Ubuntu,Cantarell,"Helvetica Neue",sans-serif;letter-spacing:-.011715625rem;line-height:1.4rem}[contenteditable]:focus,[tabindex="0"]:focus,a:focus,button:focus,input:focus,select:focus{outline:3px solid #fece3e;will-change:transform}body,button,div,h1,h2,h3{margin:0;padding:0}p{padding:0}a:hover{-webkit-text-decoration-skip:ink;text-decoration-skip-ink:auto}a:active,button:active{outline:0}nav ol{list-style-image:none}body{font-size:1.125rem}p{margin:0 0 28px}ol,ul{margin-top:0;margin-bottom:28px}p:empty{display:none}.article-page{background:#fff}.composite-layer{transform:translateZ(0)}.c-article-header{font-family:-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Oxygen-Sans,Ubuntu,Cantarell,"Helvetica Neue",sans-serif;margin-bottom:40px}.c-article-identifiers{list-style:none;font-size:1rem;line-height:1.3;display:flex;flex-wrap:wrap;color:#6f6f6f;padding:0;margin:0 0 8px}.c-article-identifiers__item{border-right:1px solid #6f6f6f;margin-right:8px;padding-right:8px;list-style:none}.c-article-identifiers__item:last-child{margin-right:0;padding-right:0;border-right:0}.c-article-identifiers__open{color:#b74616}.c-article-title{font-size:1.5rem;line-height:1.25;margin-bottom:16px}@media only screen and (min-width:768px){.c-article-title{font-size:1.875rem;line-height:1.2}}.c-article-author-list{font-size:1rem;display:inline;list-style:none;margin:0 8px 0 0;padding:0;width:100%}.c-article-author-list__item{margin-left:0}.c-article-author-list li{display:inline;padding-right:0}.c-article-author-list__item svg{margin-left:4px}.c-article-info-details{font-size:1rem;margin-bottom:8px;margin-top:16px}.c-article-info-details__cite-as{border-left:1px solid #6f6f6f;margin-left:8px;padding-left:8px}.c-article-metrics-bar{display:flex;flex-wrap:wrap;line-height:1.3;font-size:1rem}.c-article-metrics-bar__wrapper{margin:0 0 16px}.c-article-metrics-bar__item{align-items:baseline;border-right:1px solid #6f6f6f;margin-right:8px}.c-article-metrics-bar__item:last-child{border-right:0}.c-article-metrics-bar__count{font-weight:700;margin:0}.c-article-metrics-bar__label{color:#626262;font-weight:400;font-style:normal;margin:0 10px 0 5px}.c-article-metrics-bar__details{margin:0}.c-article-main-column{font-family:-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Oxygen-Sans,Ubuntu,Cantarell,"Helvetica Neue",sans-serif;margin-right:8.6%;width:60.2%}@media only screen and (max-width:1023px){.c-article-main-column{margin-right:0;width:100%}}.c-article-extras{float:left;font-family:-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Oxygen-Sans,Ubuntu,Cantarell,"Helvetica Neue",sans-serif;width:31.2%}@media only screen and (max-width:1023px){.c-article-extras{display:none}}.c-article-author-affiliation__list,.c-article-further-reading__list,.c-article-references{list-style:none;padding:0}.c-article-section__title{border-bottom:2px solid #d5d5d5;font-size:1.25rem;line-height:1.3;padding-bottom:8px;margin:0}@media only screen and (min-width:768px){.c-article-section__title{font-size:1.5rem;line-height:1.24}}.c-article-section{clear:both}.c-article-section__content{padding-top:8px;margin-bottom:40px}@media only screen and (max-width:1023px){.c-article-section__content{padding-left:0}}.c-article__pill-button{padding:8px 16px 8px 20px;background-color:#fff;border:4px solid #bcd2dc;border-radius:20px;font-size:.875rem;line-height:1.4;font-weight:700;margin-bottom:10px;font-family:-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Oxygen-Sans,Ubuntu,Cantarell,"Helvetica Neue",sans-serif;text-decoration:none;display:inline-flex;align-items:center}.c-article__pill-button:hover{background-color:#069;color:#fff;text-decoration:none}.c-article__pill-button:focus{outline:0;box-shadow:0 0 0 3px #fece3e;text-decoration:none}.c-article__pill-button:active,.c-article__pill-button:hover{box-shadow:0 0 0 0}.c-article__pill-button svg{height:.8em;margin-left:2px}.c-article__sub-heading{font-size:1.125rem;font-weight:400;font-family:-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Oxygen-Sans,Ubuntu,Cantarell,"Helvetica Neue",sans-serif;font-style:normal;line-height:1.3;color:#222;margin:0 0 8px}@media only screen and (min-width:768px){.c-article__sub-heading{font-size:1.5rem;line-height:1.24}}.c-article-references__item{display:flex;flex-wrap:wrap;border-bottom:1px solid #d5d5d5;padding-bottom:16px;margin-bottom:16px}.c-article-references__item::before{content:attr(data-counter);font-size:1.5rem;line-height:1.4;text-align:right}@media only screen and (max-width:1023px){.c-article-references__item::before{font-size:1.125rem;line-height:inherit}}.c-article-references__item>p.c-article-references__text{flex:1;padding-left:8px}.c-article-references__text{margin-bottom:8px}.c-article-references__links{display:flex;flex-basis:100%;justify-content:flex-end;flex-wrap:wrap;font-family:-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Oxygen-Sans,Ubuntu,Cantarell,"Helvetica Neue",sans-serif;font-size:1rem;font-weight:700;margin:0;padding:0;list-style:none}.c-article-references__links>a{padding-left:8px}.c-article-references__links>a:first-child{padding-left:0}.c-article-references__download{text-align:right}.c-article-references__download>a{font-size:1rem;font-weight:700;font-family:-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Oxygen-Sans,Ubuntu,Cantarell,"Helvetica Neue",sans-serif}.c-article-references__download svg,.c-bibliographic-information__download-citation svg{margin-left:4px}.c-article-author-affiliation__address{color:inherit;font-weight:700;margin:0}.c-article-author-affiliation__authors-list{padding:0;list-style:none;margin-bottom:16px;margin-top:0}.c-article-supplementary__item{margin-bottom:24px;position:relative}.c-article-supplementary__title{margin:0 0 8px;line-height:1.5}.c-article-further-reading__list{margin:0}.c-article-further-reading__item{border-bottom:1px solid #d5d5d5;margin-bottom:16px;position:relative}.c-article-further-reading__item:last-child{border:0}.c-article-further-reading__title{margin:0;padding:0;line-height:1.5}.c-article-further-reading__journal-title{color:#626262;font-size:.813rem;padding:0;font-family:-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Oxygen-Sans,Ubuntu,Cantarell,"Helvetica Neue",sans-serif}.c-article-further-reading__journal-title{margin-bottom:24px}.c-article-subject-list{font-size:1rem;font-family:-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Oxygen-Sans,Ubuntu,Cantarell,"Helvetica Neue",sans-serif;list-style:none;display:flex;flex-wrap:wrap;padding:0;margin:0 0 24px}.c-article-subject-list__subject{background-color:#dae5ea;border-radius:20px;padding:4px 10px;font-weight:700;margin-right:15px;margin-bottom:16px;flex:0 1 auto}.c-bibliographic-information{display:flex;padding-top:8px}@media only screen and (max-width:1023px){.c-bibliographic-information{padding-top:0;display:block}}.c-bibliographic-information__value{font-size:1rem;display:block}.c-bibliographic-information__column:first-child{width:81px;flex:0 0 81px}@media only screen and (max-width:1023px){.c-bibliographic-information__column:first-child{width:100%}}.c-bibliographic-information__column:last-child{flex:1}.c-bibliographic-information__column--border{border-right:1px solid #d5d5d5;margin-right:24px}@media only screen and (max-width:1023px){.c-bibliographic-information__column--border{border-bottom:1px solid #d5d5d5;padding-bottom:8px;margin-bottom:8px;border-right:0;margin-right:0}}.c-bibliographic-information__list{list-style:none;margin:0;padding:0;display:flex;justify-content:flex-start;flex-wrap:wrap}.c-bibliographic-information__list-item{flex:0 0 33%;box-sizing:border-box;padding-right:8px;margin-bottom:16px;font-size:1rem;font-family:-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Oxygen-Sans,Ubuntu,Cantarell,"Helvetica Neue",sans-serif}@media only screen and (max-width:1023px){.c-bibliographic-information__list-item{flex:0 0 100%}}.c-bibliographic-information__list-item p{margin-bottom:0}.c-bibliographic-information__list-item:last-child{padding-right:0}.c-bibliographic-information__list-item--doi{flex:0 0 100%}.c-bibliographic-information__citation,.c-bibliographic-information__download-citation{font-family:-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Oxygen-Sans,Ubuntu,Cantarell,"Helvetica Neue",sans-serif;font-size:1rem;margin-bottom:8px}.c-bibliographic-information__download-citation{margin-bottom:24px}.c-article-share-box{font-family:-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Oxygen-Sans,Ubuntu,Cantarell,"Helvetica Neue",sans-serif}.c-article-share-box__description{font-size:1rem;font-family:-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Oxygen-Sans,Ubuntu,Cantarell,"Helvetica Neue",sans-serif;margin-bottom:8px}.c-article-share-box__additional-info{color:#626262;font-size:.813rem;margin-bottom:24px}.c-article-share-box__button{background:#fff;border:1px solid #069;box-sizing:content-box;color:#069;font-size:1rem;line-height:1.5;margin-bottom:8px;padding:8px 20px;text-align:center}.c-context-bar{box-shadow:0 0 10px 0 rgba(51,51,51,.2)}.c-context-bar--sticky{position:fixed;z-index:100;top:0;left:0;width:100%}@media only screen and (max-width:320px){.c-context-bar--sticky{left:auto;right:0}}.c-context-bar--sticky:after{background:#fff;display:block;position:absolute;width:100%;height:100%;top:0;left:0;content:"";z-index:-1}@supports ((-webkit-backdrop-filter:blur(10px)) or (backdrop-filter:blur(10px))){.c-context-bar--sticky:after{background:linear-gradient(to top,rgba(255,255,255,.75) 50%,#fff);-webkit-backdrop-filter:blur(10px);backdrop-filter:blur(10px)}}@media only screen and (max-width:320px){.c-context-bar--sticky:after{background-color:#fff;background-image:none;box-shadow:none}}.c-context-bar--sticky .c-context-bar__container{padding:16px}@media only screen and (min-width:540px){.c-context-bar--sticky .c-context-bar__container{display:flex;align-items:center;justify-content:space-between;margin:0 auto;min-height:40px}}.c-context-bar--sticky .c-context-bar__title{display:none}@media only screen and (min-width:540px){.c-context-bar--sticky .c-context-bar__title{display:block;flex:1;padding-right:16px;overflow:hidden;white-space:nowrap;text-overflow:ellipsis}}.c-reading-companion{clear:both}.c-reading-companion__sticky{max-width:389px}.c-reading-companion__sticky--stuck{margin:0 0 16px;position:fixed}.c-reading-companion__scroll-pane{overflow-x:hidden;overflow-y:auto;margin:0;min-height:200px}.c-reading-companion__tabs{font-size:1rem;list-style:none;display:flex;flex-direction:row;flex-wrap:nowrap;margin:0 0 8px;padding:0}.c-reading-companion__tabs>li{flex-grow:1}.c-reading-companion__tab{color:#069;border:solid #d5d5d5;border-width:1px 1px 1px 0;background-color:#eee;padding:8px 8px 8px 15px;text-align:left;font-size:1rem;width:100%}.c-reading-companion__tab:focus{text-decoration:none;outline-offset:-4px}.c-reading-companion__tabs li:first-child .c-reading-companion__tab{border-left-width:1px}.c-reading-companion__tab--active{color:#222;background-color:#fff;border-bottom:1px solid #fff;font-weight:700}.c-reading-companion__references-list{list-style:none;padding:0}.c-header__menu--global .c-header__item svg{display:none}.c-reading-companion__reference-item{padding:8px 8px 8px 16px;border-top:1px solid #d5d5d5;font-size:1rem}.c-button-dropdown__container ul li:first-child,.c-reading-companion__reference-item:first-child{border-top:none}.c-reading-companion__reference-item a{word-break:break-word}.c-reading-companion__reference-citation{display:inline}.c-reading-companion__reference-links{list-style:none;text-align:right;margin:8px 0 0;padding:0;font-weight:700;font-size:.813rem}.c-reading-companion__reference-links>a{display:inline-block;padding-left:8px}.c-reading-companion__reference-links>a:first-child{display:inline-block;padding-left:0}.c-reading-companion__panel{border-top:none;margin-top:0;padding-top:0}.c-reading-companion__panel--active{display:block}.c-reading-companion__return{position:absolute;overflow:hidden;width:1px;height:1px}.c-reading-companion__return:focus{position:static;width:auto;height:auto;padding:0 4px}.c-article-section__figure{border:2px solid #d5d5d5;padding:20px 10px;max-width:100%;margin-bottom:24px;clear:both}.c-article-section__figure-caption{margin-bottom:8px;display:block;word-break:break-word}.c-article-section__figure-content{margin-bottom:16px}.c-article-section__figure-item{max-width:100%}.c-article-section__figure-link{max-width:100%;display:block;margin:0 0 16px;padding:0}.c-article-section__figure-item img{display:block;margin:0 auto}.c-article-section__figure-description{font-size:1rem}.c-article-section__figure-description>*{margin-bottom:0}.c-article-table{border:2px solid #d5d5d5;clear:both;padding:20px 10px;margin-bottom:24px}.c-article-table__figcaption{line-height:1.4;margin-bottom:16px;word-break:break-word}.c-pdf-download__link .u-icon{padding-top:2px}.c-pdf-download{display:flex;margin-bottom:16px;max-height:48px}@media only screen and (min-width:540px){.c-pdf-download{max-height:none}}@media only screen and (min-width:1024px){.c-pdf-download{max-height:48px}}.c-pdf-download__link{flex:1;padding:13px 24px!important}.c-pdf-download__link:hover{text-decoration:none}.c-pdf-download__text{padding-right:4px}@media only screen and (max-width:539px){.c-pdf-download__text{text-transform:capitalize}}@media only screen and (min-width:540px){.c-pdf-download__text{padding-right:8px}}.c-context-bar--sticky .c-pdf-download{margin-bottom:0;display:block;white-space:nowrap}@media only screen and (min-width:540px){.c-context-bar--sticky .c-pdf-download__link{flex:1 1 183px;align-items:center}}@media only screen and (max-width:320px){.c-context-bar--sticky .c-pdf-download__link{padding:16px}}@supports (display:grid) and (grid-auto-rows:auto){.c-article-metrics__legend ul{display:grid;grid-column-gap:8px;grid-auto-rows:auto;grid-template-columns:repeat(auto-fill,minmax(120px,1fr))}@media only screen and (min-width:540px){.c-article-metrics__legend ul{padding-left:16px;grid-template-columns:repeat(2,50%)}}@media only screen and (min-width:1024px){.c-article-metrics__legend ul{grid-template-columns:repeat(3,33%)}}.c-article-metrics__legend ul li{font-size:.875rem}}a:hover{text-decoration:underline;text-decoration-thickness:.3875rem;text-underline-offset:.08em}p{word-wrap:break-word;overflow-wrap:break-word;word-break:break-word}.c-ad{text-align:center}@media only screen and (min-width:320px){.c-ad{padding:8px}}.c-ad--728x90{display:none;background-color:#ccc}.c-ad--728x90 .c-ad__inner{min-height:calc(1.5em + 4px + 90px)}@media only screen and (min-width:768px){.js .u-show-following-ad+.c-ad--728x90{display:block}}.c-ad__label{font-family:-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Oxygen-Sans,Ubuntu,Cantarell,"Helvetica Neue",sans-serif}.c-ad__label{font-size:.875rem;font-weight:400;margin-bottom:4px;color:#333;line-height:1.5}.c-author-list{list-style:none;margin:0;padding:0;font-family:inherit;color:#6f6f6f}.c-author-list>li,.c-breadcrumbs>li,.js .c-author-list{display:inline}.c-author-list>li:not(:first-child):not(:last-child)::before{content:", "}.c-author-list>li:not(:only-child):last-child::before{content:" & "}.c-author-list--compact{line-height:1.4;font-size:.875rem}.c-author-list--truncated>li:not(:only-child):last-child::before{content:" ... "}.c-meta{list-style:none;padding:0;font-family:-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Oxygen-Sans,Ubuntu,Cantarell,"Helvetica Neue",sans-serif;font-size:.875rem;color:inherit;line-height:1.4}.c-meta__item{display:inline-block;margin-bottom:4px}.c-meta__item:not(:last-child){border-right:1px solid #d5d5d5;padding-right:4px;margin-right:4px}.c-skip-link{font-family:-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Oxygen-Sans,Ubuntu,Cantarell,"Helvetica Neue",sans-serif;position:absolute}.c-skip-link{background:#069;color:#fff;font-size:.875rem;text-align:center;padding:8px;inset:0;bottom:auto;z-index:9999;transform:translateY(-100%)}@media (prefers-reduced-motion:reduce){.c-skip-link{transition:top .3s ease-in-out}}.c-skip-link:active,.c-skip-link:hover,.c-skip-link:link,.c-skip-link:visited{color:#fff}.c-skip-link:focus{transform:translateY(0)}.c-breadcrumbs{list-style:none;margin:0;padding:0;font-size:1rem;font-family:-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Oxygen-Sans,Ubuntu,Cantarell,"Helvetica Neue",sans-serif;color:#000}.c-breadcrumbs__link,.c-breadcrumbs__link:hover,.c-breadcrumbs__link:visited{color:#666}.c-breadcrumbs__chevron{margin:4px 4px 0;fill:#888}@supports (aspect-ratio:1/1){.c-card__image{padding-bottom:0;aspect-ratio:var(--card--image-aspect-ratio, 16/9)}}@supports ((-o-object-fit:cover) or (object-fit:cover)){.c-card__image img{width:100%;height:100%;-o-object-fit:cover;object-fit:cover}}.c-corporate-footer{background-color:#222;border-top:none;font-family:-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Oxygen-Sans,Ubuntu,Cantarell,"Helvetica Neue",sans-serif;padding-top:16px;padding-bottom:24px}.c-corporate-footer__legal{color:#a2a2a2;font-size:.813rem;margin-bottom:0;padding-top:4px}.c-menu{list-style:none;margin:0;padding:0;font-size:1rem;font-family:-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Oxygen-Sans,Ubuntu,Cantarell,"Helvetica Neue",sans-serif;display:flex;line-height:1.3}.c-menu__item:not(:last-child){margin-right:16px}.c-menu__link{display:inline-block}.c-menu__link:active,.c-menu__link:visited{color:inherit}.c-menu__link:hover{text-decoration:underline}.c-menu--inherit .c-menu__link,.c-menu--inherit .c-menu__link:hover,.c-menu--inherit .c-menu__link:visited{color:inherit}.c-header{background-color:#fff;border-bottom:5px solid #000;font-size:1rem;margin-bottom:16px;line-height:1.4}.c-header__row{position:relative;padding:8px 0}.c-header__row:not(:last-child){border-bottom:1px solid #eee}.c-header__row--flush{padding:0}.c-header__split{display:flex;align-items:center;justify-content:space-between}.c-header__logo-container{flex:1 1 0;margin-right:24px;line-height:0}.c-header__logo{transform:translateZ(0)}.c-header__logo img{max-height:32px}.c-header__container{margin:0 auto;max-width:1280px;padding:0 16px}.c-header__menu{list-style:none;padding:0;display:flex;align-items:center;flex-wrap:wrap;flex:0 1 auto;font-weight:700;line-height:1.4;margin:0 -8px}.c-header__menu--global{font-weight:400;justify-content:flex-end}@media only screen and (min-width:540px){.c-header__menu--global .c-header__item svg{display:block}}.c-header__menu--journal{font-size:.875rem}@media only screen and (min-width:540px){.c-header__menu--journal{flex-wrap:nowrap;font-size:1rem}}@media only screen and (max-width:1023px){.c-header__menu--tools{display:none}}.c-header__item{display:flex}@media only screen and (min-width:540px){.c-header__item:not(:first-child){margin-left:8px}}.c-header__item--pipe{border-left:2px solid #eee;padding-left:8px}@media only screen and (max-width:767px){.c-header__item--nature-research{display:none}}.c-header__row--flush .c-header__item{padding-top:8px;padding-bottom:8px}.c-header__link{color:inherit;padding:8px;display:flex;align-items:center;white-space:nowrap}.c-header-expander a,.c-header-expander a.hover,.c-header-expander a.visited,.c-header-expander a:hover,.c-header-expander a:visited,.c-header__link.hover,.c-header__link.visited,.c-header__link:hover,.c-header__link:visited{color:inherit}.c-header__link ::first-letter{text-transform:capitalize}.c-header__link>svg{margin-left:2px;transition-duration:.2s}@media only screen and (min-width:540px){.c-header__link>svg{margin-left:4px}}.c-header__show-text{display:none}@media only screen and (min-width:540px){.c-header__show-text{display:inline}}@media only screen and (min-width:540px){.c-header__item--dropdown-menu{position:relative}}.c-footer a,.c-footer a:hover,.c-footer a:visited{color:inherit}.c-footer{background-color:#2f2f2f;color:#d5d5d5;line-height:1.4;padding:24px 0;font-size:.875rem}.c-footer a{vertical-align:middle}.c-footer__header{margin-bottom:24px;padding-bottom:24px;border-bottom:1px solid #555}@media only screen and (min-width:768px){.c-footer__header{display:flex;align-items:center}}@media only screen and (max-width:767px){.c-footer__header>:not(:last-child){margin-bottom:16px}}@media only screen and (min-width:768px){.c-footer__logo{flex:1}}.c-footer__list{list-style:none;margin:0;padding:0}.c-footer__grid{display:flex;flex-wrap:wrap}@supports (display:grid){.c-footer__grid{display:grid;grid-template-columns:repeat(auto-fit,minmax(250px,1fr));grid-column-gap:16px;grid-row-gap:32px}}.c-footer__group{flex:1 1 50%;max-width:50%;padding-right:16px;margin-bottom:16px}@media only screen and (min-width:1024px){.c-footer__group{flex-basis:25%;max-width:25%}}@supports (display:grid){.c-footer__group{padding-right:0;max-width:none;margin-bottom:0}}.c-footer__heading{color:#eee;margin-bottom:16px;font-weight:700;font-family:-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Oxygen-Sans,Ubuntu,Cantarell,"Helvetica Neue",sans-serif}.c-footer__item:not(:last-child){margin-bottom:4px}.u-button--primary svg{fill:currentColor}.u-button{text-decoration:none}.u-button{align-items:center;cursor:pointer;margin:0;position:relative;font-family:sans-serif;font-size:1rem;line-height:1.3;padding:8px;transition:.25s ease,color .25s ease,border-color .25s ease;border-radius:2px}.u-button,.u-button:visited{color:#069}.u-button,.u-button:hover{border:1px solid #069}.u-button:focus{border:1px solid #069}.u-button:focus,.u-button:hover{color:#fff;background-color:#069;background-image:none}.u-button--primary{color:#fff;background-color:#069;background-image:none}.u-button--primary:visited{color:#fff}.u-button--primary,.u-button--primary:hover{border:1px solid #069}.u-button--primary:focus{border:1px solid #069}.u-button--primary:focus,.u-button--primary:hover{color:#069;background-color:#fff;background-image:none}.u-button--disabled,.u-button:disabled{color:#222;background-color:transparent;background-image:none;border:1px solid #eee;opacity:.7;cursor:default}.u-button--disabled svg,.u-button:disabled svg{fill:currentColor}.u-button--disabled:visited,.u-button:disabled:visited{color:#222}.u-button--disabled:hover,.u-button:disabled:hover{border:1px solid #eee;text-decoration:none}.u-button--disabled:focus,.u-button:disabled:focus{border:1px solid #eee;text-decoration:none}.u-button--disabled:focus,.u-button--disabled:hover,.u-button:disabled:focus,.u-button:disabled:hover{color:#222;background-color:transparent;background-image:none}.u-button--full-width{display:flex;width:100%}.u-hide{display:none;visibility:hidden}.u-visually-hidden{border:0;clip:rect(0,0,0,0);height:1px;margin:-100%;overflow:hidden;padding:0;position:absolute!important;width:1px}@media only screen and (min-width:768px){.u-show-at-md{display:block;visibility:visible}}.u-clearfix::after,.u-clearfix::before{content:"";display:table}.u-clearfix::after{clear:both}.u-float-left{float:left}.u-icon{fill:currentColor;transform:translate(0,0);display:inline-block;vertical-align:text-top;width:1em;height:1em}.u-list-reset{list-style:none;margin:0;padding:0}.u-text-right{text-align:right}.u-container{margin:0 auto;max-width:1280px;padding:0 16px}.u-display-inline{display:inline}.u-display-flex{display:flex;width:100%}.u-flex-wrap{flex-wrap:wrap}.u-justify-content-space-between{justify-content:space-between}.u-flex-shrink{flex:0 1 auto}.u-ma-0{margin:0}.u-mt-16{margin-top:16px}.u-mt-32{margin-top:32px}.u-mt-auto{margin-top:auto}.u-mr-2{margin-right:2px}.u-mr-32{margin-right:32px}.u-mb-4{margin-bottom:4px}.u-mb-16{margin-bottom:16px}.u-mb-32{margin-bottom:32px}html *,html ::after,html ::before{box-sizing:inherit}.c-article-section__title,.c-article-title{font-weight:700}.c-footer a,.c-header__link{text-decoration:inherit}.c-footer a:hover,.c-header__link:hover,.c-site-messages .c-site-messages__close:hover,.nature-briefing-banner__checkbox-label a:hover{text-decoration:underline}.grade-c-hide{display:block}.u-lazy-ad-wrapper{min-height:137px;display:none;background-color:#ccc}@media only screen and (min-width:768px){.u-lazy-ad-wrapper{display:block}}</style>
<noscript>
        <link rel="stylesheet" type="text/css" href="/static/css/enhanced-article-eb15d37f34.css" media="only print, only all and (prefers-color-scheme: no-preference), only all and (prefers-color-scheme: light), only all and (prefers-color-scheme: dark)">
    </noscript>
<meta name=msapplication-TileColor content=#000000>
<meta name=msapplication-config content=/static/browserconfig.xml>
<meta name=theme-color content=#000000>
<meta name=application-name content=Nature>
<meta name=robots content=noarchive>
<meta name=access content=Yes>
<link rel=search href=http://www.nature.com/search>
<link rel=search href=http://www.nature.com/opensearch/opensearch.xml type=application/opensearchdescription+xml title=nature.com>
<link rel=search href=http://www.nature.com/opensearch/request type=application/sru+xml title=nature.com>
<script type=application/ld+json>{"mainEntity":{"headline":"Emotion recognition and confidence ratings predicted by vocal stimulus type and prosodic parameters","description":"Human speech expresses emotional meaning not only through semantics, but also through certain attributes of the voice, such as pitch or loudness. In investigations of vocal emotion recognition, there is considerable variability in the types of stimuli and procedures used to examine their influence on emotion recognition. In addition, accurate metacognition was argued to promote correct and confident interpretations in emotion recognition tasks. Nevertheless, such associations have rarely been studied previously. We addressed this gap by examining the impact of vocal stimulus type and prosodic speech attributes on emotion recognition and a person’s confidence in a given response. We analysed a total of 1038 emotional expressions according to a baseline set of 13 prosodic acoustic parameters. Results showed that these parameters provided sufficient discrimination between expressions of emotional categories to permit accurate statistical classification. Emotion recognition and confidence judgments were found to depend on stimulus material as they could be reliably predicted by different constellations of acoustic features. Finally, results indicated that listeners’ accuracy and confidence judgements were significantly higher for affect bursts than speech-embedded stimuli and that the correct classification of emotional expressions elicited increased confidence judgements. Together, these findings show that vocal stimulus type and prosodic attributes of speech strongly influence emotion recognition and listeners’ confidence in these given responses.","datePublished":"2020-06-17","dateModified":"2020-06-17","pageStart":"1","pageEnd":"17","license":"http://creativecommons.org/licenses/by/4.0/","sameAs":"https://doi.org/10.1057/s41599-020-0499-z","keywords":"Language and linguistics,Psychology,Science,Humanities and Social Sciences,multidisciplinary","image":"https://static-content.springer.com/image/art%3A10.1057%2Fs41599-020-0499-z/MediaObjects/41599_2020_499_Fig1_HTML.png","isPartOf":{"name":"Humanities and Social Sciences Communications","issn":["2662-9992"],"volumeNumber":"7","@type":["Periodical","PublicationVolume"]},"publisher":{"name":"Palgrave Macmillan UK","logo":{"url":"https://www.springernature.com/app-sn/public/images/logo-springernature.png","@type":"ImageObject"},"@type":"Organization"},"author":[{"name":"Lausen, Adi","url":"http://orcid.org/0000-0002-4788-9084","affiliation":[{"name":"University of Goettingen","address":{"name":"Department of Affective Neuroscience and Psychophysiology, Institute for Psychology, University of Goettingen, Goettingen, Germany","@type":"PostalAddress"},"@type":"Organization"},{"name":"University of Essex","address":{"name":"Department of Mathematical Sciences, University of Essex, Colchester, UK","@type":"PostalAddress"},"@type":"Organization"}],"email":"adi.lausen@psych.uni-goettingen.de","@type":"Person"},{"name":"Hammerschmidt, Kurt","url":"http://orcid.org/0000-0002-3430-2993","affiliation":[{"name":"University of Goettingen","address":{"name":"Cognitive Ethology Laboratory, German Primate Center, University of Goettingen, Goettingen, Germany","@type":"PostalAddress"},"@type":"Organization"},{"name":"Leibniz ScienceCampus “Primate Cognition”","address":{"name":"Leibniz ScienceCampus “Primate Cognition”, Goettingen, Germany","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"}],"isAccessibleForFree":true,"@type":"ScholarlyArticle"},"@context":"https://schema.org","@type":"WebPage"}</script>
<link rel=canonical href=https://www.nature.com/articles/s41599-020-0499-z>
<meta name=journal_id content=41599>
<meta name=dc.title content="Emotion recognition and confidence ratings predicted by vocal stimulus type and prosodic parameters">
<meta name=dc.source content="Humanities and Social Sciences Communications 2020 7:1">
<meta name=dc.format content=text/html>
<meta name=dc.publisher content=Palgrave>
<meta name=dc.date content=2020-06-17>
<meta name=dc.type content=OriginalPaper>
<meta name=dc.language content=En>
<meta name=dc.copyright content="2020 The Author(s)">
<meta name=dc.rights content="2020 The Author(s)">
<meta name=dc.rightsAgent content=journalpermissions@springernature.com>
<meta name=dc.description content="Human speech expresses emotional meaning not only through semantics, but also through certain attributes of the voice, such as pitch or loudness. In investigations of vocal emotion recognition, there is considerable variability in the types of stimuli and procedures used to examine their influence on emotion recognition. In addition, accurate metacognition was argued to promote correct and confident interpretations in emotion recognition tasks. Nevertheless, such associations have rarely been studied previously. We addressed this gap by examining the impact of vocal stimulus type and prosodic speech attributes on emotion recognition and a person’s confidence in a given response. We analysed a total of 1038 emotional expressions according to a baseline set of 13 prosodic acoustic parameters. Results showed that these parameters provided sufficient discrimination between expressions of emotional categories to permit accurate statistical classification. Emotion recognition and confidence judgments were found to depend on stimulus material as they could be reliably predicted by different constellations of acoustic features. Finally, results indicated that listeners’ accuracy and confidence judgements were significantly higher for affect bursts than speech-embedded stimuli and that the correct classification of emotional expressions elicited increased confidence judgements. Together, these findings show that vocal stimulus type and prosodic attributes of speech strongly influence emotion recognition and listeners’ confidence in these given responses.">
<meta name=prism.issn content=2662-9992>
<meta name=prism.publicationName content="Humanities and Social Sciences Communications">
<meta name=prism.publicationDate content=2020-06-17>
<meta name=prism.volume content=7>
<meta name=prism.number content=1>
<meta name=prism.section content=OriginalPaper>
<meta name=prism.startingPage content=1>
<meta name=prism.endingPage content=17>
<meta name=prism.copyright content="2020 The Author(s)">
<meta name=prism.rightsAgent content=journalpermissions@springernature.com>
<meta name=prism.url content=https://www.nature.com/articles/s41599-020-0499-z>
<meta name=prism.doi content=doi:10.1057/s41599-020-0499-z>
<meta name=citation_pdf_url content=https://www.nature.com/articles/s41599-020-0499-z.pdf>
<meta name=citation_fulltext_html_url content=https://www.nature.com/articles/s41599-020-0499-z>
<meta name=citation_journal_title content="Humanities and Social Sciences Communications">
<meta name=citation_journal_abbrev content="Humanit Soc Sci Commun">
<meta name=citation_publisher content=Palgrave>
<meta name=citation_issn content=2662-9992>
<meta name=citation_title content="Emotion recognition and confidence ratings predicted by vocal stimulus type and prosodic parameters">
<meta name=citation_volume content=7>
<meta name=citation_issue content=1>
<meta name=citation_online_date content=2020/06/17>
<meta name=citation_firstpage content=1>
<meta name=citation_lastpage content=17>
<meta name=citation_article_type content=Article>
<meta name=citation_fulltext_world_readable content>
<meta name=citation_language content=en>
<meta name=dc.identifier content=doi:10.1057/s41599-020-0499-z>
<meta name=DOI content=10.1057/s41599-020-0499-z>
<meta name=size content=453159>
<meta name=citation_doi content=10.1057/s41599-020-0499-z>
<meta name=citation_springer_api_url content="http://api.springer.com/xmldata/jats?q=doi:10.1057/s41599-020-0499-z&amp;api_key=">
<meta name=description content="Human speech expresses emotional meaning not only through semantics, but also through certain attributes of the voice, such as pitch or loudness. In investigations of vocal emotion recognition, there is considerable variability in the types of stimuli and procedures used to examine their influence on emotion recognition. In addition, accurate metacognition was argued to promote correct and confident interpretations in emotion recognition tasks. Nevertheless, such associations have rarely been studied previously. We addressed this gap by examining the impact of vocal stimulus type and prosodic speech attributes on emotion recognition and a person’s confidence in a given response. We analysed a total of 1038 emotional expressions according to a baseline set of 13 prosodic acoustic parameters. Results showed that these parameters provided sufficient discrimination between expressions of emotional categories to permit accurate statistical classification. Emotion recognition and confidence judgments were found to depend on stimulus material as they could be reliably predicted by different constellations of acoustic features. Finally, results indicated that listeners’ accuracy and confidence judgements were significantly higher for affect bursts than speech-embedded stimuli and that the correct classification of emotional expressions elicited increased confidence judgements. Together, these findings show that vocal stimulus type and prosodic attributes of speech strongly influence emotion recognition and listeners’ confidence in these given responses.">
<meta name=dc.creator content="Lausen, Adi">
<meta name=dc.creator content="Hammerschmidt, Kurt">
<meta name=dc.subject content="Language and linguistics">
<meta name=dc.subject content=Psychology>
<meta name=citation_reference content="citation_journal_title=Eur J Soc Psychol; citation_title=Thinking about thinking: causal, evaluative and finalistic cognitions about social situations; citation_author=A Abele; citation_volume=15; citation_publication_date=1985; citation_pages=315-332; citation_doi=10.1002/ejsp.2420150306; citation_id=CR1">
<meta name=citation_reference content="citation_journal_title=Q J Exp Psychol; citation_title=Perceptual and acoustic differences between authentic and acted nonverbal emotional vocalizations; citation_author=A Anikin, CF Lima; citation_volume=71; citation_publication_date=2018; citation_pages=622-641; citation_doi=10.1080/17470218.2016.1270976; citation_id=CR2">
<meta name=citation_reference content="Bąk HK (2016) The state of emotional prosody research—a meta-analysis. In: Bąk HK (ed) Emotional prosody processing for non-native English speakers, 1st edn. Springer International Publishing, pp. 79–112">
<meta name=citation_reference content="citation_journal_title=J Pers Soc Psychol; citation_title=Acoustic profiles in vocal emotion expression; citation_author=R Banse, KR Scherer; citation_volume=70; citation_publication_date=1996; citation_pages=614-636; citation_doi=10.1037/0022-3514.70.3.614; citation_id=CR4">
<meta name=citation_reference content="citation_journal_title=Rev Gen Psychol; citation_title=Bad is stronger than good; citation_author=RF Baumeister, E Bratslavsky, C Finkenauer; citation_volume=5; citation_publication_date=2001; citation_pages=323-370; citation_doi=10.1037//1089-2680.5.4.323; citation_id=CR5">
<meta name=citation_reference content="citation_journal_title=Speech Commun; citation_title=The role of intonation in emotional expressions; citation_author=T Baenziger, KR Scherer; citation_volume=46; citation_publication_date=2005; citation_pages=252-267; citation_doi=10.1016/j.specom.2005.02.016; citation_id=CR6">
<meta name=citation_reference content="citation_journal_title=Soc Cogn Affect Neurosci; citation_title=Confidence of emotion expression recognition recruits brain regions outside the face perception network; citation_author=I Bègue, M Vaessen, J Hofmeister; citation_volume=4; citation_publication_date=2019; citation_pages=81-95; citation_doi=10.1093/scan/nsy102; citation_id=CR7">
<meta name=citation_reference content="citation_journal_title=Behav Res Methods; citation_title=The Montreal affective voices: a validated set of nonverbal affect bursts for research on auditory affective processing; citation_author=P Belin, S Fillion-Bilodeau, F Gosselin; citation_volume=40; citation_publication_date=2008; citation_pages=531-539; citation_doi=10.3758/BRM.40.2.531; citation_id=CR8">
<meta name=citation_reference content="citation_journal_title=J Speech Lang Hear Res; citation_title=Prosody and semantics are separate but not separable channels in the perception of emotional speech: test for rating of emotions in speech; citation_author=BM Ben-David, N Multani, V Shakuf; citation_volume=59; citation_publication_date=2016; citation_pages=1-18; citation_doi=10.1044/2015_jslhr-h-14-0323; citation_id=CR9">
<meta name=citation_reference content="citation_journal_title=Psychophysiology; citation_title=Recognition of affective prosody: continuous wavelet measures of event-related brain potentials to emotional exclamations; citation_author=V Bostanov, B Kotchoubey; citation_volume=41; citation_publication_date=2004; citation_pages=259-268; citation_doi=10.1111/j.1469-8986.2003.00142.x; citation_id=CR10">
<meta name=citation_reference content="citation_journal_title=Mach Learn; citation_title=Random forests; citation_author=L Breiman; citation_volume=45; citation_publication_date=2001; citation_pages=5-32; citation_doi=10.1023/A:1010933404324; citation_id=CR11">
<meta name=citation_reference content="Burkhardt F, Paeschke A, Rolfes M et al. (2005) A database of German emotional speech. In: European conference on speech and language processing, Lisbon, Portugal, pp. 1517–1520. 
                  https://www.researchgate.net/publication/221491017_A_database_of_German_emotional_speech
                  
                . Accessed 10 Nov 2015">
<meta name=citation_reference content="citation_journal_title=Vis Res; citation_title=A principal component analysis of facial expressions; citation_author=AJ Calder, AM Burton, P Miller; citation_volume=41; citation_publication_date=2001; citation_pages=1179-1208; citation_doi=10.1016/S0042-6989(01)00002-5; citation_id=CR14">
<meta name=citation_reference content="citation_journal_title=Behav Res Methods; citation_title=Recognizing emotions in spoken language: a validated set of Portuguese sentences and pseudosentences for research on emotional prosody; citation_author=SL Castro, CF Lima; citation_volume=42; citation_publication_date=2010; citation_pages=74-81; citation_doi=10.3758/BRM.42.1.74; citation_id=CR15">
<meta name=citation_reference content="citation_journal_title=Sci Rep.; citation_title=The development of cross-cultural recognition of vocal emotions during childhood and adolescence; citation_author=G Chronaki, M Wigelsworth, MD Pell; citation_volume=8; citation_publication_date=2018; citation_doi=10.1038/s41598-018-26889-1; citation_id=CR16">
<meta name=citation_reference content="citation_journal_title=Emotion; citation_title=The voice conveys emotion in ten globalized cultures and one remote village in Bhutan; citation_author=DT Cordaro, D Keltner, S Tshering; citation_volume=16; citation_publication_date=2016; citation_pages=117-128; citation_doi=10.1037/emo0000100; citation_id=CR19">
<meta name=citation_reference content="citation_journal_title=Cogn Emot; citation_title=There’s more to emotion than meets the eye: a processing bias for neutral content in the domain of emotional prosody; citation_author=L Cornew, L Carver, T Love; citation_volume=24; citation_publication_date=2009; citation_pages=1133-1152; citation_doi=10.1080/02699930903247492; citation_id=CR20">
<meta name=citation_reference content="citation_journal_title=Nat Hum Behav; citation_title=The primacy of categories in the recognition of 12 emotions in speech prosody across two cultures; citation_author=AS Cowen, P Laukka, HA Elfenbein; citation_volume=3; citation_publication_date=2019; citation_pages=369-382; citation_doi=10.1038/s41562-019-0533-6; citation_id=CR21">
<meta name=citation_reference content="citation_journal_title=Am Psychol; citation_title=Mapping 24 emotions conveyed by brief human vocalization; citation_author=AS Cowen, HA Elfenbein, P Laukka; citation_volume=74; citation_publication_date=2019; citation_pages=698-712; citation_doi=10.1037/amp0000399; citation_id=CR22">
<meta name=citation_reference content="Cox DR, Snell EJ (1989) Analysis of binary data, 2nd edn. Chapman &amp; Hall">
<meta name=citation_reference content="citation_journal_title=Chemom Intell Lab Syst; citation_title=Comparison of performance of five common classifiers represented as boundary methods: Euclidean distance to centroids, linear discriminant analysis, quadratic discriminant analysis, learning vector quantization and support vector machines, as dependent on data structure; citation_author=SJ Dixon, RG Brereton; citation_volume=95; citation_publication_date=2009; citation_pages=1-17; citation_doi=10.1016/j.chemolab.2008.07.010; citation_id=CR25">
<meta name=citation_reference content="citation_title=Confidence judgements; citation_inbook_title=Metacognition; citation_publication_date=2009; citation_pages=118-139; citation_id=CR26; citation_author=J Dunlosky; citation_author=J Metcalfe; citation_publisher=Sage Publications">
<meta name=citation_reference content="citation_journal_title=IEEE Trans Affect Comput; citation_title=The Geneva minimalistic acoustic parameter set (GeMAPS) for voice research and affective computing; citation_author=F Eyben, KR Scherer, BW Schuller; citation_volume=7; citation_publication_date=2016; citation_pages=190-202; citation_doi=10.1109/TAFFC.2015.2457417; citation_id=CR27">
<meta name=citation_reference content="citation_journal_title=Emot Rev; citation_title=Interjections and emotion (with special reference to “surprise” and “disgust”); citation_author=C Goddard; citation_volume=6; citation_publication_date=2014; citation_pages=53-63; citation_doi=10.1177/1754073913491843; citation_id=CR28">
<meta name=citation_reference content="citation_journal_title=J Acoust Soc Am; citation_title=Beyond arousal: valence and potency/control cues in the vocal expression of emotion; citation_author=M Goudbeek, KR Scherer; citation_volume=128; citation_publication_date=2010; citation_pages=1322-1336; citation_doi=10.1121/1.3466853; citation_id=CR29">
<meta name=citation_reference content="citation_journal_title=J Pers Soc Psychol; citation_title=Generality of impression-formation processes for evaluative and nonevaluative judgments; citation_author=DL Hamilton, LJ Huffman; citation_volume=20; citation_publication_date=1971; citation_pages=200-207; citation_doi=10.1037/h0031698; citation_id=CR30">
<meta name=citation_reference content="citation_journal_title=J Exp Res Pers; citation_title=Differential weighting of favorable and unfavorable attributes in impressions of personality; citation_author=DL Hamilton, MP Zanna; citation_volume=6; citation_publication_date=1972; citation_pages=204-212; citation_id=CR31">
<meta name=citation_reference content="citation_journal_title=J Voice; citation_title=Acoustical correlates of affective prosody; citation_author=K Hammerschmidt, U Juergens; citation_volume=21; citation_publication_date=2007; citation_pages=531-540; citation_doi=10.1016/j.jvoice.2006.03.002; citation_id=CR32">
<meta name=citation_reference content="citation_journal_title=Emotion; citation_title=“Worth a thousand words”: absolute and relative decoding of nonlinguistic affect vocalizations; citation_author=ST Hawk, GA Kleef, AH Fischer; citation_volume=9; citation_publication_date=2009; citation_pages=293-305; citation_doi=10.1037/a0015178; citation_id=CR33">
<meta name=citation_reference content="citation_journal_title=Biom J; citation_title=Simultaneous inference in general parametric models; citation_author=T Hothorn, F Bretz, P Westfall; citation_volume=50; citation_publication_date=2008; citation_pages=346-363; citation_doi=10.1002/bimj.200810425; citation_id=CR34">
<meta name=citation_reference content="citation_journal_title=J Pers Soc Psychol; citation_title=Negative information weighs more heavily on the brain: the negativity bias in evaluative categorizations; citation_author=TA Ito, JT Larsen, NK Smith; citation_volume=75; citation_publication_date=1998; citation_pages=887-900; citation_doi=10.1037/0022-3514.75.4.887; citation_id=CR35">
<meta name=citation_reference content="citation_title=An introduction to statistical learning with applications in R; citation_inbook_title=Springer texts in statistics; citation_publication_date=2013; citation_pages=303-332; citation_id=CR36; citation_author=G James; citation_author=D Witten; citation_author=T Hastie; citation_publisher=Springer">
<meta name=citation_reference content="citation_journal_title=Speech Commun; citation_title=The sound of confidence and doubt; citation_author=X Jiang, MD Pell; citation_volume=88; citation_publication_date=2017; citation_pages=106-126; citation_doi=10.1016/j.specom.2017.01.011; citation_id=CR37">
<meta name=citation_reference content="Jiang X, Pell DM (2014) Encoding and decoding confidence information in speech. In: Proceedings of the 7th international conference in speech prosody (social and linguistic speech prosody). pp. 573–576. 
                  http://fastnet.netsoc.ie/sp7/sp7book.pdf
                  
                . Accessed 30 Nov 2018">
<meta name=citation_reference content="citation_title=Vocal communication of emotion; citation_inbook_title=The handbook of emotion; citation_publication_date=2000; citation_pages=220-235; citation_id=CR39; citation_author=T Johnstone; citation_author=KR Scherer; citation_publisher=Guildford">
<meta name=citation_reference content="citation_title=Vocal expression of affect; citation_inbook_title=The new handbook of methods in nonverbal behavior research; citation_publication_date=2005; citation_id=CR40; citation_author=PN Juslin; citation_author=KR Scherer; citation_publisher=Oxford University Press">
<meta name=citation_reference content="citation_journal_title=Psychol Bull; citation_title=Communication of emotions in vocal expression and music performance: different channels, same code?; citation_author=PN Juslin, P Laukka; citation_volume=129; citation_publication_date=2003; citation_pages=770-814; citation_doi=10.1037/0033-2909.129.5.770; citation_id=CR41">
<meta name=citation_reference content="citation_journal_title=Emotion; citation_title=Impact of intended emotion intensity on cue utilization and decoding accuracy in vocal expression of emotion; citation_author=PN Juslin, P Laukka; citation_volume=1; citation_publication_date=2001; citation_pages=381-412; citation_doi=10.1037//1528-3542.1.4.381; citation_id=CR42">
<meta name=citation_reference content="citation_journal_title=Front Psychol; citation_title=Hot speech and exploding bombs: autonomic arousal during emotion classification of prosodic utterances and affective sounds; citation_author=R Juergens, J Fischer, A Schacht; citation_volume=9; citation_publication_date=2018; citation_pages=228; citation_doi=10.3389/fpsyg.2018.00228; citation_id=CR43">
<meta name=citation_reference content="citation_journal_title=J Nonverbal Behav; citation_title=Effect of acting experience on emotion expression and recognition in voice: non-actors provide better stimuli than expected; citation_author=R Juergens, A Grass, M Drolet; citation_volume=39; citation_publication_date=2015; citation_pages=195-214; citation_doi=10.1007/s10919-015-0209-5; citation_id=CR44">
<meta name=citation_reference content="citation_journal_title=Front Psychol; citation_title=Encoding conditions affect recognition of vocally expressed emotions across cultures; citation_author=R Juergens, M Drolet, R Pirow; citation_volume=4; citation_publication_date=2013; citation_pages=111; citation_doi=10.3389/fpsyg.2013.00111; citation_id=CR45">
<meta name=citation_reference content="citation_journal_title=Front Psychol; citation_title=Authentic and play-acted vocal emotion expressions reveal acoustic differences; citation_author=R Juergens, K Hammerschmidt, J Fischer; citation_volume=2; citation_publication_date=2011; citation_pages=180; citation_doi=10.3389/fpsyg.2011.00180; citation_id=CR46">
<meta name=citation_reference content="citation_journal_title=Emotion; citation_title=Metacognition of emotional face recognition; citation_author=KJ Kelly, J Metcalfe; citation_volume=11; citation_publication_date=2011; citation_pages=896-906; citation_doi=10.1037/a0023746; citation_id=CR47">
<meta name=citation_reference content="citation_journal_title=J Nonverbal Behav; citation_title=Vocal signs of confidence; citation_author=C Kimble, S Seidel; citation_volume=15; citation_publication_date=1991; citation_pages=99-105; citation_doi=10.1007/BF00998265; citation_id=CR48">
<meta name=citation_reference content="citation_journal_title=Cogn Emot; citation_title=Word and voice: spontaneous attention to emotional speech in two cultures; citation_author=S Kitayama, K Ishii; citation_volume=16; citation_publication_date=2002; citation_pages=29-59; citation_doi=10.1080/0269993943000121; citation_id=CR49">
<meta name=citation_reference content="citation_journal_title=Phonetica; citation_title=‘Speech-smile’, ‘speech-laugh’, ‘laughter’ and their sequencing in dialogic interaction; citation_author=KJ Kohler; citation_volume=65; citation_publication_date=2008; citation_pages=1-18; citation_doi=10.1159/000130013; citation_id=CR50">
<meta name=citation_reference content="citation_journal_title=Psychon Bull Rev; citation_title=When confidence in a choice is independent of which choice is made; citation_author=A Koriat; citation_volume=15; citation_publication_date=2008; citation_pages=997-1001; citation_doi=10.3758/PBR.15.5.997; citation_id=CR51">
<meta name=citation_reference content="citation_journal_title=Brain Res; citation_title=When emotional prosody and semantics dance cheek to cheek: ERP evidence; citation_author=SA Kotz, S Paulmann; citation_volume=1151; citation_publication_date=2007; citation_pages=107-118; citation_doi=10.1016/j.brainres.2007.03.015; citation_id=CR52">
<meta name=citation_reference content="citation_journal_title=Emotion; citation_title=Affect bursts: dynamic patterns of facial expression; citation_author=EG Krumhuber, KR Scherer; citation_volume=11; citation_publication_date=2011; citation_pages=825-841; citation_doi=10.1037/a0023856; citation_id=CR53">
<meta name=citation_reference content="citation_journal_title=Front Psychol; citation_title=Gender differences in the recognition of vocal emotions; citation_author=A Lausen, A Schacht; citation_volume=9; citation_publication_date=2018; citation_pages=882; citation_doi=10.3389/fpsyg.2018.00882; citation_id=CR54">
<meta name=citation_reference content="citation_journal_title=Behav Res Methods; citation_title=When voices get emotional: a corpus of nonverbal vocalizations for research on emotion processing; citation_author=CF Lima, SL Castro, SK Scott; citation_volume=45; citation_publication_date=2013; citation_pages=1234-1245; citation_doi=10.3758/s13428-013-0324-3; citation_id=CR55">
<meta name=citation_reference content="citation_journal_title=Neuroreport; citation_title=Electrophysiological insights into processing nonverbal emotional vocalizations; citation_author=T Liu, AP Pinheiro, G Deng; citation_volume=23; citation_publication_date=2012; citation_pages=108-112; citation_doi=10.1097/WNR.0b013e32834ea757; citation_id=CR56">
<meta name=citation_reference content="citation_journal_title=J Exp Psychol Learn Mem Cogn; citation_title=The cue-familiarity heuristic in metacognition; citation_author=J Metcalfe, BL Schwartz, SG Joaquim; citation_volume=19; citation_publication_date=1993; citation_pages=851-861; citation_doi=10.1037//0278-7393.19.4.851; citation_id=CR57">
<meta name=citation_reference content="citation_journal_title=Neuropsychologia; citation_title=The neural response to emotional prosody, as revealed by functional magnetic resonance imaging; citation_author=RLC Mitchell, R Elliott, M Barry; citation_volume=41; citation_publication_date=2003; citation_pages=1410-1421; citation_doi=10.1016/S0028-3932(03)00017-4; citation_id=CR58">
<meta name=citation_reference content="Mozziconacci S (2002) Prosody and emotions. In: Proceedings of speech prosody, pp. 1–9. 
                  https://www.isca-speech.org/archive/sp2002/
                  
                . Accessed 30 Nov 2018">
<meta name=citation_reference content="citation_journal_title=Int J Speech Technol; citation_title=Vocal-based emotion recognition using random forests and decision tree; citation_author=F Noroozi, T Sapiński, D Kamińska; citation_volume=20; citation_publication_date=2017; citation_pages=239-246; citation_doi=10.1007/s10772-017-9396-2; citation_id=CR60">
<meta name=citation_reference content="Norsonic Nor140 (2017) Instruction manual. Lierskogen, Norway. 
                  https://www.campbell-associates.co.uk/norsonic-140-sound-level-meter
                  
                ">
<meta name=citation_reference content="citation_journal_title=Mem Cogn; citation_title=Resolution of lexical ambiguity by emotional tone of voice; citation_author=L Nygaard, E Lunders; citation_volume=30; citation_publication_date=2002; citation_pages=583-593; citation_doi=10.3758/BF03194959; citation_id=CR61">
<meta name=citation_reference content="citation_journal_title=Behav Res Methods; citation_title=GSU Praat tools: scripts for modifying and analyzing sounds using Praat acoustics software; citation_author=MJ Owren; citation_volume=40; citation_publication_date=2008; citation_pages=822-829; citation_doi=10.3758/BRM.40.3.822; citation_id=CR62">
<meta name=citation_reference content="citation_title=Fear and anxiety as emotional phenomenon: clinical phenomenology, evolutionary perspectives, and information-processing mechanisms; citation_inbook_title=Handbook of emotions; citation_publication_date=1993; citation_pages=511-536; citation_id=CR63; citation_author=A Oehman; citation_publisher=Guildford Press">
<meta name=citation_reference content="citation_journal_title=Front Psychol; citation_title=Introducing the Oxford Vocal (OxVoc) Sounds database: a validated set of non-acted affective sounds from human infants, adults, and domestic animals; citation_author=CE Parsons, KS Young, MG Craske; citation_volume=5; citation_publication_date=2014; citation_pages=562; citation_doi=10.3389/fpsyg.2014.00562; citation_id=CR64">
<meta name=citation_reference content="citation_title=The neurocognition of prosody; citation_inbook_title=Neurobiology of language; citation_publication_date=2016; citation_id=CR65; citation_author=S Paulmann; citation_publisher=Elsevier">
<meta name=citation_reference content="citation_journal_title=Cogn Emot; citation_title=Cross-cultural emotional prosody recognition: evidence from Chinese and British listeners; citation_author=S Paulmann, AK Uskul; citation_volume=28; citation_publication_date=2014; citation_pages=230-244; citation_doi=10.1080/02699931.2013.812033; citation_id=CR66">
<meta name=citation_reference content="citation_journal_title=Brain Lang; citation_title=An ERP investigation on the temporal dynamics of emotional prosody and emotional semantics in pseudo- and lexical sentence context; citation_author=S Paulmann, SA Kotz; citation_volume=105; citation_publication_date=2008; citation_pages=59-69; citation_doi=10.1016/j.bandl.2007.11.005; citation_id=CR67">
<meta name=citation_reference content="citation_journal_title=Brain Lang; citation_title=How aging affects the recognition of emotional speech; citation_author=S Paulmann, MD Pell, SA Kotz; citation_volume=104; citation_publication_date=2008; citation_pages=262-269; citation_doi=10.1016/j.bandl.2007.03.002; citation_id=CR68">
<meta name=citation_reference content="citation_journal_title=Biol Psychol; citation_title=Preferential decoding of emotion from human non-linguistic vocalizations versus speech prosody; citation_author=MD Pell, K Rothermich, P Liu; citation_volume=111; citation_publication_date=2015; citation_pages=14-25; citation_doi=10.1016/j.biopsycho.2015.08.008; citation_id=CR69">
<meta name=citation_reference content="citation_journal_title=PLoS ONE; citation_title=On the time course of vocal emotion recognition; citation_author=MD Pell, SA Kotz; citation_volume=6; citation_issue=11; citation_publication_date=2011; citation_doi=10.1371/journal.pone.0027256; citation_id=CR70">
<meta name=citation_reference content="citation_journal_title=Cogn Emot; citation_title=Emotional speech processing: disentangling the effects of prosody and semantic cues; citation_author=MD Pell, A Jaywant, L Monetta; citation_volume=25; citation_publication_date=2011; citation_pages=834-853; citation_doi=10.1080/02699931.2010.516915; citation_id=CR71">
<meta name=citation_reference content="citation_journal_title=J Nonverbal Behav; citation_title=Recognizing emotions in a foreign language; citation_author=MD Pell, L Monetta, S Paulmann; citation_volume=33; citation_publication_date=2009; citation_pages=107-120; citation_doi=10.1007/s10919-008-0065-7; citation_id=CR72">
<meta name=citation_reference content="citation_journal_title=J Phon; citation_title=Factors in the recognition of vocally expressed emotions: a comparison of four languages; citation_author=MD Pell, S Paulmann, C Dara; citation_volume=37; citation_publication_date=2009; citation_pages=417-435; citation_doi=10.1016/j.wocn.2009.07.005; citation_id=CR73">
<meta name=citation_reference content="citation_title=Positive–negative asymmetry in evaluations: the distinction between affective and informational negativity effects; citation_inbook_title=European review of social psychology, vol. 1; citation_publication_date=1990; citation_pages=33-60; citation_id=CR74; citation_author=G Peeters; citation_author=J Czapinski; citation_publisher=Wiley">
<meta name=citation_reference content="citation_journal_title=J Acoust Soc Am; citation_title=Importance of F0 for predicting vocal emotion categorization; citation_author=MK Pichora-Fuller, K Dupuis, P Lieshout; citation_volume=140; citation_publication_date=2016; citation_pages=3401-3401; citation_doi=10.1121/1.4970917; citation_id=CR75">
<meta name=citation_reference content="citation_title=R: a language and environment for statistical computing; citation_publication_date=2017; citation_id=CR76; citation_publisher=R Foundation for Statistical Computing">
<meta name=citation_reference content="citation_journal_title=Front Psychol; citation_title=Feeling backwards? How temporal order in speech affects the time course of vocal emotion recognition; citation_author=S Rigoulot, E Wassiliwizky, MD Pell; citation_volume=4; citation_publication_date=2013; citation_pages=367; citation_doi=10.3389/fpsyg.2013.00367; citation_id=CR77">
<meta name=citation_reference content="citation_journal_title=Q J Exp Psychol; citation_title=Perceptual cues in nonverbal vocal expressions of emotion; citation_author=DA Sauter, F Eisner, AJ Calder; citation_volume=63; citation_publication_date=2010; citation_pages=2251-2272; citation_doi=10.1080/17470211003721642; citation_id=CR78">
<meta name=citation_reference content="Sauter DA (2006) An investigation into vocal expressions of emotions: the roles of valence, culture, and acoustic factors. Unpublished Ph.D. thesis, University College London">
<meta name=citation_reference content="citation_title=Extracting emotions and communication styles from prosody; citation_inbook_title=Physiological computing systems, vol. 8908; citation_publication_date=2014; citation_id=CR80; citation_author=L Sbattella; citation_author=L Colombo; citation_author=C Rinaldi; citation_publisher=Springer">
<meta name=citation_reference content="citation_journal_title=PLoS ONE; citation_title=Unfolding and dynamics of affect bursts decoding in humans; citation_author=S Schaerlaeken, D Grandjean; citation_volume=13; citation_publication_date=2018; citation_doi=10.1371/journal.pone.0206216; citation_id=CR81">
<meta name=citation_reference content="Scherer KR, Baenziger T (2004) Emotional expression in prosody: a review and an agenda for future research. In: Bel B, Marlien I (eds) Speech prosody, Nara, Japan, pp. 359–366">
<meta name=citation_reference content="citation_journal_title=J Cross Cult Psychol; citation_title=Emotion inferences from vocal expression correlate across languages and cultures; citation_author=KR Scherer, R Banse, H Wallbott; citation_volume=32; citation_publication_date=2001; citation_pages=76-92; citation_doi=10.1177/0022022101032001009; citation_id=CR83">
<meta name=citation_reference content="citation_title=Affect bursts; citation_inbook_title=Emotions: essays on emotion theory; citation_publication_date=1994; citation_pages=161-193; citation_id=CR84; citation_author=KR Scherer; citation_publisher=Erlbaum">
<meta name=citation_reference content="citation_journal_title=J Res Pers; citation_title=The voice of confidence: Paralinguistic cues and audience evaluation; citation_author=KR Scherer, H London, J Wolf; citation_volume=7; citation_publication_date=1973; citation_pages=31-44; citation_doi=10.1016/0092-6566(73)90030-5; citation_id=CR85">
<meta name=citation_reference content="citation_journal_title=PLoS ONE; citation_title=Mark my words: tone of voice changes affective word representations in memory; citation_author=A Schirmer; citation_volume=5; citation_issue=2; citation_publication_date=2010; citation_doi=10.1371/journal.pone.0009080; citation_id=CR86">
<meta name=citation_reference content="citation_journal_title=J Cogn Neurosci; citation_title=ERP evidence for a sex-specific Stroop effect in emotional speech; citation_author=A Schirmer, SA Kotz; citation_volume=15; citation_publication_date=2003; citation_pages=1135-1148; citation_doi=10.1162/089892903322598102; citation_id=CR87">
<meta name=citation_reference content="Scott SK, Sauter D, McGettigan C (2010) Brain mechanisms for processing perceived emotional vocalizations in humans. In: Brudzynski SM (ed), Handbook of behavioral neuroscience, Elsevier, pp. 187–197">
<meta name=citation_reference content="Seber GAF (1984) Multivariate observations. John Wiley &amp; Sons">
<meta name=citation_reference content="Thompson WF, Balkwill LL (2009) Cross-cultural similarities and differences. In: Juslin PN, Sloboda JA (eds) Handbook of music and emotion: theory, research, applications, 1st edn. Oxford University Press, New York, pp. 755–791. 
                  https://doi.org/10.1093/acprof:oso/9780199230143.003.0027
                  
                ">
<meta name=citation_reference content="citation_journal_title=Lang Speech; citation_title=Automatic discrimination of emotion from spoken Finnish; citation_author=J Toivanen, E Väyrynen, T Sepännen; citation_volume=47; citation_publication_date=2004; citation_pages=383-412; citation_doi=10.1177/00238309040470040301; citation_id=CR92">
<meta name=citation_reference content="citation_journal_title=J Nonverbal Behav; citation_title=On measuring performance in category judgement studies of nonverbal behaviour; citation_author=HL Wagner; citation_volume=17; citation_publication_date=1993; citation_pages=3-28; citation_doi=10.1007/BF00987006; citation_id=CR93">
<meta name=citation_reference content="Wendt B, Scheich H (2002) The “Magdeburger Prosodie Korpus”—a spoken language corpus for fMRI-Studies. In: Bel B, Marlien I (eds) Speech prosody. Aix-en-Provence, SproSIG, pp. 699–701">
<meta name=citation_reference content="citation_journal_title=Perspect Psychol Sci; citation_title=Explaining away: a model of affective adaptation; citation_author=TD Wilson, DT Gilbert; citation_volume=3; citation_publication_date=2008; citation_pages=370-386; citation_doi=10.1111/j.1745-6924.2008.00085.x; citation_id=CR95">
<meta name=citation_reference content="citation_journal_title=JAMA; citation_title=World Medical Association Declaration of Helsinki: ethical principles form medical research involving human subjects; citation_author=; citation_volume=310; citation_publication_date=2013; citation_pages=2191-2194; citation_doi=10.1001/jama.2013.281053; citation_id=CR96">
<meta name=citation_author content="Lausen, Adi">
<meta name=citation_author_institution content="Department of Affective Neuroscience and Psychophysiology, Institute for Psychology, University of Goettingen, Goettingen, Germany">
<meta name=citation_author_institution content="Department of Mathematical Sciences, University of Essex, Colchester, UK">
<meta name=citation_author content="Hammerschmidt, Kurt">
<meta name=citation_author_institution content="Cognitive Ethology Laboratory, German Primate Center, University of Goettingen, Goettingen, Germany">
<meta name=citation_author_institution content="Leibniz ScienceCampus “Primate Cognition”, Goettingen, Germany">
<meta name=access_endpoint content=https://www.nature.com/platform/readcube-access>
<meta name=twitter:site content=@HSScomms>
<meta name=twitter:card content=summary_large_image>
<meta name=twitter:image:alt content="Content cover image">
<meta name=twitter:title content="Emotion recognition and confidence ratings predicted by vocal stimulus type and prosodic parameters">
<meta name=twitter:description content="Humanities and Social Sciences Communications - Emotion recognition and confidence ratings predicted by vocal stimulus type and prosodic parameters">
<meta name=twitter:image content=https://media.springernature.com/full/springer-static/image/art%3A10.1057%2Fs41599-020-0499-z/MediaObjects/41599_2020_499_Fig1_HTML.png>
<meta property=og:url content=https://www.nature.com/articles/s41599-020-0499-z>
<meta property=og:type content=article>
<meta property=og:site_name content=Nature>
<meta property=og:title content="Emotion recognition and confidence ratings predicted by vocal stimulus type and prosodic parameters - Humanities and Social Sciences Communications">
<meta property=og:image content=https://media.springernature.com/m685/springer-static/image/art%3A10.1057%2Fs41599-020-0499-z/MediaObjects/41599_2020_499_Fig1_HTML.png>
<meta http-equiv=origin-trial content="A+cA2PUOfIOKAdSDJOW5CP9ZlxONy1yu+hqAq72zUtKw4rLdihqRp6Nui/jUyCyegr+BUtH+C+Elv0ufn05yBQEAAACFeyJvcmlnaW4iOiJodHRwczovL2RvdWJsZWNsaWNrLm5ldDo0NDMiLCJmZWF0dXJlIjoiUHJpdmFjeVNhbmRib3hBZHNBUElzIiwiZXhwaXJ5IjoxNjY5NzY2Mzk5LCJpc1N1YmRvbWFpbiI6dHJ1ZSwiaXNUaGlyZFBhcnR5Ijp0cnVlfQ=="><meta http-equiv=origin-trial content="A+zsdH3aNZT/bkjT8U/o5ACzyaeNYzTvtoVmwf/KOilfv39pxY2AIsOwhQJv+YnXp98i3TqrQibIVtMWs5UHjgoAAACLeyJvcmlnaW4iOiJodHRwczovL2dvb2dsZXN5bmRpY2F0aW9uLmNvbTo0NDMiLCJmZWF0dXJlIjoiUHJpdmFjeVNhbmRib3hBZHNBUElzIiwiZXhwaXJ5IjoxNjY5NzY2Mzk5LCJpc1N1YmRvbWFpbiI6dHJ1ZSwiaXNUaGlyZFBhcnR5Ijp0cnVlfQ=="><meta http-equiv=origin-trial content="AxceVEhIegcDEHqLXFQ2+vPKqzCppoJYsRCZ/BdfVnbM/sUUF2BXV8lwNosyYjvoxnTh2FC8cOlAnA5uULr/zAUAAACLeyJvcmlnaW4iOiJodHRwczovL2dvb2dsZXRhZ3NlcnZpY2VzLmNvbTo0NDMiLCJmZWF0dXJlIjoiUHJpdmFjeVNhbmRib3hBZHNBUElzIiwiZXhwaXJ5IjoxNjY5NzY2Mzk5LCJpc1N1YmRvbWFpbiI6dHJ1ZSwiaXNUaGlyZFBhcnR5Ijp0cnVlfQ=="><meta http-equiv=origin-trial content="A+Mt6wQ7St5J869uXW1A/aL3lJaYJYff4gUwPvwSbTuZ7z/T1l4np41d/t4W9TdeS/EMua5fYfBoz4v4TT9tdAgAAACAeyJvcmlnaW4iOiJodHRwczovL2NyaXRlby5uZXQ6NDQzIiwiZmVhdHVyZSI6IlByaXZhY3lTYW5kYm94QWRzQVBJcyIsImV4cGlyeSI6MTY2MTI5OTE5OSwiaXNTdWJkb21haW4iOnRydWUsImlzVGhpcmRQYXJ0eSI6dHJ1ZX0="><meta http-equiv=origin-trial content="AxFuPIivbOVh9A1iWigZYBKLEsd09F0TKyZCh3vhaAKunGI5SMnDaV1g2yqrnkAKqythLyd+bS8ShWXWx388aQIAAACAeyJvcmlnaW4iOiJodHRwczovL2NyaXRlby5jb206NDQzIiwiZmVhdHVyZSI6IlByaXZhY3lTYW5kYm94QWRzQVBJcyIsImV4cGlyeSI6MTY2MTI5OTE5OSwiaXNTdWJkb21haW4iOnRydWUsImlzVGhpcmRQYXJ0eSI6dHJ1ZX0="><meta itemprop=position content=1 class=sf-hidden><meta itemprop=position content=2 class=sf-hidden><meta itemprop=position content=3 class=sf-hidden><meta itemprop=position content=4 class=sf-hidden><meta itemprop=publisher content="Springer Nature" class=sf-hidden><link rel=icon type=image/png sizes=32x32 href="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAQAAADZc7J/AAAABGdBTUEAALGPC/xhBQAAACBjSFJNAAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAAAAmJLR0QA/4ePzL8AAAAJcEhZcwAAFxEAABcRAcom8z8AAAAHdElNRQfjCwUMDTMfbuSQAAACE0lEQVRIx6WVv2tTURTHP+++NCmk4A8UmziISJdiHJNAl1qdXJztUBDXLg6FDp10EDK4OLSTkH9BELu0DalToaBEhKDQCoH8ENQOsdgm7zi8+17eTV5elHfucvhyvt973n3nh8WoJZjjHnfJMUsa6NKixh47fKHHBLMpsMUx58jQOeeYLQrYUfQsJToj1ODpUCI7jp5nHyeSLggO++TD6EvUJ5K9U2dp9PZ/p7sSefPbq1HhVhhadd/CBhQbPArqpbjFA67zlSmKPOExD7nDKS1kEHQDhx0XKNL2dG0pyLq8lYaIlCQtL+SHeNaSZTOHNkU3h80BaMsrn/BSnklfgvZRMqbEJjbMcxQEF+SXDn8vNXknr+WN/NTIqdw3BY6Yh1Wz6i7JoQ7/IAVJCpKQpzoTR1aGq3NVsUgi+IBd2tr7xiFnQI8KJwBYpIa7ZlGRMzEnpF/+jO+hnCIzqb8iLaNIxxJIq1h0QNGNxe8qmrEEmoqaiVh4X6WwQrwhqykq5j9SJLVnB2geMWnSe1QUuzSC2BQz2pv2x1/Kr7ULpkCDXUWd7SA2w2U/eFp7F/2bZ02BbeqKPmU6AyzLVe1d44r2bvpSc74HdCjTTwAHlFnz0BOe69TP+K2xz6whgMV3/12AMgeDayNHWuipmuM95lCF2GPdzSLWYnHfItZqc8vvP5erFSLirffbZPR6b/Jp3Hr/C7x93EqSDPa6AAAAJXRFWHRkYXRlOmNyZWF0ZQAyMDE5LTExLTA1VDEyOjEzOjUxKzAxOjAwnQ8pTAAAACV0RVh0ZGF0ZTptb2RpZnkAMjAxOS0xMS0wNVQxMjoxMzo1MSswMTowMOxSkfAAAABXelRYdFJhdyBwcm9maWxlIHR5cGUgaXB0YwAAeJzj8gwIcVYoKMpPy8xJ5VIAAyMLLmMLEyMTS5MUAxMgRIA0w2QDI7NUIMvY1MjEzMQcxAfLgEigSi4A6hcRdPJCNZUAAAAASUVORK5CYII="><style>.sf-hidden{display:none!important}</style><meta http-equiv=content-security-policy content="default-src 'none'; font-src 'self' data:; img-src 'self' data:; style-src 'unsafe-inline'; media-src 'self' data:; script-src 'unsafe-inline' data:;"></head>
<body class=article-page>
<noscript><iframe src="https://collect.nature.com/ns.html?id=GTM-MRVXSHQ"
                  height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<div class="position-relative cleared z-index-50 background-white" data-test=top-containers>
 <a class=c-skip-link href=#content>Skip to main content</a>
<div class="c-grade-c-banner u-hide sf-hidden">
 
</div>
 
 <div class=u-lazy-ad-wrapper>
 
 
 <div class="u-hide u-show-following-ad sf-hidden"></div>
 <aside class="c-ad c-ad--728x90">
 <div class=c-ad__inner data-container-type=banner-advert>
 <p class=c-ad__label>Advertisement</p>
 
 
 
 <div id=div-gpt-ad-top-1 class="div-gpt-ad advert leaderboard js-ad text-center hide-print grade-c-hide" data-ad-type=top data-test=top-ad data-pa11y-ignore data-gpt data-gpt-unitpath=/285/palcomms.nature.com/article data-gpt-sizes=728x90 data-gpt-targeting="type=article;pos=top;artid=s41599-020-0499-z;doi=10.1057/s41599-020-0499-z;subjmeta=4009,4014,477;kwrd=Language+and+linguistics,Psychology" data-google-query-id=CJizmu6x6_kCFRiHgwcdOVEO2w>
 
 <div id=google_ads_iframe_/270604982/nature/palcomms/articles_0__container__ style="border:0pt none"></div></div>
 
 
 </div>
 </aside>
 </div>
 <header class=c-header id=header data-header data-track-component=nature-150-split-header style=border-color:#a5a9d6>
 <div class="c-header__row c-header__row--flush">
 <div class=c-header__container>
 <div class=c-header__split>
 
 
 <div class=c-header__logo-container>
 
 <a href=https://www.nature.com/palcomms data-track=click data-track-action=home data-track-label=image>
 <picture class=c-header__logo>
 
 <img src=data:null;base64, alt="Humanities and Social Sciences Communications" srcset sizes>
 </picture>
 </a>
 
 </div>
 
 <ul class="c-header__menu c-header__menu--global">
 <li class="c-header__item c-header__item--nature-research">
 <a class=c-header__link href=https://www.nature.com/siteindex data-test=siteindex-link data-track=click data-track-action="open nature research index" data-track-label=link>
 <span>View all journals</span>
 </a>
 </li>
 <li class="c-header__item c-header__item--pipe">
 <a class=c-header__link href=https://www.nature.com/articles/s41599-020-0499-z data-header-expander data-test=search-link data-track=click data-track-action="open search tray" data-track-label=button role=button aria-haspopup=true aria-expanded=false>
 <span>Search</span><svg role=img aria-hidden=true focusable=false height=22 width=22 viewBox="0 0 18 18" xmlns=http://www.w3.org/2000/svg><path d="M16.48 15.455c.283.282.29.749.007 1.032a.738.738 0 01-1.032-.007l-3.045-3.044a7 7 0 111.026-1.026zM8 14A6 6 0 108 2a6 6 0 000 12z"></path></svg>
 </a><div id=search-menu class="c-header-expander c-header-expander--tray u-hide-print has-tethered u-js-hide sf-hidden" data-track-component=nature-150-split-header hidden>
 
</div>
 </li>
 <li class=c-header__item>
 <a href=https://www.nature.com/nams/svc/myaccount id=my-account class="c-header__link placeholder" data-test=login-link data-track=click data-track-action="my account" data-track-category=nature-150-split-header data-track-label=link style=display:none>
 <svg role=img aria-hidden=true focusable=false height=22 width=22 viewBox="0 0 18 18" xmlns=http://www.w3.org/2000/svg><path d="M10.238 16.905a7.96 7.96 0 003.53-1.48c-.874-2.514-2.065-3.936-3.768-4.319V9.83a3.001 3.001 0 10-2 0v1.277c-1.703.383-2.894 1.805-3.767 4.319A7.96 7.96 0 009 17c.419 0 .832-.032 1.238-.095zm4.342-2.172a8 8 0 10-11.16 0c.757-2.017 1.84-3.608 3.49-4.322a4 4 0 114.182 0c1.649.714 2.731 2.305 3.488 4.322zM9 18A9 9 0 119 0a9 9 0 010 18z" fill=#333 fill-rule=evenodd></path></svg>
</a>
<a href="https://idp.nature.com/authorize/natureuser?client_id=grover&amp;redirect_uri=https%3A%2F%2Fwww.nature.com%2Farticles%2Fs41599-020-0499-z" id=login-button class="c-header__link placeholder" data-test=login-link data-track=click data-track-action=login data-track-category=nature-150-split-header data-track-label=link>
 <span>Login</span><svg role=img aria-hidden=true focusable=false height=22 width=22 viewBox="0 0 18 18" xmlns=http://www.w3.org/2000/svg><path d="M10.238 16.905a7.96 7.96 0 003.53-1.48c-.874-2.514-2.065-3.936-3.768-4.319V9.83a3.001 3.001 0 10-2 0v1.277c-1.703.383-2.894 1.805-3.767 4.319A7.96 7.96 0 009 17c.419 0 .832-.032 1.238-.095zm4.342-2.172a8 8 0 10-11.16 0c.757-2.017 1.84-3.608 3.49-4.322a4 4 0 114.182 0c1.649.714 2.731 2.305 3.488 4.322zM9 18A9 9 0 119 0a9 9 0 010 18z" fill=#333 fill-rule=evenodd></path></svg>
</a>
 </li>
 </ul>
 </div>
 </div>
 </div>
 
 <div class=c-header__row>
 <div class=c-header__container data-test=navigation-row>
 <div class=c-header__split>
 <div class=c-header__split>
 <ul class="c-header__menu c-header__menu--journal">
 
 <li class="c-header__item c-header__item--dropdown-menu" data-test=explore-content-button>
 <a href=https://www.nature.com/articles/s41599-020-0499-z class="c-header__link c-header__link--chevron" data-header-expander data-test=menu-button--explore data-track=click data-track-action="open explore expander" data-track-label=button role=button aria-haspopup=true aria-expanded=false>
 <span><span class=c-header__show-text>Explore</span> content</span><svg role=img aria-hidden=true focusable=false height=16 viewBox="0 0 16 16" width=16 xmlns=http://www.w3.org/2000/svg><path d="m5.58578644 3-3.29289322-3.29289322c-.39052429-.39052429-.39052429-1.02368927 0-1.41421356s1.02368927-.39052429 1.41421356 0l4 4c.39052429.39052429.39052429 1.02368927 0 1.41421356l-4 4c-.39052429.39052429-1.02368927.39052429-1.41421356 0s-.39052429-1.02368927 0-1.41421356z" transform="matrix(0 1 -1 0 11 3)"></path></svg>
 </a><nav class="u-hide-print c-header-expander has-tethered u-js-hide sf-hidden" aria-labelledby=Explore-content data-test=Explore-content id=explore data-track-component=nature-150-split-header hidden>
 
 </nav>
 </li>
 
 
 <li class="c-header__item c-header__item--dropdown-menu">
 <a href=https://www.nature.com/articles/s41599-020-0499-z class="c-header__link c-header__link--chevron" data-header-expander data-test=menu-button--about-the-journal data-track=click data-track-action="open about the journal expander" data-track-label=button role=button aria-haspopup=true aria-expanded=false>
 <span>About <span class=c-header__show-text>the journal</span></span><svg role=img aria-hidden=true focusable=false height=16 viewBox="0 0 16 16" width=16 xmlns=http://www.w3.org/2000/svg><path d="m5.58578644 3-3.29289322-3.29289322c-.39052429-.39052429-.39052429-1.02368927 0-1.41421356s1.02368927-.39052429 1.41421356 0l4 4c.39052429.39052429.39052429 1.02368927 0 1.41421356l-4 4c-.39052429.39052429-1.02368927.39052429-1.41421356 0s-.39052429-1.02368927 0-1.41421356z" transform="matrix(0 1 -1 0 11 3)"></path></svg>
 </a><nav class="u-hide-print c-header-expander has-tethered u-js-hide sf-hidden" aria-labelledby=About-the-journal id=about-the-journal data-test=about-the-journal data-track-component=nature-150-split-header hidden>
 
 </nav>
 </li>
 
 <li class="c-header__item c-header__item--dropdown-menu u-mr-2" data-test=publish-with-us-button>
 <a href=https://www.nature.com/articles/s41599-020-0499-z class="c-header__link c-header__link--chevron c-header__link--dropdown-menu" data-header-expander data-test=menu-button--publish data-track=click data-track-action="open publish with us expander" data-track-label=button role=button aria-haspopup=true aria-expanded=false>
 <span>Publish <span class=c-header__show-text>with us</span></span><svg role=img aria-hidden=true focusable=false height=16 viewBox="0 0 16 16" width=16 xmlns=http://www.w3.org/2000/svg><path d="m5.58578644 3-3.29289322-3.29289322c-.39052429-.39052429-.39052429-1.02368927 0-1.41421356s1.02368927-.39052429 1.41421356 0l4 4c.39052429.39052429.39052429 1.02368927 0 1.41421356l-4 4c-.39052429.39052429-1.02368927.39052429-1.41421356 0s-.39052429-1.02368927 0-1.41421356z" transform="matrix(0 1 -1 0 11 3)"></path></svg>
 </a><nav class="u-hide-print c-header-expander has-tethered u-js-hide sf-hidden" aria-labelledby=Publish-with-us-label id=publish-with-us data-test=publish-with-us data-track-component=nature-150-split-header hidden>
 
 </nav>
 </li>
 
 
 </ul>
 
 </div>
 <ul class="c-header__menu c-header__menu--tools">
 
 <li class=c-header__item>
 <a class=c-header__link href="https://www.nature.com/my-account/alerts/subscribe-journal?list-id=338" rel=nofollow data-track=click data-track-action="Sign up for alerts" data-track-label="link (desktop site header)" data-track-external>
 <span>Sign up for alerts</span><svg role=img aria-hidden=true focusable=false height=18 viewBox="0 0 18 18" width=18 xmlns=http://www.w3.org/2000/svg><path d="m4 10h2.5c.27614237 0 .5.2238576.5.5s-.22385763.5-.5.5h-3.08578644l-1.12132034 1.1213203c-.18753638.1875364-.29289322.4418903-.29289322.7071068v.1715729h14v-.1715729c0-.2652165-.1053568-.5195704-.2928932-.7071068l-1.7071068-1.7071067v-3.4142136c0-2.76142375-2.2385763-5-5-5-2.76142375 0-5 2.23857625-5 5zm3 4c0 1.1045695.8954305 2 2 2s2-.8954305 2-2zm-5 0c-.55228475 0-1-.4477153-1-1v-.1715729c0-.530433.21071368-1.0391408.58578644-1.4142135l1.41421356-1.4142136v-3c0-3.3137085 2.6862915-6 6-6s6 2.6862915 6 6v3l1.4142136 1.4142136c.3750727.3750727.5857864.8837805.5857864 1.4142135v.1715729c0 .5522847-.4477153 1-1 1h-4c0 1.6568542-1.3431458 3-3 3-1.65685425 0-3-1.3431458-3-3z" fill=#222></path></svg>
 </a>
 </li>
 
 
 <li class="c-header__item c-header__item--pipe">
 <a class=c-header__link href=http://feeds.nature.com/palcomms/rss/current data-track=click data-track-action="rss feed" data-track-label=link>
 <span>RSS feed</span>
 </a>
 </li>
 
 </ul>
 </div>
 </div>
 </div>
 
 </header>
 
 
 <nav class="u-mb-16 u-hide u-show-at-md" aria-label=breadcrumbs>
 <div class=u-container>
 <ol class=c-breadcrumbs itemscope itemtype=https://schema.org/BreadcrumbList>
 <li class=c-breadcrumbs__item id=breadcrumb0 itemprop=itemListElement itemscope itemtype=https://schema.org/ListItem><a class=c-breadcrumbs__link href=https://www.nature.com/ itemprop=item data-track=click data-track-action=breadcrumb data-track-category=header data-track-label=link:nature><span itemprop=name>nature</span></a>
 <svg class=c-breadcrumbs__chevron role=img aria-hidden=true focusable=false height=10 viewBox="0 0 10 10" width=10 xmlns=http://www.w3.org/2000/svg>
 <path d="m5.96738168 4.70639573 2.39518594-2.41447274c.37913917-.38219212.98637524-.38972225 1.35419292-.01894278.37750606.38054586.37784436.99719163-.00013556 1.37821513l-4.03074001 4.06319683c-.37758093.38062133-.98937525.38100976-1.367372-.00003075l-4.03091981-4.06337806c-.37759778-.38063832-.38381821-.99150444-.01600053-1.3622839.37750607-.38054587.98772445-.38240057 1.37006824.00302197l2.39538588 2.4146743.96295325.98624457z" fill=#666 fill-rule=evenodd transform="matrix(0 -1 1 0 0 10)"></path>
 </svg>
 <li class=c-breadcrumbs__item id=breadcrumb1 itemprop=itemListElement itemscope itemtype=https://schema.org/ListItem><a class=c-breadcrumbs__link href=https://www.nature.com/palcomms itemprop=item data-track=click data-track-action=breadcrumb data-track-category=header data-track-label="link:humanities and social sciences communications"><span itemprop=name>humanities and social sciences communications</span></a>
 <svg class=c-breadcrumbs__chevron role=img aria-hidden=true focusable=false height=10 viewBox="0 0 10 10" width=10 xmlns=http://www.w3.org/2000/svg>
 <path d="m5.96738168 4.70639573 2.39518594-2.41447274c.37913917-.38219212.98637524-.38972225 1.35419292-.01894278.37750606.38054586.37784436.99719163-.00013556 1.37821513l-4.03074001 4.06319683c-.37758093.38062133-.98937525.38100976-1.367372-.00003075l-4.03091981-4.06337806c-.37759778-.38063832-.38381821-.99150444-.01600053-1.3622839.37750607-.38054587.98772445-.38240057 1.37006824.00302197l2.39538588 2.4146743.96295325.98624457z" fill=#666 fill-rule=evenodd transform="matrix(0 -1 1 0 0 10)"></path>
 </svg>
 <li class=c-breadcrumbs__item id=breadcrumb2 itemprop=itemListElement itemscope itemtype=https://schema.org/ListItem><a class=c-breadcrumbs__link href="https://www.nature.com/palcomms/articles?type=article" itemprop=item data-track=click data-track-action=breadcrumb data-track-category=header data-track-label=link:articles><span itemprop=name>articles</span></a>
 <svg class=c-breadcrumbs__chevron role=img aria-hidden=true focusable=false height=10 viewBox="0 0 10 10" width=10 xmlns=http://www.w3.org/2000/svg>
 <path d="m5.96738168 4.70639573 2.39518594-2.41447274c.37913917-.38219212.98637524-.38972225 1.35419292-.01894278.37750606.38054586.37784436.99719163-.00013556 1.37821513l-4.03074001 4.06319683c-.37758093.38062133-.98937525.38100976-1.367372-.00003075l-4.03091981-4.06337806c-.37759778-.38063832-.38381821-.99150444-.01600053-1.3622839.37750607-.38054587.98772445-.38240057 1.37006824.00302197l2.39538588 2.4146743.96295325.98624457z" fill=#666 fill-rule=evenodd transform="matrix(0 -1 1 0 0 10)"></path>
 </svg>
 <li class=c-breadcrumbs__item id=breadcrumb3 itemprop=itemListElement itemscope itemtype=https://schema.org/ListItem>
 <span itemprop=name>article</span></li>
 </ol>
 </div>
 </nav>
 
 
</div>
<div class="u-container u-mt-32 u-mb-32 u-clearfix" id=content data-component=article-container data-container-type=article>
 <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
 
 <div class="c-context-bar c-context-bar--sticky" data-test=context-bar data-context-bar aria-hidden=false>
 <div class="c-context-bar__container u-container">
 <div class=c-context-bar__title>
 Emotion recognition and confidence ratings predicted by vocal stimulus type and prosodic parameters
 </div>
 
 
 <div class="c-pdf-download u-clear-both js-pdf-download">
 <a href=https://www.nature.com/articles/s41599-020-0499-z.pdf class="u-button u-button--full-width u-button--primary u-justify-content-space-between c-pdf-download__link" data-article-pdf=true data-readcube-pdf-url=true data-test=download-pdf data-draft-ignore=true data-track=click data-track-action="download pdf" data-track-label="button sticky" data-track-external tabindex=-1>
 <span class=c-pdf-download__text>Download PDF</span>
 <svg aria-hidden=true focusable=false width=16 height=16 class=u-icon><use xlink:href=#global-icon-download></use></svg>
 </a>
 </div>
 
 </div>
 </div>
 
 <article lang=en>
 
 <div class="c-pdf-button__container u-hide-at-lg js-context-bar-sticky-point-mobile sf-hidden">
 
 </div>
 
 <div class=c-article-header>
 <header>
 <ul class=c-article-identifiers data-test=article-identifier>
 
 
 <li class=c-article-identifiers__item data-test=article-category>Article</li>
 
 
 <li class=c-article-identifiers__item>
 <span class=c-article-identifiers__open data-test=open-access>Open Access</span>
 </li>
 
 
 <li class=c-article-identifiers__item><a href=#article-info data-track=click data-track-action="publication date" data-track-label=link>Published: <time datetime=2020-06-17>17 June 2020</time></a></li>
 </ul>
 <h1 class=c-article-title data-test=article-title data-article-title>Emotion recognition and confidence ratings predicted by vocal stimulus type and prosodic parameters</h1>
 <ul class="c-article-author-list js-etal-collapsed js-no-scroll" data-etal=25 data-etal-small=3 data-test=authors-list data-component-authors-activator=authors-list><li class=c-article-author-list__item><a data-test=author-name data-track=click data-track-action="open author" data-track-label=link href=#auth-Adi-Lausen aria-label="Read more about Adi Lausen" data-author-popup=auth-Adi-Lausen data-corresp-id=c1>Adi Lausen<svg width=16 height=16 focusable=false role=img aria-hidden=true class=u-icon><use xmlns:xlink=http://www.w3.org/1999/xlink xlink:href=#global-icon-email></use></svg></a><span class="u-js-hide sf-hidden">&nbsp;
 </span><sup class="u-js-hide sf-hidden">,</sup> &amp; <li class=c-article-author-list__item><a data-test=author-name data-track=click data-track-action="open author" data-track-label=link href=#auth-Kurt-Hammerschmidt aria-label="Read more about Kurt Hammerschmidt" data-author-popup=auth-Kurt-Hammerschmidt>Kurt Hammerschmidt</a><span class="u-js-hide sf-hidden">&nbsp;
 </span><sup class="u-js-hide sf-hidden">,</sup>&nbsp;</ul>
 
 <p class=c-article-info-details data-container-section=info>
 
 <a data-test=journal-link href=https://www.nature.com/palcomms><i data-test=journal-title>Humanities and Social Sciences Communications</i></a>
 <b data-test=journal-volume><span class=u-visually-hidden>volume</span>&nbsp;7</b>, Article&nbsp;number:&nbsp;<span data-test=article-number>2</span> (<span data-test=article-publication-year>2020</span>)
 <a href=#citeas class="c-article-info-details__cite-as u-hide-print" data-track=click data-track-action="cite this article" data-track-label=link>Cite this article</a>
 </p>
 
 <div class="c-article-metrics-bar__wrapper u-clear-both">
 <ul class="c-article-metrics-bar u-list-reset">
 
 <li class=c-article-metrics-bar__item>
 <p class=c-article-metrics-bar__count>7325 <span class=c-article-metrics-bar__label>Accesses</span></p>
 </li>
 
 
 <li class=c-article-metrics-bar__item>
 <p class=c-article-metrics-bar__count>11 <span class=c-article-metrics-bar__label>Citations</span></p>
 </li>
 
 
 
 <li class=c-article-metrics-bar__item>
 <p class=c-article-metrics-bar__count>7 <span class=c-article-metrics-bar__label>Altmetric</span></p>
 </li>
 
 
 <li class=c-article-metrics-bar__item>
 <p class=c-article-metrics-bar__details><a href=https://www.nature.com/articles/s41599-020-0499-z/metrics data-track=click data-track-action="view metrics" data-track-label=link rel=nofollow>Metrics <span class=u-visually-hidden>details</span></a></p>
 </li>
 </ul>
 </div>
 
 </header>
 
 
 
 
 
 
 </div>
 <div class=c-article-body>
 <section aria-labelledby=Abs1 data-title=Abstract lang=en data-gtm-vis-recent-on-screen-50443292_563=829 data-gtm-vis-first-on-screen-50443292_563=829 data-gtm-vis-total-visible-time-50443292_563=10000 data-gtm-vis-recent-on-screen-50443292_562=829 data-gtm-vis-first-on-screen-50443292_562=829 data-gtm-vis-total-visible-time-50443292_562=10000 data-gtm-vis-has-fired-50443292_563=1 data-gtm-vis-has-fired-50443292_562=1><div class=c-article-section id=Abs1-section><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id=Abs1>Abstract</h2><div class=c-article-section__content id=Abs1-content><p>Human speech expresses emotional meaning not only through semantics, but also through certain attributes of the voice, such as pitch or loudness. In investigations of vocal emotion recognition, there is considerable variability in the types of stimuli and procedures used to examine their influence on emotion recognition. In addition, accurate metacognition was argued to promote correct and confident interpretations in emotion recognition tasks. Nevertheless, such associations have rarely been studied previously. We addressed this gap by examining the impact of vocal stimulus type and prosodic speech attributes on emotion recognition and a person’s confidence in a given response. We analysed a total of 1038 emotional expressions according to a baseline set of 13 prosodic acoustic parameters. Results showed that these parameters provided sufficient discrimination between expressions of emotional categories to permit accurate statistical classification. Emotion recognition and confidence judgments were found to depend on stimulus material as they could be reliably predicted by different constellations of acoustic features. Finally, results indicated that listeners’ accuracy and confidence judgements were significantly higher for affect bursts than speech-embedded stimuli and that the correct classification of emotional expressions elicited increased confidence judgements. Together, these findings show that vocal stimulus type and prosodic attributes of speech strongly influence emotion recognition and listeners’ confidence in these given responses.</p></div></div></section>
 <noscript>
                
            </noscript>
 
 
 <section data-title=Introduction data-gtm-vis-recent-on-screen-50443292_563=6526 data-gtm-vis-first-on-screen-50443292_563=6526 data-gtm-vis-total-visible-time-50443292_563=10000 data-gtm-vis-recent-on-screen-50443292_562=6526 data-gtm-vis-first-on-screen-50443292_562=6526 data-gtm-vis-total-visible-time-50443292_562=10000 data-gtm-vis-has-fired-50443292_563=1 data-gtm-vis-has-fired-50443292_562=1><div class=c-article-section id=Sec1-section><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id=Sec1>Introduction</h2><div class=c-article-section__content id=Sec1-content><p>The ability to correctly understand and appropriately respond to other persons’ emotions plays an important role in everyday social interactions (Chronaki et al., <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2018" title="Chronaki G, Wigelsworth M, Pell MD et al. (2018) The development of cross-cultural recognition of vocal emotions during childhood and adolescence. Sci Rep. 8:8659. 
                  https://doi.org/10.1038/s41598-018-26889-1
                  
                " href=#ref-CR16 id=ref-link-section-d166511592e391>2018</a>; Juslin and Scherer, <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2005" title="Juslin PN, Scherer KR (2005) Vocal expression of affect. In: Harrigan JA, Rosenthal R, Scherer KR (eds) The new handbook of methods in nonverbal behavior research, 1st edn. Oxford University Press, Oxford, pp. 65–135" href=#ref-CR40 id=ref-link-section-d166511592e394>2005</a>). In verbal communication, for instance, humans do not merely consider what their interlocutors are saying (i.e., semantic meaning), but also how they are conveying the spoken information (e.g., high/low pitch of their voice). An all-encompassing term for such vocal qualities of speech is prosody (i.e., tone of voice). Research has shown that prosody may support the correct interpretations of utterances independently of linguistic comprehension (Paulmann, <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2016" title="Paulmann S (2016) The neurocognition of prosody. In: Hickok G, Small S (eds) Neurobiology of language. Elsevier, San Diego, pp. 1109–1120" href=#ref-CR65 id=ref-link-section-d166511592e397>2016</a>; Thompson and Balkwill, <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2009" title="Thompson WF, Balkwill LL (2009) Cross-cultural similarities and differences. In: Juslin PN, Sloboda JA (eds) Handbook of music and emotion: theory, research, applications, 1st edn. Oxford University Press, New York, pp. 755–791. 
                  https://doi.org/10.1093/acprof:oso/9780199230143.003.0027
                  
                " href=#ref-CR91 id=ref-link-section-d166511592e400>2009</a>; Kitayama and Ishii, <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2002" title="Kitayama S, Ishii K (2002) Word and voice: spontaneous attention to emotional speech in two cultures. Cogn Emot 16:29–59. 
                  https://doi.org/10.1080/0269993943000121
                  
                " href=#ref-CR49 id=ref-link-section-d166511592e403>2002</a>), with studies reporting recognition rates for emotions to be significantly higher than chance (Cowen et al., <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2019a" title="Cowen AS, Laukka P, Elfenbein HA et al. (2019a) The primacy of categories in the recognition of 12 emotions in speech prosody across two cultures. Nat Hum Behav 3:369–382. 
                  https://doi.org/10.1038/s41562-019-0533-6
                  
                " href=#ref-CR21 id=ref-link-section-d166511592e407>2019a</a>, <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2019b" title="Cowen AS, Elfenbein HA, Laukka P et al. (2019b) Mapping 24 emotions conveyed by brief human vocalization. Am Psychol 74:698–712. 
                  https://doi.org/10.1037/amp0000399
                  
                " href=#ref-CR22 id=ref-link-section-d166511592e410>2019b</a>; Lausen and Schacht, <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2018" title="Lausen A, Schacht A (2018) Gender differences in the recognition of vocal emotions. Front Psychol 9:882. 
                  https://doi.org/10.3389/fpsyg.2018.00882
                  
                " href=#ref-CR54 id=ref-link-section-d166511592e413>2018</a>; Cordaro et al., <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2016" title="Cordaro DT, Keltner D, Tshering S et al. (2016) The voice conveys emotion in ten globalized cultures and one remote village in Bhutan. Emotion 16:117–128. 
                  https://doi.org/10.1037/emo0000100
                  
                " href=#ref-CR19 id=ref-link-section-d166511592e416>2016</a>; Paulmann and Uskul, <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2014" title="Paulmann S, Uskul AK (2014) Cross-cultural emotional prosody recognition: evidence from Chinese and British listeners. Cogn Emot 28:230–244. 
                  https://doi.org/10.1080/02699931.2013.812033
                  
                " href=#ref-CR66 id=ref-link-section-d166511592e419>2014</a>; Juergens et al., <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2013" title="Juergens R, Drolet M, Pirow R et al. (2013) Encoding conditions affect recognition of vocally expressed emotions across cultures. Front Psychol 4:111. 
                  https://doi.org/10.3389/fpsyg.2013.00111
                  
                " href=#ref-CR45 id=ref-link-section-d166511592e422>2013</a>; Pell et al., <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2009" title="Pell MD, Paulmann S, Dara C et al. (2009) Factors in the recognition of vocally expressed emotions: a comparison of four languages. J Phon 37:417–435. 
                  https://doi.org/10.1016/j.wocn.2009.07.005
                  
                " href=#ref-CR73 id=ref-link-section-d166511592e426>2009</a>; Scherer et al., <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2001" title="Scherer KR, Banse R, Wallbott H (2001) Emotion inferences from vocal expression correlate across languages and cultures. J Cross Cult Psychol 32:76–92. 
                  https://doi.org/10.1177/0022022101032001009
                  
                " href=#ref-CR83 id=ref-link-section-d166511592e429>2001</a>). In addition, metacognition, the ability to actively monitor and reflect upon one’s own performance, has been argued to impact judgements of accuracy in emotion recognition tasks (Bègue et al., <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2019" title="Bègue I, Vaessen M, Hofmeister J et al. (2019) Confidence of emotion expression recognition recruits brain regions outside the face perception network. Soc Cogn Affect Neurosci 4:81–95. 
                  https://doi.org/10.1093/scan/nsy102
                  
                " href=#ref-CR7 id=ref-link-section-d166511592e432>2019</a>; Kelly and Metcalfe, <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2011" title="Kelly KJ, Metcalfe J (2011) Metacognition of emotional face recognition. Emotion 11:896–906. 
                  https://doi.org/10.1037/a0023746
                  
                " href=#ref-CR47 id=ref-link-section-d166511592e435>2011</a>; Dunlosky and Metcalfe, <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2009" title="Dunlosky J, Metcalfe J (2009) Confidence judgements. In: Dunlosky J, Metcalfe J (eds) Metacognition, 1st edn. Sage Publications, Washington, pp. 118–139" href=#ref-CR26 id=ref-link-section-d166511592e438>2009</a>). This ability reflects how faithfully our self-judgement relates to actual performance in a task and can be measured by retrospective confidence ratings (Bègue et al., <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2019" title="Bègue I, Vaessen M, Hofmeister J et al. (2019) Confidence of emotion expression recognition recruits brain regions outside the face perception network. Soc Cogn Affect Neurosci 4:81–95. 
                  https://doi.org/10.1093/scan/nsy102
                  
                " href=#ref-CR7 id=ref-link-section-d166511592e441>2019</a>). To better understand the mechanisms underlying the recognition of emotions from the voice, the present study examined how different types of vocal stimuli and their acoustic attributes influence listeners’ recognition of emotions and confidence ratings.<p>In their endeavour to assess the recognition of emotions from prosody, researchers created a wide variety of stimulus materials. Some decided to use sentences as stimuli because they have been argued to have higher ecological validity (Sauter, <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2006" title="Sauter DA (2006) An investigation into vocal expressions of emotions: the roles of valence, culture, and acoustic factors. Unpublished Ph.D. thesis, University College London" href=#ref-CR79 id=ref-link-section-d166511592e447>2006</a>). However, as emotions are not expressed to the same degree in each word of a sentence it has been suggested that such long-lasting stimuli might contain increased variation and noise in the signal (Sauter, <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2006" title="Sauter DA (2006) An investigation into vocal expressions of emotions: the roles of valence, culture, and acoustic factors. Unpublished Ph.D. thesis, University College London" href=#ref-CR79 id=ref-link-section-d166511592e450>2006</a>). Thus, other investigators choose single words as stimulus material as they do not “dilute” the characteristics of a specific emotion (Hammerschmidt and Juergens, <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2007" title="Hammerschmidt K, Juergens U (2007) Acoustical correlates of affective prosody. J Voice 21:531–540. 
                  https://doi.org/10.1016/j.jvoice.2006.03.002
                  
                " href=#ref-CR32 id=ref-link-section-d166511592e453>2007</a>). To examine the role of semantics in prosody processing, previous research manipulated both these aspects orthogonally and then compared the conditions in which semantics and prosody are emotionally congruent or incongruent (Kotz and Paulmann, <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2007" title="Kotz SA, Paulmann S (2007) When emotional prosody and semantics dance cheek to cheek: ERP evidence. Brain Res 1151:107–118. 
                  https://doi.org/10.1016/j.brainres.2007.03.015
                  
                " href=#ref-CR52 id=ref-link-section-d166511592e456>2007</a>). The reported results across studies indicated that specific emotions are more easily recognized if semantic information is available even if semantics are irrelevant to the given task (e.g., Ben-David et al., <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2016" title="Ben-David BM, Multani N, Shakuf V et al. (2016) Prosody and semantics are separate but not separable channels in the perception of emotional speech: test for rating of emotions in speech. J Speech Lang Hear Res 59:1–18. 
                  https://doi.org/10.1044/2015_jslhr-h-14-0323
                  
                " href=#ref-CR9 id=ref-link-section-d166511592e459>2016</a>; Paulmann and Kotz, <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2008" title="Paulmann S, Kotz SA (2008) An ERP investigation on the temporal dynamics of emotional prosody and emotional semantics in pseudo- and lexical sentence context. Brain Lang 105:59–69. 
                  https://doi.org/10.1016/j.bandl.2007.11.005
                  
                " href=#ref-CR67 id=ref-link-section-d166511592e463>2008</a>), while others reported that semantic information might facilitate or interfere with a listener’s judgment about the emotional content of the stimulus when spoken in a congruent or incongruent prosody (e.g., Kotz and Paulmann, <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2007" title="Kotz SA, Paulmann S (2007) When emotional prosody and semantics dance cheek to cheek: ERP evidence. Brain Res 1151:107–118. 
                  https://doi.org/10.1016/j.brainres.2007.03.015
                  
                " href=#ref-CR52 id=ref-link-section-d166511592e466>2007</a>; Mitchell et al., <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2003" title="Mitchell RLC, Elliott R, Barry M et al. (2003) The neural response to emotional prosody, as revealed by functional magnetic resonance imaging. Neuropsychologia 41:1410–1421. 
                  https://doi.org/10.1016/S0028-3932(03)00017-4
                  
                " href=#ref-CR58 id=ref-link-section-d166511592e469>2003</a>; Nygaard and Lunders, <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2002" title="Nygaard L, Lunders E (2002) Resolution of lexical ambiguity by emotional tone of voice. Mem Cogn 30:583–593. 
                  https://doi.org/10.3758/BF03194959
                  
                " href=#ref-CR61 id=ref-link-section-d166511592e472>2002</a>). For instance, Schirmer (<a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2010" title="Schirmer A (2010) Mark my words: tone of voice changes affective word representations in memory. PLoS ONE 5(2):e9080. 
                  https://doi.org/10.1371/journal.pone.0009080
                  
                " href=#ref-CR86 id=ref-link-section-d166511592e475>2010</a>) found that recognition performance is similar for words studied with emotional and neutral prosody. However, compared to words with neutral prosody, words with sad prosody were rated as more negative, while words with happy prosody were rated as more positive. Given that trade-off effects are bound to occur when semantics are congruent or incongruent with prosody (i.e., semantic information might confound prosody of the spoken stimuli), previous research used either neutral sentences/words intoned to convey different emotions (e.g., Hammerschmidt and Juergens, <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2007" title="Hammerschmidt K, Juergens U (2007) Acoustical correlates of affective prosody. J Voice 21:531–540. 
                  https://doi.org/10.1016/j.jvoice.2006.03.002
                  
                " href=#ref-CR32 id=ref-link-section-d166511592e478>2007</a>) or presented the speech-embedded material in a pseudo-language (i.e., an artificially created language devoid of meaning). This last procedure represents a useful way to neutralize or mask the semantic content while retaining the prosodic information (Rigoulot et al., <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2013" title="Rigoulot S, Wassiliwizky E, Pell MD (2013) Feeling backwards? How temporal order in speech affects the time course of vocal emotion recognition. Front Psychol 4:367. 
                  https://doi.org/10.3389/fpsyg.2013.00367
                  
                " href=#ref-CR77 id=ref-link-section-d166511592e482>2013</a>; Banse and Scherer, <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 1996" title="Banse R, Scherer KR (1996) Acoustic profiles in vocal emotion expression. J Pers Soc Psychol 70:614–636. 
                  https://doi.org/10.1037/0022-3514.70.3.614
                  
                " href=#ref-CR4 id=ref-link-section-d166511592e485>1996</a>). Studies on the identification of vocal emotions from pseudo-utterances found overall recognition rates for discrete emotions to be significantly higher than chance (e.g., Pell et al., <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2009" title="Pell MD, Paulmann S, Dara C et al. (2009) Factors in the recognition of vocally expressed emotions: a comparison of four languages. J Phon 37:417–435. 
                  https://doi.org/10.1016/j.wocn.2009.07.005
                  
                " href=#ref-CR73 id=ref-link-section-d166511592e488>2009</a>; Scherer et al., <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2001" title="Scherer KR, Banse R, Wallbott H (2001) Emotion inferences from vocal expression correlate across languages and cultures. J Cross Cult Psychol 32:76–92. 
                  https://doi.org/10.1177/0022022101032001009
                  
                " href=#ref-CR83 id=ref-link-section-d166511592e491>2001</a>) and that semantic neutral sentences and pseudo-sentences elicit similar recognition accuracy rates (e.g., Castro and Lima, <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2010" title="Castro SL, Lima CF (2010) Recognizing emotions in spoken language: a validated set of Portuguese sentences and pseudosentences for research on emotional prosody. Behav Res Methods 42:74–81. 
                  https://doi.org/10.3758/BRM.42.1.74
                  
                " href=#ref-CR15 id=ref-link-section-d166511592e494>2010</a>). However, the analysis of emotional prosody in isolation (i.e., without lexico-semantic content) might not only increase the artifice of the acted emotions but could also lead to poorer decoding accuracy (Parsons et al., <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2014" title="Parsons CE, Young KS, Craske MG et al. (2014) Introducing the Oxford Vocal (OxVoc) Sounds database: a validated set of non-acted affective sounds from human infants, adults, and domestic animals. Front Psychol 5:562. 
                  https://doi.org/10.3389/fpsyg.2014.00562
                  
                " href=#ref-CR64 id=ref-link-section-d166511592e497>2014</a>). Thus, it has been suggested that affective (non-speech) sounds or affect bursts (e.g., laughter, screams) are more reliable stimuli (Juergens et al., <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2018" title="Juergens R, Fischer J, Schacht A (2018) Hot speech and exploding bombs: autonomic arousal during emotion classification of prosodic utterances and affective sounds. Front Psychol 9:228. 
                  https://doi.org/10.3389/fpsyg.2018.00228
                  
                " href=#ref-CR43 id=ref-link-section-d166511592e501>2018</a>) comprising the most ‘natural and ancient language of emotion communication whose expressiveness no words can ever achieve’ (Bostanov and Kotchoubey, <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2004" title="Bostanov V, Kotchoubey B (2004) Recognition of affective prosody: continuous wavelet measures of event-related brain potentials to emotional exclamations. Psychophysiology 41:259–268. 
                  https://doi.org/10.1111/j.1469-8986.2003.00142.x
                  
                " href=#ref-CR10 id=ref-link-section-d166511592e504>2004</a>, p. 259).<p>The discussion whether the recognition of vocal emotions from speech-embedded materials has an advantage over non-speech embedded materials or vice-versa is far from settled. The extraction of acoustic cues from their created materials led, however, to an agreement among researchers that <i>pitch</i>, <i>loudness</i>, <i>tempo</i> and <i>quality</i> (stressed/breathy voice) are the most relevant paralinguistic features that speakers employ when expressing emotions (e.g., Goudbeek and Scherer, <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2010" title="Goudbeek M, Scherer KR (2010) Beyond arousal: valence and potency/control cues in the vocal expression of emotion. J Acoust Soc Am 128:1322–1336. 
                  https://doi.org/10.1121/1.3466853
                  
                " href=#ref-CR29 id=ref-link-section-d166511592e522>2010</a>). A series of statistics on these paralinguistic features has revealed that parameters related to pitch or fundamental frequency (<i>F</i><sub>0</sub>) (e.g., minimum, maximum, mean, jitter), energy/amplitude- (e.g., loudness, shimmer), temporal (e.g., duration) and quality parameters (e.g., harmonics-to-noise ratio [HNR]) are amongst the most important ‘candidates’ for prosodic correlates of emotion in speech (e.g., Juslin and Laukka, <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2003" title="Juslin PN, Laukka P (2003) Communication of emotions in vocal expression and music performance: different channels, same code? Psychol Bull 129:770–814. 
                  https://doi.org/10.1037/0033-2909.129.5.770
                  
                " href=#ref-CR41 id=ref-link-section-d166511592e530>2003</a>; Johnstone and Scherer, <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2000" title="Johnstone T, Scherer KR (2000) Vocal communication of emotion. In: Lewis M, Haviland J (eds) The handbook of emotion, 2nd edn. Guildford, New York, pp. 220–235" href=#ref-CR39 id=ref-link-section-d166511592e533>2000</a>). In their seminal work on the acoustic profiles of vocal emotion expression, Banse and Scherer (<a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 1996" title="Banse R, Scherer KR (1996) Acoustic profiles in vocal emotion expression. J Pers Soc Psychol 70:614–636. 
                  https://doi.org/10.1037/0022-3514.70.3.614
                  
                " href=#ref-CR4 id=ref-link-section-d166511592e536>1996</a>) examined relations between 14 induced emotions and 29 acoustic parameters. Implementing linear discriminant analysis (LDA) with jackknife and cross-validation procedures for the evaluation of classification errors the authors found that, on a 7% chance level assuming a uniform distribution, the overall patterns when categorizing emotions were similar to those of listeners’ accuracy (LDA = 40% jackknife estimate of accuracy; LDA = 25% cross-validation estimate of accuracy; listeners = 48% accuracy). Subsequent studies conducted with different stimulus types (e.g., words, affect bursts, semantically emotional and neutral sentences), with less or larger acoustic parameter sets (from 3 to 40 prosodic features), with the same (i.e., LDA) or other classification methods (e.g., random forest (RF), <i>k</i>-nearest-neighbour classifier) obtained comparable results showing that both, classifiers and listeners perform similarly well when predicting emotion category membership based on the acoustic profiles of their utterances (e.g., Noroozi et al., <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2017" title="Noroozi F, Sapiński T, Kamińska D et al. (2017) Vocal-based emotion recognition using random forests and decision tree. Int J Speech Technol 20:239–246. 
                  https://doi.org/10.1007/s10772-017-9396-2
                  
                " href=#ref-CR60 id=ref-link-section-d166511592e543>2017</a>; Pichora-Fuller et al., <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2016" title="Pichora-Fuller MK, Dupuis K, Van Lieshout P (2016) Importance of F0 for predicting vocal emotion categorization. J Acoust Soc Am 140:3401–3401. 
                  https://doi.org/10.1121/1.4970917
                  
                " href=#ref-CR75 id=ref-link-section-d166511592e546>2016</a>; Sbattella et al., <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2014" title="Sbattella L, Colombo L, Rinaldi C et al. (2014) Extracting emotions and communication styles from prosody. In: da Silva H, Holzinger A, Fairclough S, Majoe D (eds) Physiological computing systems, vol. 8908. Springer, Heidelberg, pp. 21–42" href=#ref-CR80 id=ref-link-section-d166511592e549>2014</a>; Sauter et al., <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2010" title="Sauter DA, Eisner F, Calder AJ et al. (2010) Perceptual cues in nonverbal vocal expressions of emotion. Q J Exp Psychol 63:2251–2272. 
                  https://doi.org/10.1080/17470211003721642
                  
                " href=#ref-CR78 id=ref-link-section-d166511592e552>2010</a>; Castro and Lima, <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2010" title="Castro SL, Lima CF (2010) Recognizing emotions in spoken language: a validated set of Portuguese sentences and pseudosentences for research on emotional prosody. Behav Res Methods 42:74–81. 
                  https://doi.org/10.3758/BRM.42.1.74
                  
                " href=#ref-CR15 id=ref-link-section-d166511592e555>2010</a>; Paulmann et al., <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2008" title="Paulmann S, Pell MD, Kotz SA (2008) How aging affects the recognition of emotional speech. Brain Lang 104:262–269. 
                  https://doi.org/10.1016/j.bandl.2007.03.002
                  
                " href=#ref-CR68 id=ref-link-section-d166511592e558>2008</a>; Hammerschmidt and Juergens, <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2007" title="Hammerschmidt K, Juergens U (2007) Acoustical correlates of affective prosody. J Voice 21:531–540. 
                  https://doi.org/10.1016/j.jvoice.2006.03.002
                  
                " href=#ref-CR32 id=ref-link-section-d166511592e562>2007</a>; Toivanen et al., <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2004" title="Toivanen J, Väyrynen E, Sepännen T (2004) Automatic discrimination of emotion from spoken Finnish. Lang Speech 47:383–412. 
                  https://doi.org/10.1177/00238309040470040301
                  
                " href=#ref-CR92 id=ref-link-section-d166511592e565>2004</a>; Juslin and Laukka, <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2001" title="Juslin PN, Laukka P (2001) Impact of intended emotion intensity on cue utilization and decoding accuracy in vocal expression of emotion. Emotion 1:381–412. 
                  https://doi.org/10.1037//1528-3542.1.4.381
                  
                " href=#ref-CR42 id=ref-link-section-d166511592e568>2001</a>).<p>Together these findings allow the conclusion that prosodic acoustic parameters (among other cues, e.g., semantics) provide listeners with appropriate cues to generally understand the intended emotion and, thus, contribute in a cumulative fashion to the communication and recognition of emotions (Thompson and Balkwill, <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2009" title="Thompson WF, Balkwill LL (2009) Cross-cultural similarities and differences. In: Juslin PN, Sloboda JA (eds) Handbook of music and emotion: theory, research, applications, 1st edn. Oxford University Press, New York, pp. 755–791. 
                  https://doi.org/10.1093/acprof:oso/9780199230143.003.0027
                  
                " href=#ref-CR91 id=ref-link-section-d166511592e574>2009</a>). Nevertheless, it has been argued that using different types of stimuli, different sets of acoustic parameters and implementing various classification methods causes serious difficulties when interpreting the results across studies, endangering the accumulation of empirical evidence (Eyben et al., <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2016" title="Eyben F, Scherer KR, Schuller BW et al. (2016) The Geneva minimalistic acoustic parameter set (GeMAPS) for voice research and affective computing. IEEE Trans Affect Comput 7:190–202. 
                  https://doi.org/10.1109/TAFFC.2015.2457417
                  
                " href=#ref-CR27 id=ref-link-section-d166511592e577>2016</a>). Thus, adopting a baseline set of acoustic parameters and systematically analyzing their influence on emotion recognition accuracy across the various types of vocal stimulus material would improve methodological rigour and increase the reliability of findings (Eyben et al., <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2016" title="Eyben F, Scherer KR, Schuller BW et al. (2016) The Geneva minimalistic acoustic parameter set (GeMAPS) for voice research and affective computing. IEEE Trans Affect Comput 7:190–202. 
                  https://doi.org/10.1109/TAFFC.2015.2457417
                  
                " href=#ref-CR27 id=ref-link-section-d166511592e580>2016</a>; Bąk, <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2016" title="Bąk HK (2016) The state of emotional prosody research—a meta-analysis. In: Bąk HK (ed) Emotional prosody processing for non-native English speakers, 1st edn. Springer International Publishing, pp. 79–112" href=#ref-CR3 id=ref-link-section-d166511592e583>2016</a>; Juslin and Laukka, <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2003" title="Juslin PN, Laukka P (2003) Communication of emotions in vocal expression and music performance: different channels, same code? Psychol Bull 129:770–814. 
                  https://doi.org/10.1037/0033-2909.129.5.770
                  
                " href=#ref-CR41 id=ref-link-section-d166511592e586>2003</a>).<p>As emotion recognition is subject to environmental influences and dictates a great variability in the way individuals interpret emotional messages, it has been argued that metacognition (i.e., the awareness of one’s own knowledge) might impact judgements of accuracy in emotion recognition tasks. Kelly and Metcalfe (<a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2011" title="Kelly KJ, Metcalfe J (2011) Metacognition of emotional face recognition. Emotion 11:896–906. 
                  https://doi.org/10.1037/a0023746
                  
                " href=#ref-CR47 id=ref-link-section-d166511592e593>2011</a>), for instance, investigated whether individuals can accurately predict and assess their performance on two face emotion recognition tasks (i.e., <i>Mind in the Eyes task</i> and <i>Ekman Emotional Expression Multimorph Task</i>). For each emotional expression, participants were asked to predict (1) their future performance in correctly identifying the emotions, i.e., prospective judgements, and (2) the accuracy regarding their confidence in the given responses, i.e., retrospective judgements. Results from the <i>Mind in the Eyes task</i> showed significantly higher scores for retrospective than prospective confidence judgements, however, no significant relationship between these judgements and performance accuracy was found. Even though in the <i>Emotional Expression Multimorph Task</i>, the gamma correlations were slightly greater for retrospective (<i>r</i> = 0.43) than prospective judgements (<i>r</i> = 0.32), the authors found a significant relationship between both types of judgements and performance accuracy. Based on these findings, the authors concluded that individuals who perform better in emotion recognition tasks are also more accurate in their metacognitive assessments. While some studies on the perceptual–acoustic correlates of the confidence that is expressed in the voice of the speaker, indicated that for listeners both linguistic and acoustic-prosodic cues were fundamental when making retrospective judgements about speakers’ mental states (e.g., Jiang and Pell, <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2017" title="Jiang X, Pell MD (2017) The sound of confidence and doubt. Speech Commun 88:106–126. 
                  https://doi.org/10.1016/j.specom.2017.01.011
                  
                " href=#ref-CR37 id=ref-link-section-d166511592e615>2017</a>, <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2014" title="Jiang X, Pell DM (2014) Encoding and decoding confidence information in speech. In: Proceedings of the 7th international conference in speech prosody (social and linguistic speech prosody). pp. 573–576. 
                  http://fastnet.netsoc.ie/sp7/sp7book.pdf
                  
                . Accessed 30 Nov 2018" href=#ref-CR38 id=ref-link-section-d166511592e618>2014</a>; Kimble and Seidel, <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 1991" title="Kimble C, Seidel S (1991) Vocal signs of confidence. J Nonverbal Behav 15:99–105. 
                  https://doi.org/10.1007/BF00998265
                  
                " href=#ref-CR48 id=ref-link-section-d166511592e621>1991</a>; Scherer et al., <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 1973" title="Scherer KR, London H, Wolf J (1973) The voice of confidence: Paralinguistic cues and audience evaluation. J Res Pers 7:31–44. 
                  https://doi.org/10.1016/0092-6566(73)90030-5
                  
                " href=#ref-CR85 id=ref-link-section-d166511592e624>1973</a>), other studies demonstrated that in tasks assessing vocal expressions of emotion, listeners’ confidence increased with stimulus duration (Pell and Kotz, <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2011" title="Pell MD, Kotz SA (2011) On the time course of vocal emotion recognition. PLoS ONE 6(11):e27256. 
                  https://doi.org/10.1371/journal.pone.0027256
                  
                " href=#ref-CR70 id=ref-link-section-d166511592e628>2011</a>). For instance, Rigoulot et al. (<a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2013" title="Rigoulot S, Wassiliwizky E, Pell MD (2013) Feeling backwards? How temporal order in speech affects the time course of vocal emotion recognition. Front Psychol 4:367. 
                  https://doi.org/10.3389/fpsyg.2013.00367
                  
                " href=#ref-CR77 id=ref-link-section-d166511592e631>2013</a>) investigated the time course of vocal emotion recognition employing a modified version of an auditory gating paradigm. Results showed that, independent of stimulus presentation (forward or backward), listeners’ confidence in categorizing the emotions increased significantly with longer gate intervals (i.e., number of syllables). This pattern of results clearly indicates that when assessing the recognition of vocal emotions, duration, among other acoustic parameters (e.g., pitch, loudness), progressively activates emotion-specific knowledge leading to higher accuracy and confidence ratings (for an example on how the recognition of discrete vocal emotion in affect bursts evolves over time, see Schaerlaeken and Grandjean, <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2018" title="Schaerlaeken S, Grandjean D (2018) Unfolding and dynamics of affect bursts decoding in humans. PLoS ONE 13:e0206215. 
                  https://doi.org/10.1371/journal.pone.0206216
                  
                " href=#ref-CR81 id=ref-link-section-d166511592e634>2018</a>). While these findings revealed how much information might be needed for listeners to consciously reflect on and categorize vocally expressed emotions from paralinguistic attributes of speech, there is a lack of direct evidence examining the influence of vocal stimulus type and their related acoustic parameters on emotion recognition and confidence ratings. In the present investigation, we contribute to filling this gap by means of two studies.</p></div></div></section><section data-title="Study 1: Performance accuracy by classification algorithms"><div class=c-article-section id=Sec2-section><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id=Sec2>Study 1: Performance accuracy by classification algorithms</h2><div class=c-article-section__content id=Sec2-content><p>Humans communicate emotions with the voice through prosody and vocal bursts (Cowen et al., <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2019b" title="Cowen AS, Elfenbein HA, Laukka P et al. (2019b) Mapping 24 emotions conveyed by brief human vocalization. Am Psychol 74:698–712. 
                  https://doi.org/10.1037/amp0000399
                  
                " href=#ref-CR22 id=ref-link-section-d166511592e645>2019b</a>). Research has long claimed that certain acoustic features, such as pitch, loudness, tempo or quality and their related parameters (e.g., fundamental frequency, jitter, shimmer, harmonics-to-noise ratio) drive the recognition of emotions from prosody and vocal bursts (e.g., Sauter et al., <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2010" title="Sauter DA, Eisner F, Calder AJ et al. (2010) Perceptual cues in nonverbal vocal expressions of emotion. Q J Exp Psychol 63:2251–2272. 
                  https://doi.org/10.1080/17470211003721642
                  
                " href=#ref-CR78 id=ref-link-section-d166511592e648>2010</a>; Scherer and Baenziger, <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2004" title="Scherer KR, Baenziger T (2004) Emotional expression in prosody: a review and an agenda for future research. In: Bel B, Marlien I (eds) Speech prosody, Nara, Japan, pp. 359–366" href=#ref-CR82 id=ref-link-section-d166511592e651>2004</a>; Banse and Scherer, <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 1996" title="Banse R, Scherer KR (1996) Acoustic profiles in vocal emotion expression. J Pers Soc Psychol 70:614–636. 
                  https://doi.org/10.1037/0022-3514.70.3.614
                  
                " href=#ref-CR4 id=ref-link-section-d166511592e654>1996</a>). Nevertheless, a comparison of findings across different studies on emotion recognition from speech seems an invalid approach as performance accuracy has been argued to essentially depend on the stimulus material and the extracted set of acoustic parameters (e.g., Bąk, <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2016" title="Bąk HK (2016) The state of emotional prosody research—a meta-analysis. In: Bąk HK (ed) Emotional prosody processing for non-native English speakers, 1st edn. Springer International Publishing, pp. 79–112" href=#ref-CR3 id=ref-link-section-d166511592e657>2016</a>; Sauter et al., <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2010" title="Sauter DA, Eisner F, Calder AJ et al. (2010) Perceptual cues in nonverbal vocal expressions of emotion. Q J Exp Psychol 63:2251–2272. 
                  https://doi.org/10.1080/17470211003721642
                  
                " href=#ref-CR78 id=ref-link-section-d166511592e661>2010</a>; Toivanen et al., <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2004" title="Toivanen J, Väyrynen E, Sepännen T (2004) Automatic discrimination of emotion from spoken Finnish. Lang Speech 47:383–412. 
                  https://doi.org/10.1177/00238309040470040301
                  
                " href=#ref-CR92 id=ref-link-section-d166511592e664>2004</a>). Guided by these observations, we extracted a baseline set of acoustic parameters from our stimuli datasets and employed two procedures to capture the psychophysical properties of these measurements.</p></div></div></section><section data-title=Method><div class=c-article-section id=Sec3-section><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id=Sec3>Method</h2><div class=c-article-section__content id=Sec3-content><h3 class=c-article__sub-heading id=Sec4>Stimulus material and acoustic analyses</h3><p>One thousand thirty-eight emotional expressions spoken in an <i>angry</i>, <i>disgusted</i>, <i>fearful</i>, <i>happy</i>, <i>neutral</i>, <i>sad</i> and <i>surprised</i> tone of voice were sampled from established speech corpora or from researchers that developed their own stimulus materials. Table <a data-track=click data-track-label=link data-track-action="table anchor" href=#Tab1>1</a> provides a brief summary of the datasets [for further details on stimuli selection (i.e., inclusion criteria and normalization procedures) see Lausen and Schacht, <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2018" title="Lausen A, Schacht A (2018) Gender differences in the recognition of vocal emotions. Front Psychol 9:882. 
                  https://doi.org/10.3389/fpsyg.2018.00882
                  
                " href=#ref-CR54 id=ref-link-section-d166511592e704>2018</a>].<div class=c-article-table data-test=inline-table data-container-section=table id=table-1><figure><figcaption class=c-article-table__figcaption><b id=Tab1 data-test=table-caption>Table 1 Summary of the speech and non-speech datasets.</b></figcaption><div class="u-text-right u-hide-print"><a class=c-article__pill-button data-test=table-link data-track=click data-track-action="view table" data-track-label=button rel=nofollow href=https://www.nature.com/articles/s41599-020-0499-z/tables/1 aria-label="Full size table 1"><span>Full size table</span><svg width=16 height=16 focusable=false role=img aria-hidden=true class=u-icon><use xmlns:xlink=http://www.w3.org/1999/xlink xlink:href=#global-icon-chevron-right></use></svg></a></div></figure></div><p>The stimulus material was analysed for <i>frequency</i>-related parameters (mean fundamental frequency (<i>F</i><sub>0</sub>), minimum <i>F</i><sub>0</sub>, maximum <i>F</i><sub>0</sub>, standard deviation <i>F</i><sub>0</sub>, jitter), <i>energy/amplitude</i>-related parameters (shimmer, amplitude [dB], peak amplitude, mean HNR, maximum HNR, standard deviation HNR) and <i>temporal features</i> (duration, peak time) using <i>GSU Praat Tools</i> script packages developed by Owren (<a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2008" title="Owren MJ (2008) GSU Praat tools: scripts for modifying and analyzing sounds using Praat acoustics software. Behav Res Methods 40:822–829. 
                  https://doi.org/10.3758/BRM.40.3.822
                  
                " href=#ref-CR62 id=ref-link-section-d166511592e1100>2008</a>), which allows batch processing during measurement (for details on the processing of acoustic parameters see supplementary material). Following the procedures of Goudbeek and Scherer (<a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2010" title="Goudbeek M, Scherer KR (2010) Beyond arousal: valence and potency/control cues in the vocal expression of emotion. J Acoust Soc Am 128:1322–1336. 
                  https://doi.org/10.1121/1.3466853
                  
                " href=#ref-CR29 id=ref-link-section-d166511592e1104>2010</a>), Sauter et al. (<a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2010" title="Sauter DA, Eisner F, Calder AJ et al. (2010) Perceptual cues in nonverbal vocal expressions of emotion. Q J Exp Psychol 63:2251–2272. 
                  https://doi.org/10.1080/17470211003721642
                  
                " href=#ref-CR78 id=ref-link-section-d166511592e1107>2010</a>) and Juslin and Laukka (<a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2001" title="Juslin PN, Laukka P (2001) Impact of intended emotion intensity on cue utilization and decoding accuracy in vocal expression of emotion. Emotion 1:381–412. 
                  https://doi.org/10.1037//1528-3542.1.4.381
                  
                " href=#ref-CR42 id=ref-link-section-d166511592e1110>2001</a>) the measurements were made over the entire utterances, across all speakers and all items of the same type of stimulus.<h3 class=c-article__sub-heading id=Sec5>Statistical analysis</h3><p>A LDA was then performed for each type of stimulus separately and across all stimuli in both groups (i.e., <i>Group Words</i> and <i>Group Sentences</i>; for details see “Methods” section Study 2) to determine the optimal combination of the 13 above-mentioned acoustic parameters for predicting emotion category membership. In the analysis, acoustic measurements served as independent variables while the dependent variable was the intended emotional category. As the set of acoustic parameters was not very large, no feature selection method (e.g., stepwise analysis) was used to reduce the number of parameters. LDA is optimal if the acoustic parameters have a multivariate normal distribution with different means for each emotion and identical variance matrices for all emotions. However, if the underlying multivariate structure is more complex, other classification algorithms have been suggested to yield better performance (James et al., <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2013" title="James G, Witten D, Hastie T et al. (2013) An introduction to statistical learning with applications in R. In: Cassella G, Fienberg S, Olkin I (eds) Springer texts in statistics. Springer, New York, pp. 303–332" href=#ref-CR36 id=ref-link-section-d166511592e1127>2013</a>). To assess whether our LDA model shows better predictive performance than other classification techniques we implemented RF as an additional classification algorithm. This ensemble classification methodology, which combines a large number of decision trees using different sets of predictors at each node of the trees, was argued to be a more robust alternative to discriminant analysis or multinomial regression as it allows a selection of the important potential predictors among a large number of variables with complex interactions (Anikin and Lima, <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2018" title="Anikin A, Lima CF (2018) Perceptual and acoustic differences between authentic and acted nonverbal emotional vocalizations. Q J Exp Psychol 71:622–641. 
                  https://doi.org/10.1080/17470218.2016.1270976
                  
                " href=#ref-CR2 id=ref-link-section-d166511592e1130>2018</a>; Breiman, <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2001" title="Breiman L (2001) Random forests. Mach Learn 45:5–32. 
                  https://doi.org/10.1023/A:1010933404324
                  
                " href=#ref-CR11 id=ref-link-section-d166511592e1133>2001</a>; see also supplementary material for more details on RF).<p>These two classification methods were chosen due to their easy implementation and parametrization. LDA is currently the most used method for emotion classification in vocal stimuli (e.g., Sauter et al., <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2010" title="Sauter DA, Eisner F, Calder AJ et al. (2010) Perceptual cues in nonverbal vocal expressions of emotion. Q J Exp Psychol 63:2251–2272. 
                  https://doi.org/10.1080/17470211003721642
                  
                " href=#ref-CR78 id=ref-link-section-d166511592e1139>2010</a>; Hammerschmidt and Juergens, <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2007" title="Hammerschmidt K, Juergens U (2007) Acoustical correlates of affective prosody. J Voice 21:531–540. 
                  https://doi.org/10.1016/j.jvoice.2006.03.002
                  
                " href=#ref-CR32 id=ref-link-section-d166511592e1142>2007</a>; Banse and Scherer, <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 1996" title="Banse R, Scherer KR (1996) Acoustic profiles in vocal emotion expression. J Pers Soc Psychol 70:614–636. 
                  https://doi.org/10.1037/0022-3514.70.3.614
                  
                " href=#ref-CR4 id=ref-link-section-d166511592e1145>1996</a>). Multivariate normality and homoscedasticity of the data are assumed for this method and, therefore, data transformations must often be carried out to reach or to come close to these requirements (Seber, <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 1984" title="Seber GAF (1984) Multivariate observations. John Wiley &amp; Sons" href=#ref-CR89 id=ref-link-section-d166511592e1148>1984</a>). A potential drawback of this method includes a tendency towards overfitting (Dixon and Brereton, <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2009" title="Dixon SJ, Brereton RG (2009) Comparison of performance of five common classifiers represented as boundary methods: Euclidean distance to centroids, linear discriminant analysis, quadratic discriminant analysis, learning vector quantization and support vector machines, as dependent on data structure. Chemom Intell Lab Syst 95:1–17. 
                  https://doi.org/10.1016/j.chemolab.2008.07.010
                  
                " href=#ref-CR25 id=ref-link-section-d166511592e1151>2009</a>). In contrast, RF has previously been used for discrimination among vocal emotions by only few studies (e.g., Anikin and Lima, <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2018" title="Anikin A, Lima CF (2018) Perceptual and acoustic differences between authentic and acted nonverbal emotional vocalizations. Q J Exp Psychol 71:622–641. 
                  https://doi.org/10.1080/17470218.2016.1270976
                  
                " href=#ref-CR2 id=ref-link-section-d166511592e1155>2018</a>; Noroozi et al., <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2017" title="Noroozi F, Sapiński T, Kamińska D et al. (2017) Vocal-based emotion recognition using random forests and decision tree. Int J Speech Technol 20:239–246. 
                  https://doi.org/10.1007/s10772-017-9396-2
                  
                " href=#ref-CR60 id=ref-link-section-d166511592e1158>2017</a>). While other machine-learning methods are available, we chose RF because it represents an adequate analysis to LDA, requires no distributional assumption of the dataset and has no possibility of overfitting (Breiman, <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2001" title="Breiman L (2001) Random forests. Mach Learn 45:5–32. 
                  https://doi.org/10.1023/A:1010933404324
                  
                " href=#ref-CR11 id=ref-link-section-d166511592e1161>2001</a>). The two classification methods were compared by the estimated classification errors using <i>10-fold cross-validation</i>.<p>The data was analysed using the R language and environment for statistical computing and graphics version 3.4.3 (R Core Team, <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2017" title="R Core Team (2017) R: a language and environment for statistical computing. R Foundation for Statistical Computing, Vienna" href=#ref-CR76 id=ref-link-section-d166511592e1170>2017</a>) and the integrated environment <i>R-Studio version 1.0.153</i> (used packages: <i>lda</i>; <i>randomForest</i>; <i>ipred</i>).</p></div></div></section><section data-title=Results><div class=c-article-section id=Sec6-section><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id=Sec6>Results</h2><div class=c-article-section__content id=Sec6-content><p>The results obtained from LDA showed that the vast majority of variance was accounted by the first two LD functions (accounted variance ranging from 37.80% to 73.96% for LD1 and from 14.86% to 32.50% for LD2). In supplementary material are presented the complete tables for the correlations between acoustic parameters and all LD functions as well as the accounted variance for each function [Table <a data-track=click data-track-label=link data-track-action="table anchor" href=#Tab1>S1</a> (A<sub>I</sub> to A<sub>X</sub><sub>I</sub>)]. Figure <a data-track=click data-track-label=link data-track-action="figure anchor" href=#Fig1>1</a> illustrates how the scores of the two LD functions separate the emotional categories for each- and across all stimuli.<div class="c-article-section__figure js-c-reading-companion-figures-item" data-test=figure data-container-section=figure id=figure-1 data-title="Linear discriminant analysis for each- and across all stimuli."><figure><figcaption><b id=Fig1 class=c-article-section__figure-caption data-test=figure-caption-text>Fig. 1: Linear discriminant analysis for each- and across all stimuli.</b></figcaption><div class=c-article-section__figure-content><div class=c-article-section__figure-item><a class=c-article-section__figure-link data-test=img-link data-track=click data-track-label=image data-track-action="view figure" href=https://www.nature.com/articles/s41599-020-0499-z/figures/1 rel=nofollow><picture><img aria-describedby=Fig1 src=data:null;base64, alt="figure 1" loading=lazy width=685 height=754 srcset sizes></picture></a></div><div class=c-article-section__figure-description data-test=bottom-caption id=figure-1-desc><p>Each stimulus is plotted according to its scores for the discriminant function 1 and function 2. The squares represent the group means on each of the discriminant functions. On <i>x-</i> and <i>y-axis</i> are displayed the parameters that most strongly correlated with the first two linear discriminant functions (<i>Min.</i> = minimum; <i>Max.</i> = maximum; <i>M</i> = mean; <i>SD</i> = standard deviation; <i>HNR</i> = harmonics-to-noise ratio; <i>F</i><sub>0</sub> = fundamental frequency; <i>Dur</i>. = duration; <i>Amp.</i> = amplitude).</p></div></div><div class="u-text-right u-hide-print"><a class=c-article__pill-button data-test=article-link data-track=click data-track-label=button data-track-action="view figure" href=https://www.nature.com/articles/s41599-020-0499-z/figures/1 data-track-dest="link:Figure1 Full size image" aria-label="Full size image figure 1" rel=nofollow><span>Full size image</span><svg width=16 height=16 focusable=false role=img aria-hidden=true class=u-icon><use xmlns:xlink=http://www.w3.org/1999/xlink xlink:href=#global-icon-chevron-right></use></svg></a></div></figure></div><p>Comparisons between RF and LDA revealed that the error rates were overall smaller by RF than LDA when predicting emotion category membership across all 1038 stimuli. Specifically, error rates were reduced by 23.11% across all stimuli types, by 11.82% across all stimuli in <i>Group Words</i> and by 19.53% across all stimuli in <i>Group Sentences</i>. Table <a data-track=click data-track-label=link data-track-action="table anchor" href=#Tab2>2</a> displays the error rates for both classification methods and the differences between RF and LDA error rates relative to the error rates of LDA.<div class=c-article-table data-test=inline-table data-container-section=table id=table-2><figure><figcaption class=c-article-table__figcaption><b id=Tab2 data-test=table-caption>Table 2 Linear discriminant analysis (LDA) and random forest (RF) 10-fold cross-validation classification error rates for predicting vocal stimuli emotional category membership.</b></figcaption><div class="u-text-right u-hide-print"><a class=c-article__pill-button data-test=table-link data-track=click data-track-action="view table" data-track-label=button rel=nofollow href=https://www.nature.com/articles/s41599-020-0499-z/tables/2 aria-label="Full size table 2"><span>Full size table</span><svg width=16 height=16 focusable=false role=img aria-hidden=true class=u-icon><use xmlns:xlink=http://www.w3.org/1999/xlink xlink:href=#global-icon-chevron-right></use></svg></a></div></figure></div></div></div></section><section data-title=Discussion><div class=c-article-section id=Sec7-section><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id=Sec7>Discussion</h2><div class=c-article-section__content id=Sec7-content><p>Our results showed that LDA was able to correctly classify the indicated emotion category of 42% for all stimuli types unveiling specific constellations of predictors for each emotion and stimulus set with cross-validation estimates of accuracy, ranging from 41% for <i>lexical sentences</i> to 61% for <i>Anna</i> stimuli. The results of this analysis compare well with previous work reporting accuracy rates for their stimulus materials between 40% and 57% (e.g., Juergens et al., <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2013" title="Juergens R, Drolet M, Pirow R et al. (2013) Encoding conditions affect recognition of vocally expressed emotions across cultures. Front Psychol 4:111. 
                  https://doi.org/10.3389/fpsyg.2013.00111
                  
                " href=#ref-CR45 id=ref-link-section-d166511592e1651>2013</a>; Sauter et al., <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2010" title="Sauter DA, Eisner F, Calder AJ et al. (2010) Perceptual cues in nonverbal vocal expressions of emotion. Q J Exp Psychol 63:2251–2272. 
                  https://doi.org/10.1080/17470211003721642
                  
                " href=#ref-CR78 id=ref-link-section-d166511592e1654>2010</a>; Castro and Lima, <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2010" title="Castro SL, Lima CF (2010) Recognizing emotions in spoken language: a validated set of Portuguese sentences and pseudosentences for research on emotional prosody. Behav Res Methods 42:74–81. 
                  https://doi.org/10.3758/BRM.42.1.74
                  
                " href=#ref-CR15 id=ref-link-section-d166511592e1657>2010</a>; Hammerschmidt and Jürgens, <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2007" title="Hammerschmidt K, Juergens U (2007) Acoustical correlates of affective prosody. J Voice 21:531–540. 
                  https://doi.org/10.1016/j.jvoice.2006.03.002
                  
                " href=#ref-CR32 id=ref-link-section-d166511592e1661>2007</a>; Banse and Scherer, <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 1996" title="Banse R, Scherer KR (1996) Acoustic profiles in vocal emotion expression. J Pers Soc Psychol 70:614–636. 
                  https://doi.org/10.1037/0022-3514.70.3.614
                  
                " href=#ref-CR4 id=ref-link-section-d166511592e1664>1996</a>). By implementing RF as an additional classification method and using the same acoustic predictors, we observed that RF achieved a cross-validation classification accuracy across all stimuli, which was 32% relative higher than the accuracy of the LDA model. This result is in line with the findings reported by Noroozi et al. (<a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2017" title="Noroozi F, Sapiński T, Kamińska D et al. (2017) Vocal-based emotion recognition using random forests and decision tree. Int J Speech Technol 20:239–246. 
                  https://doi.org/10.1007/s10772-017-9396-2
                  
                " href=#ref-CR60 id=ref-link-section-d166511592e1667>2017</a>), who investigated whether RF shows better predictive performance than deep neural networks (DNN) or more established techniques such as LDA, based on a set of 14 acoustic parameters extracted from <i>Surrey Audio–Visual Expressed Emotion</i> database. Their results showed that on average RF recognition rate was 26% higher relative to LDA and 11% higher compared to DNN. Although the comparison between different classification methods highlights that RF outperforms techniques such as LDA (Noroozi et al., <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2017" title="Noroozi F, Sapiński T, Kamińska D et al. (2017) Vocal-based emotion recognition using random forests and decision tree. Int J Speech Technol 20:239–246. 
                  https://doi.org/10.1007/s10772-017-9396-2
                  
                " href=#ref-CR60 id=ref-link-section-d166511592e1673>2017</a>), we would like to note that results from both classification analyses demonstrated that acoustic measurements alone provide sufficient information to discriminate successfully between stimuli from different emotional categories.</p></div></div></section><section data-title="Study 2: Emotion recognition and confidence ratings by stimulus type and emotion categories"><div class=c-article-section id=Sec8-section><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id=Sec8>Study 2: Emotion recognition and confidence ratings by stimulus type and emotion categories</h2><div class=c-article-section__content id=Sec8-content><p>Considering that many perceptual experiments (e.g., Noroozi et al., <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2017" title="Noroozi F, Sapiński T, Kamińska D et al. (2017) Vocal-based emotion recognition using random forests and decision tree. Int J Speech Technol 20:239–246. 
                  https://doi.org/10.1007/s10772-017-9396-2
                  
                " href=#ref-CR60 id=ref-link-section-d166511592e1685>2017</a>; Pichora-Fuller et al., <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2016" title="Pichora-Fuller MK, Dupuis K, Van Lieshout P (2016) Importance of F0 for predicting vocal emotion categorization. J Acoust Soc Am 140:3401–3401. 
                  https://doi.org/10.1121/1.4970917
                  
                " href=#ref-CR75 id=ref-link-section-d166511592e1688>2016</a>; Sbattella et al., <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2014" title="Sbattella L, Colombo L, Rinaldi C et al. (2014) Extracting emotions and communication styles from prosody. In: da Silva H, Holzinger A, Fairclough S, Majoe D (eds) Physiological computing systems, vol. 8908. Springer, Heidelberg, pp. 21–42" href=#ref-CR80 id=ref-link-section-d166511592e1691>2014</a>; Sauter et al., <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2010" title="Sauter DA, Eisner F, Calder AJ et al. (2010) Perceptual cues in nonverbal vocal expressions of emotion. Q J Exp Psychol 63:2251–2272. 
                  https://doi.org/10.1080/17470211003721642
                  
                " href=#ref-CR78 id=ref-link-section-d166511592e1694>2010</a>), including our first study, have demonstrated that automatic classification of simulated vocal emotions is possible with a reasonably high accuracy rate, the general finding across studies has so far been that the automatic classification of emotions is considerably less successful than human classification performance (for a review see Toivanen et al., <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2004" title="Toivanen J, Väyrynen E, Sepännen T (2004) Automatic discrimination of emotion from spoken Finnish. Lang Speech 47:383–412. 
                  https://doi.org/10.1177/00238309040470040301
                  
                " href=#ref-CR92 id=ref-link-section-d166511592e1697>2004</a>). As outlined in the introduction, research on metacognition has related this skill to vital aspects of socio-emotional processes and has reliably shown that confidence judgements are higher when given after rather than before a response to an emotion recognition task. In this emerging science, however, it is not well-understood how different types of vocal stimuli and their acoustic attributes influence listeners’ recognition of emotions and confidence ratings. Such data would allow a more differentiated assessment of the factors assumed to impact both emotion recognition accuracy and a person’s confidence in a given response. We therefore sought in our second study to analyse: (1) whether the automatic classification of vocal emotions is less successful than human classification performance (by taking as a benchmark the overall ‘best’ performing classification method from our first study, i.e., RF); (2) which vocal stimulus types are best predicted by the acoustic parameters and which acoustic parameters are best suited for which stimulus types; (3) whether listeners’ performance accuracy and confidence ratings vary across certain types of stimuli and specific emotion categories; (4) if correct recognition of emotions is related to increased confidence judgements.</p></div></div></section><section data-title=Method><div class=c-article-section id=Sec9-section><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id=Sec9>Method</h2><div class=c-article-section__content id=Sec9-content><h3 class=c-article__sub-heading id=Sec10>Participants</h3><p>Two-hundred ninety participants (143 females, 147 males; age range = 18–36 years) completed the study after responding to advertisements posted on social media (e.g., Facebook) or to flyers distributed across the university campus. Participants averaged 23.83 years in age (SD = 3.73) with 62% having completed a general qualification for university entrance, 25% a bachelor degree, 12% a master degree and 1% a general certificate of secondary education. To reduce the length of the experiment, participants were allocated to two groups of equal size. One group listened to words and pseudo-words (Group Words: <i>n</i> = 145, <i>M</i><sub>age</sub> = 24.00, SD<sub>age</sub> = 3.67), while the other group listened to affect bursts, sentences and pseudo-sentences (Group Sentences: <i>n</i> = 145, <i>M</i><sub>age</sub> = 23.66, SD<sub>age</sub> = 3.80). No significant age difference between the two groups was observed (<i>t</i><sub>(288)</sub> = 0.786; <i>p</i> = 0.432; CI<sub>95%</sub> = [−0.52; 1.21]). All participants were native speakers of German and reported no hearing difficulties.<h3 class=c-article__sub-heading id=Sec11>Procedure and experimental task</h3><p>Up to four participants were invited to each experimental session, which lasted ~60 min. At arrival, the experimenter informed the participants about the aim of the study, i.e., to validate a set of auditory stimuli with emotional content. Prior to formal testing, participants signed a consent form and completed a short demographic questionnaire concerning age, gender and education level. Participants were informed that all stimuli would be presented only once, the number of presented emotions might vary from the number of categories given as possible choices, and some of the stimuli were not supposed to carry any semantic meaning and might sound ‘foreign’. After these instructions and completion of ten practice trials, participants started the main experiment, presented via <i>Presentation</i> software (Version 14.1, Neurobehavioral Systems Inc., Albany, CA). Stimuli were presented to the participants binaurally with <i>Bayerdynamic DT 770 PRO</i> headphones plugged-in the tower box of a <i>Dell OptiPlex 780 SFF Desktop PC Computer</i>. To ensure equal physical volume of stimulus presentation across the four PCs, we measured the sound level meters of the ten practice stimuli with a professional sound level meter (Norsonic Nor140, <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2017" title="Norsonic Nor140 (2017) Instruction manual. Lierskogen, Norway. 
                  https://www.campbell-associates.co.uk/norsonic-140-sound-level-meter
                  
                " href=#ref-CR97 id=ref-link-section-d166511592e1757>2017</a>). No significant difference in volume intensity was observed (<i>F</i><sub>(3,27)</sub> &lt; 1). Following each stimulus presentation listeners rendered two judgements. First, they classified which emotion was being expressed by the speaker from a list of seven categories presented on the computer screen. To assess metacognition, this rating was followed by a 7-point rating scale on the screen to estimate their confidence in the preceding response (1 = not at all confident; 7 = extremely confident). Each trial began with a white fixation-cross presented on a grey screen, which was shown until participants’ response had been recorded. The presentation of the stimuli was initiated by pressing the <i>Enter-key</i>. The auditory stimulus was then presented alongside the fixation cross. The responses were made using the marked computer keyboard (<i>Z</i> to <i>M</i> for the emotion judgements, which were labelled corresponding to the emotion categories, and 1–7 for confidence). There was no time limit for emotion judgements or confidence ratings. At the end of each block a visual message in the centre of the screen instructed participants to take a break if they wished to or to press the <i>Spacebar</i> to proceed with the next block. The set of stimuli in <i>Group Words</i> was split into three blocks (i.e., <i>Anna</i>, <i>Pseudo-words</i>, and <i>Nouns</i>), while in <i>Group Sentences</i> the set of stimuli was divided into four blocks (i.e., <i>Pseudo-sentences</i>, <i>Lexical Sentences</i>, <i>Neutral sentences</i>, and <i>Affect bursts</i>). The order of blocks and of the stimuli within each block were randomized. Blocks were separated by a break of self-determined duration. The reimbursement of participants consisted of 8€ or course credit.<p>The study has been approved by the Ethical Committee of the Georg-Elias- Mueller-Institute of Psychology, University of Goettingen, Germany (<i>number 149</i>) and conducted in accordance with the ethical principles formulated in the <i>Declaration of Helsinki</i> (<a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2013" title="World Medical Association (2013) World Medical Association Declaration of Helsinki: ethical principles form medical research involving human subjects. JAMA 310:2191–2194. 
                  https://doi.org/10.1001/jama.2013.281053
                  
                " href=#ref-CR96 id=ref-link-section-d166511592e1815>2013</a>).<h3 class=c-article__sub-heading id=Sec12>Study design and power analysis</h3><p>To assess listeners’ judgements of emotions and confidence in their judgements a <i>within-subjects design</i> was fitted for <i>Group Words</i> and <i>Group Sentences</i>. The design was balanced for emotion categories in each stimulus type. Independent within-subject factors were <i>stimuli types</i>, <i>acoustic parameters</i> and <i>emotion categories</i>. Dependent variables were <i>emotion recognition</i> and <i>confidence ratings</i>. To assess whether we had enough power to answer our research questions, an <i>approximate correlation power analysis</i> was calculated and, Bonferroni corrected for the 13 acoustic parameters. A sample size of 145 participants per group with a minimum set of stimuli per participant (i.e., 70) allowed us to detect correlations of <i>r</i> = 0.037 with a type I error rate of 5% and power 80%. To describe the power to detect differences between emotion categories and stimulus types an <i>approximate Tukey’s multiple pairwise comparisons</i> power analysis was computed. Assuming a minimum set of 10 stimuli for each emotion category and a sample size of 145 participants per group allowed us to detect a difference of 0.044 for recognition probability at 0.80 with a type I error rate of 5% and power 80% (for further details on statistical power calculations see supplementary material).<h3 class=c-article__sub-heading id=Sec13>Statistical analysis</h3><p>The data was analysed by <i>generalized linear models</i> (quasi-binomial logistic regression) for the binary response variable emotion recognition and by <i>linear models</i> for the response variable confidence ratings. To find a reduced model that best explains the data on the 13 acoustic parameters for the two dependent variables a <i>backward stepwise variable selection</i> (R function <i>step</i>) was conducted in a generalized linear model (binomial logistic regression) for the binary response variable emotion recognition. The dispersion parameter of the quasi-binomial model and the nominal variable participants accounted for dependencies caused by repeated measurements within the participants.<p>In the global models, <i>stimulus types</i>, <i>acoustic parameters</i> and <i>emotions</i> were included as predictor variables. Participants, emotions, and stimulus types were fitted as nominal variables and acoustic parameters as quantitative variables. The order of the acoustic parameters in the models was determined by importance in a <i>backward stepwise variable selection</i>, that is in descending order starting with the acoustic parameter that explained most of the deviance. Conditional models were fitted for each stimulus type to account for interactions between stimulus types, emotions and acoustic parameters, since differences between fitted parameters of the models can be interpreted in terms of interactions (Hothorn et al., <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2008" title="Hothorn T, Bretz F, Westfall P (2008) Simultaneous inference in general parametric models. Biom J 50:346–363. 
                  https://doi.org/10.1002/bimj.200810425
                  
                " href=#ref-CR34 id=ref-link-section-d166511592e1895>2008</a>). The relation between confidence ratings and emotion recognition was analysed by a linear model with the response variable confidence ratings and the predictor variables stimulus types and emotion recognition. Chi-square tests of the deviance analysis were used to analyse effects of predictor variables. In the quasi-binomial logistic regression, odds ratios (ORs) were used to compare emotion categories as well as stimulus types. Confidence ratings of the linear model were compared by calculating the differences of the means. Tukey’s method of multiple pairwise comparisons was used to compute simultaneous 95% confidence intervals for both, OR and mean differences.<p>For the descriptive analysis of the data the following calculations were carried out: relative frequencies, confusion matrices, classification errors by RF and listeners’ judgements of emotion categories, confidence intervals by binomial test and Wagner’s (<a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 1993" title="Wagner HL (1993) On measuring performance in category judgement studies of nonverbal behaviour. J Nonverbal Behav 17:3–28. 
                  https://doi.org/10.1007/BF00987006
                  
                " href=#ref-CR93 id=ref-link-section-d166511592e1901>1993</a>) unbiased hit rate (<i>H</i><sub>u</sub>), which is the rate of correctly identified stimuli multiplied by the rate of correct judgements of the stimuli. The data was analysed using the R language and environment for statistical computing and graphics version 3.4.3 (R Core Team, <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2017" title="R Core Team (2017) R: a language and environment for statistical computing. R Foundation for Statistical Computing, Vienna" href=#ref-CR76 id=ref-link-section-d166511592e1908>2017</a>) and the integrated environment R-Studio version 1.0.153 (used packages: <i>pwr</i>; <i>MASS</i>; <i>multcomp</i>; <i>mvtnorm; ggplot2</i>; <i>stats {binom.test}</i>).</p></div></div></section><section data-title=Results><div class=c-article-section id=Sec14-section><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id=Sec14>Results</h2><div class=c-article-section__content id=Sec14-content><h3 class=c-article__sub-heading id=Sec15>Human classification performance vs. RF</h3><p>The accuracy percentage of correctly identified emotions, Wagner’s unbiased hit rate, the comparisons in classification errors between listeners’ judgements of emotion categories and RF algorithm, as well as, the <sub>95%</sub>CI of the exact binomial test are displayed in Table <a data-track=click data-track-label=link data-track-action="table anchor" href=#Tab3>3</a>. Figure <a data-track=click data-track-label=link data-track-action="supplementary material anchor" href=#MOESM1>S1A, B</a> in supplementary material displays listeners’ recognition accuracy by stimulus type and emotions. As it can be observed from these analyses, listeners most frequently misclassified happy for surprise (<i>Anna</i>, <i>nouns</i>, <i>affect bursts</i>, <i>neutral sentences</i>, <i>pseudo-sentences</i>), and fear for sadness (<i>Anna</i>, <i>pseudo-words</i>, <i>semantic negative nouns</i>, <i>semantic positive nouns</i>, <i>lexical sentences</i>, <i>pseudo-sentences</i>). In <i>lexical</i>- and <i>pseudo-sentences</i>, however, participants often mistook surprise for happy, whereas the sad tone of voice was frequently misclassified as neutral (<i>pseudo-words</i>, <i>nouns</i>, <i>lexical sentences</i>). Although generally well-recognized, utterances spoken in an angry tone of voice were often mistaken for surprise (<i>affect bursts</i>, <i>lexical</i>- and <i>pseudo-sentences</i>, <i>pseudo-words</i>) and disgust (<i>nouns</i>), while for neutral and disgusted prosody no clear error pattern emerged [e.g., some utterances spoken in a neutral tone of voice were either mistaken for angry (<i>Anna</i>, <i>nouns</i>) or sad (<i>lexical</i>-, <i>neutral sentences</i>), while utterances spoken in a disgusted tone of voice were misclassified as angry (<i>nouns</i>, <i>affect bursts</i>) or neutral (<i>lexical</i>-, <i>pseudo-sentences</i>)]. Comparing the proportion of classification errors between listeners’ judgements of emotions and RF, one could assume that globally, i.e., across all emotions, humans were significantly better at predicting emotion category membership relative to RF, except for <i>Anna</i> stimuli, where no significant difference was observed. Looking at specific emotion categories, results indicated that in some stimulus sets the RF algorithm significantly outperformed listeners when classifying disgust and sad (<i>pseudo-words</i> and <i>nouns</i>), fear (<i>pseudo-words</i> and <i>neutral sentences</i>), happy (<i>Anna</i>) and surprise (i.e., <i>pseudo-, lexical sentences)</i>.<div class=c-article-table data-test=inline-table data-container-section=table id=table-3><figure><figcaption class=c-article-table__figcaption><b id=Tab3 data-test=table-caption>Table 3 Confusion matrices, unbiased hit rates (<i>H</i><sub>u</sub>), comparisons of classification errors by random forest (RF) and listeners’ judgments of emotion categories and 95%CI of the exact binomial test for listeners classification errors.</b></figcaption><div class="u-text-right u-hide-print"><a class=c-article__pill-button data-test=table-link data-track=click data-track-action="view table" data-track-label=button rel=nofollow href=https://www.nature.com/articles/s41599-020-0499-z/tables/3 aria-label="Full size table 3"><span>Full size table</span><svg width=16 height=16 focusable=false role=img aria-hidden=true class=u-icon><use xmlns:xlink=http://www.w3.org/1999/xlink xlink:href=#global-icon-chevron-right></use></svg></a></div></figure></div><h3 class=c-article__sub-heading id=Sec16>Vocal stimulus types predicted by acoustic parameters</h3><p>The proportion of variance explained by the acoustic measures for emotion recognition and confidence ratings is displayed in Table <a data-track=click data-track-label=link data-track-action="table anchor" href=#Tab4>4</a>.<div class=c-article-table data-test=inline-table data-container-section=table id=table-4><figure><figcaption class=c-article-table__figcaption><b id=Tab4 data-test=table-caption>Table 4 Proportion of variance explained by the acoustic measures for emotion recognition and confidence ratings by stimulus type.</b></figcaption><div class="u-text-right u-hide-print"><a class=c-article__pill-button data-test=table-link data-track=click data-track-action="view table" data-track-label=button rel=nofollow href=https://www.nature.com/articles/s41599-020-0499-z/tables/4 aria-label="Full size table 4"><span>Full size table</span><svg width=16 height=16 focusable=false role=img aria-hidden=true class=u-icon><use xmlns:xlink=http://www.w3.org/1999/xlink xlink:href=#global-icon-chevron-right></use></svg></a></div></figure></div><p>An inspection of Table <a data-track=click data-track-label=link data-track-action="table anchor" href=#Tab4>4</a> indicates that the proportion of explained variance for emotion recognition ranged from 0.06 for neutral sentences to 0.24 for non-speech sounds (affect bursts), while for confidence ratings from 0.02 for semantic positive nouns to 0.07 for affect bursts. In general, the results from the full models indicate that non-speech sounds are best predicted by the acoustic parameters. Listeners’ emotion recognition and confidence ratings were significantly driven by many acoustic parameters and the specific constellation of predictors was unique for each stimulus type. In non-speech sounds, for instance, listeners’ recognition of emotions was predicted by amplitude parameters (peak amplitude, mean and standard deviation HNR), while their confidence in the given answer was explained by temporal parameters (duration). For <i>Anna</i>, <i>semantic neutral nouns</i> and <i>neutral sentences</i>, the recognition of emotions was also best predicted by amplitude-, as well as, by frequency- (minimum <i>F</i><sub>0</sub> for Anna and standard deviation <i>F</i><sub>0</sub> for neutral sentences) and temporal related parameters (duration for Anna and semantic neutral nouns). In contrast, for <i>lexical sentences</i>, <i>semantic positive-</i> and <i>negative nouns</i>, the recognition of emotions was best predicted by temporal- (duration) and frequency related parameters (duration, standard deviation <i>F</i><sub>0</sub> and jitter accounted for more than 1.5% of explained variance). Although in pseudo-sentences emotion recognition was best predicted by temporal features (duration), in pseudo-words this perceptual property had no significant effect. Even though the proportion of explained variance was higher for emotion recognition than for confidence ratings, one could observe that for shorter stimuli (<i>semantic neutral</i>-, <i>negative</i> and <i>positive nouns</i>) listeners confidence in the given answer was, in general, best predicted by temporal features (duration), while in longer stimuli (<i>lexical</i> and <i>pseudo-sentences</i>) by frequency related parameters (mean <i>F</i><sub>0</sub>, minimum <i>F</i><sub>0</sub> and jitter).<h3 class=c-article__sub-heading id=Sec17>Stimuli- and emotion comparisons/confidence predicted by correct emotion recognition</h3><p>The quasi-binomial and linear models revealed that stimuli types (<i>p</i> &lt; 0.001), acoustic parameters (most <i>p</i>-values &lt; 0.001) and emotions (<i>p</i> &lt; 0.001) significantly influenced listeners’ performance accuracy of recognizing emotions and their confidence judgements. Moreover, results showed that listeners’ confidence judgements were significantly affected by the correct identification of emotions (<i>p</i> &lt; 0.001). The corresponding test statistics from the global- (across all stimuli) and conditional models (for each stimulus type separately) are reported in supplementary material [see Tables <a data-track=click data-track-label=link data-track-action="supplementary material anchor" href=#MOESM1>S3 (A, A<sub>I</sub>, A<sub>II</sub>)</a> for Group Words, <i>S3</i> (<i>B</i>, <i>B</i><sub>I</sub>, <i>B</i><sub>II</sub>) for Group Sentences and <i>S3</i> (<i>C</i>, <i>C</i><sub>I</sub>, <i>C</i><sub>II</sub> to <i>K</i>, <i>K</i><sub>I</sub>, <i>K</i><sub>II</sub>) for each stimulus type].<p>Figure <a data-track=click data-track-label=link data-track-action="figure anchor" href=#Fig2>2</a> illustrates the comparisons between stimuli types in both, <i>Group Words</i> and <i>Group Sentences</i>. As indicated by the OR estimates, listeners were significantly more accurate at recognizing emotions in stimuli with a semantic connotation than in those spoken in a language devoid of meaning (<i>pseudo-speech</i>) or expressing a person’s name (<i>Anna</i>), with the latter being worst recognized. No significant differences in recognition accuracy were found in comparisons across the different stimuli types carrying semantic content (Fig. <a data-track=click data-track-label=link data-track-action="figure anchor" href=#Fig2>2a</a>). As indicated by the multiple comparisons of the estimated means, the pattern of the differences in confidence judgements occurred similar to the pattern of recognition accuracies. Listeners, however, were less confident when identifying emotions in semantically meaningful <i>nouns</i> spoken in neutral or positive compared to negative prosody (Fig. <a data-track=click data-track-label=link data-track-action="figure anchor" href=#Fig2>2a<sub>I</sub></a>). After adjusting for emotion recognition in the linear model, confidence ratings were significantly lower for <i>pseudo-words</i> than for <i>Anna</i> stimuli (Fig. <a data-track=click data-track-label=link data-track-action="figure anchor" href=#Fig2>2a<sub>II</sub></a>). The odds of correctly identifying emotions as well as listeners’ confidence judgements were also significantly higher for <i>affect bursts</i> and <i>lexical sentences</i> than for <i>neutral sentences</i> and, lower for <i>lexical sentences</i> when compared to <i>affect bursts</i> (Fig. <a data-track=click data-track-label=link data-track-action="figure anchor" href=#Fig2>2b, b<sub>I</sub></a>). Recognition accuracy and confidence ratings were significantly lower for <i>pseudo-sentences</i> than for <i>affect bursts</i>, <i>lexical-</i> and <i>neutral sentences</i> (Fig. <a data-track=click data-track-label=link data-track-action="figure anchor" href=#Fig2>2b, b<sub>I</sub></a>). This pattern remained similar even after adjusting for emotion recognition in the linear model (Fig. <a data-track=click data-track-label=link data-track-action="figure anchor" href=#Fig2>2b<sub>II</sub></a>).<div class="c-article-section__figure js-c-reading-companion-figures-item" data-test=figure data-container-section=figure id=figure-2 data-title="Comparisons between stimuli types."><figure><figcaption><b id=Fig2 class=c-article-section__figure-caption data-test=figure-caption-text>Fig. 2: Comparisons between stimuli types.</b></figcaption><div class=c-article-section__figure-content><div class=c-article-section__figure-item><a class=c-article-section__figure-link data-test=img-link data-track=click data-track-label=image data-track-action="view figure" href=https://www.nature.com/articles/s41599-020-0499-z/figures/2 rel=nofollow><picture><img aria-describedby=Fig2 src=data:null;base64, alt="figure 2" loading=lazy width=685 height=908 srcset sizes></picture></a></div><div class=c-article-section__figure-description data-test=bottom-caption id=figure-2-desc><p>Emotion recognition <i>odds ratio estimates</i> (OR) for the comparisons between stimuli types are illustrated in panel <b>a</b> and panel <b>b</b>. The <i>linear contrasts</i> (<i>Δ</i>) for confidence ratings are illustrated in panel <b>a</b><sub><b>I</b></sub> and <b>b</b><sub><b>I</b></sub>, while the <i>confidence ratings after adjusting for emotion recognition</i> are displayed in panel <b>a</b><sub><b>II</b></sub> and <b>b</b><sub><b>II</b></sub>. Odds ratio of stimulus 1 (e.g., AB) vs. stimulus 2 (e.g., NS) less than 1 indicate that the recognition probability of stimulus 2 (e.g., NS) is higher than of stimulus 1 (e.g., AB), whereas values greater than 1 vice-versa. If the odds ratio of 1 is covered in the confidence interval, the difference in the recognition probabilities is not significant. Negative differences of confidence ratings of stimulus 1 (e.g., AB) vs. stimulus 2 (e.g., NS) indicate that the confidence ratings of stimulus 2 (e.g., NS) is higher than of stimulus 1 (e.g., AB), whereas positive differences vice-versa. If the difference of zero is covered in the 95%CI, the difference in the confidence ratings is not significant. The significant combinations are highlighted in bold.</p></div></div><div class="u-text-right u-hide-print"><a class=c-article__pill-button data-test=article-link data-track=click data-track-label=button data-track-action="view figure" href=https://www.nature.com/articles/s41599-020-0499-z/figures/2 data-track-dest="link:Figure2 Full size image" aria-label="Full size image figure 2" rel=nofollow><span>Full size image</span><svg width=16 height=16 focusable=false role=img aria-hidden=true class=u-icon><use xmlns:xlink=http://www.w3.org/1999/xlink xlink:href=#global-icon-chevron-right></use></svg></a></div></figure></div><p>The comparison of performance accuracy across the emotion categories showed that in <i>Group Words</i> listeners were significantly less accurate and rated themselves as less confident when identifying emotional expressions spoken in a disgusted, neutral, fearful, happy, sad and surprised tone of voice than when spoken in an angry prosody. Although in <i>Group Sentences</i>, listeners were significantly more accurate at categorizing utterances spoken in a neutral- than in an angry tone of voice, the difference in confidence ratings was significantly lower for neutral than for angry expressions. In addition, results showed that both, recognition accuracy and confidence ratings, were significantly higher when comparing neutral to other emotional prosodies. Figure <a data-track=click data-track-label=link data-track-action="figure anchor" href=#Fig3>3</a> illustrates the comparisons between emotion categories in <i>Group Words</i> and <i>Group Sentences</i>. The pattern of results obtained for the comparisons between emotion categories for each type of stimulus are presented in supplementary material (see Fig. <a data-track=click data-track-label=link data-track-action="supplementary material anchor" href=#MOESM1>S2A, A<sub>I</sub> and A<sub>II</sub></a> and Table <a data-track=click data-track-label=link data-track-action="supplementary material anchor" href=#MOESM1>S4</a> for a schematic overview).<div class="c-article-section__figure js-c-reading-companion-figures-item" data-test=figure data-container-section=figure id=figure-3 data-title="Comparisons between emotion categories in Group Words (GW) and Group Sentences (GS)."><figure><figcaption><b id=Fig3 class=c-article-section__figure-caption data-test=figure-caption-text>Fig. 3: Comparisons between emotion categories in Group Words (GW) and Group Sentences (GS).</b></figcaption><div class=c-article-section__figure-content><div class=c-article-section__figure-item><a class=c-article-section__figure-link data-test=img-link data-track=click data-track-label=image data-track-action="view figure" href=https://www.nature.com/articles/s41599-020-0499-z/figures/3 rel=nofollow><picture><img aria-describedby=Fig3 src=data:null;base64, alt="figure 3" loading=lazy width=685 height=470 srcset sizes></picture></a></div><div class=c-article-section__figure-description data-test=bottom-caption id=figure-3-desc><p>The <i>odds ratio estimates</i> (OR) for the comparisons between emotion categories are illustrated in panel <b>a</b> for GW and panel <b>b</b> for GS, while the <i>linear contrasts</i> (<i>Δ</i>) for confidence ratings are illustrated in panel <b>a</b><sub><b>I</b></sub> for GW and <b>b</b><sub><b>I</b></sub> for GS. Odds ratio of emotion 1 (e.g., <i>disgust</i>) vs. emotion 2 (e.g., <i>angry</i>) less than 1 indicate that the recognition probability of emotion 2 (e.g., <i>angry</i>) is higher than of emotion 1 (e.g., <i>disgust</i>), whereas values greater than 1 vice-versa. If the odds ratio of 1 is covered in the confidence interval, the difference in the recognition probabilities is not significant. Negative differences of confidence ratings of emotion 1 (e.g., <i>disgust</i>) vs. emotion 2 (e.g., <i>angry</i>) indicate that the confidence ratings of emotion 2 (e.g., <i>angry</i>) is higher than of emotion 1 (e.g., <i>disgust</i>), whereas positive differences vice-versa. If the difference of zero is covered in the 95%CI, the difference in the confidence ratings is not significant. The significant combinations are highlighted in bold.</p></div></div><div class="u-text-right u-hide-print"><a class=c-article__pill-button data-test=article-link data-track=click data-track-label=button data-track-action="view figure" href=https://www.nature.com/articles/s41599-020-0499-z/figures/3 data-track-dest="link:Figure3 Full size image" aria-label="Full size image figure 3" rel=nofollow><span>Full size image</span><svg width=16 height=16 focusable=false role=img aria-hidden=true class=u-icon><use xmlns:xlink=http://www.w3.org/1999/xlink xlink:href=#global-icon-chevron-right></use></svg></a></div></figure></div></div></div></section><section data-title=Discussion><div class=c-article-section id=Sec18-section><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id=Sec18>Discussion</h2><div class=c-article-section__content id=Sec18-content><h3 class=c-article__sub-heading id=Sec19>Human classification performance vs. RF</h3><p>Using RF as a benchmark of listeners’ performance accuracy, we observed that despite the fact that RF error patterns were significantly lower for certain emotions than those of listeners, across all emotions the automatic classification of emotions was considerably less successful than listeners’ classification performance. Listeners’ superior performance may be due to the fact that they could draw on a greater number of emotional markers [e.g., intonation patterns, emblems of distinct emotions (i.e., laughter, sighs), valence, arousal] inaccessible to the statistical algorithm, whose emotion category predictions were based on a fully automatic set of acoustic parameters extracted from relatively small learning datasets. Although classification algorithms do not seem to replicate the inference processes of human decoders, they appear to have lower error rates than listeners’ when classifying certain emotions (e.g., disgust) solely based on their acoustic profiles. This has been indicated by previous work (Banse and Scherer, <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 1996" title="Banse R, Scherer KR (1996) Acoustic profiles in vocal emotion expression. J Pers Soc Psychol 70:614–636. 
                  https://doi.org/10.1037/0022-3514.70.3.614
                  
                " href=#ref-CR4 id=ref-link-section-d166511592e10333>1996</a>) as well as our current dataset.<h3 class=c-article__sub-heading id=Sec20>Vocal stimulus types predicted by acoustic parameters</h3><p>In line with previous findings (e.g., Lima et al., <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2013" title="Lima CF, Castro SL, Scott SK (2013) When voices get emotional: a corpus of nonverbal vocalizations for research on emotion processing. Behav Res Methods 45:1234–1245. 
                  https://doi.org/10.3758/s13428-013-0324-3
                  
                " href=#ref-CR55 id=ref-link-section-d166511592e10344>2013</a>; Sauter, <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2006" title="Sauter DA (2006) An investigation into vocal expressions of emotions: the roles of valence, culture, and acoustic factors. Unpublished Ph.D. thesis, University College London" href=#ref-CR79 id=ref-link-section-d166511592e10347>2006</a>), our results clearly indicated that the recognition of emotions from vocal bursts is much more dependent on the preservation of acoustic information (amplitude, pitch) than speech-embedded stimuli. This result corresponds to the idea that vocal bursts are more primitive and salient signals of emotion than speech-embedded vocalizations in an evolutionary sense (Krumhuber and Scherer, <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2011" title="Krumhuber EG, Scherer KR (2011) Affect bursts: dynamic patterns of facial expression. Emotion 11:825–841. 
                  https://doi.org/10.1037/a0023856
                  
                " href=#ref-CR53 id=ref-link-section-d166511592e10350>2011</a>). Since they do not require the dynamic spectral shaping caused by the rapid movements of the articulators (i.e., the tongue, jaw, lips and soft palate which shape the sound produced at the larynx), it has been suggested that vocal bursts resemble animal vocalizations more than they do spoken language (Krumhuber and Scherer, <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2011" title="Krumhuber EG, Scherer KR (2011) Affect bursts: dynamic patterns of facial expression. Emotion 11:825–841. 
                  https://doi.org/10.1037/a0023856
                  
                " href=#ref-CR53 id=ref-link-section-d166511592e10353>2011</a>; Scott et al., <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2010" title="Scott SK, Sauter D, McGettigan C (2010) Brain mechanisms for processing perceived emotional vocalizations in humans. In: Brudzynski SM (ed), Handbook of behavioral neuroscience, Elsevier, pp. 187–197" href=#ref-CR88 id=ref-link-section-d166511592e10356>2010</a>). For instance, laughter has been described as more akin to modified breathing, involving inhalation as well as exhalation, than to speaking (Kohler, <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2008" title="Kohler KJ (2008) ‘Speech-smile’, ‘speech-laugh’, ‘laughter’ and their sequencing in dialogic interaction. Phonetica 65:1–18. 
                  https://doi.org/10.1159/000130013
                  
                " href=#ref-CR50 id=ref-link-section-d166511592e10360>2008</a>). In addition, we found that neutral words (i.e., Anna; semantic neutral nouns) are better predicted by acoustic parameters than other speech-embedded stimuli. This fits well with the idea that words, similar to affect bursts, are arbitrary combinations of phonemes that derive their valence from what they symbolize (Schirmer, <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2010" title="Schirmer A (2010) Mark my words: tone of voice changes affective word representations in memory. PLoS ONE 5(2):e9080. 
                  https://doi.org/10.1371/journal.pone.0009080
                  
                " href=#ref-CR86 id=ref-link-section-d166511592e10363>2010</a>).<p>Although amplitude-, frequency-, and temporal related parameters are important in the emotional inflection of spoken language (Baenziger and Scherer, <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2005" title="Baenziger T, Scherer KR (2005) The role of intonation in emotional expressions. Speech Commun 46:252–267. 
                  https://doi.org/10.1016/j.specom.2005.02.016
                  
                " href=#ref-CR6 id=ref-link-section-d166511592e10369>2005</a>; Banse and Scherer, <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 1996" title="Banse R, Scherer KR (1996) Acoustic profiles in vocal emotion expression. J Pers Soc Psychol 70:614–636. 
                  https://doi.org/10.1037/0022-3514.70.3.614
                  
                " href=#ref-CR4 id=ref-link-section-d166511592e10372>1996</a>), as shown by our data the proportion of variance explained by these measures is slightly lower for sentences or stimuli with a semantic negative- or positive connotation. While these differences may be due to the quality of our speech-embedded stimuli, the difference between emotional speech and vocal bursts may also reflect the fact that emotion in speech is overlaid on the speech signal. This would mean speech is somewhat more constrained in its emotional expression than are non-verbal vocalizations (Scott et al., <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2010" title="Scott SK, Sauter D, McGettigan C (2010) Brain mechanisms for processing perceived emotional vocalizations in humans. In: Brudzynski SM (ed), Handbook of behavioral neuroscience, Elsevier, pp. 187–197" href=#ref-CR88 id=ref-link-section-d166511592e10375>2010</a>). Thus, there could be conflicts between the prosodic cues in sentence-level speech, which denote the emotional information, and those that cue linguistic information. Indeed, research has suggested that in speech-embedded stimuli emotional information derived from semantics is somehow stronger than that conveyed by prosodic cues (e.g., Pell et al., <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2011" title="Pell MD, Jaywant A, Monetta L et al. (2011) Emotional speech processing: disentangling the effects of prosody and semantic cues. Cogn Emot 25:834–853. 
                  https://doi.org/10.1080/02699931.2010.516915
                  
                " href=#ref-CR71 id=ref-link-section-d166511592e10378>2011</a>; Paulmann and Kotz, <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2008" title="Paulmann S, Kotz SA (2008) An ERP investigation on the temporal dynamics of emotional prosody and emotional semantics in pseudo- and lexical sentence context. Brain Lang 105:59–69. 
                  https://doi.org/10.1016/j.bandl.2007.11.005
                  
                " href=#ref-CR67 id=ref-link-section-d166511592e10381>2008</a>). Similar to previous studies on perceptual-acoustic correlates of the confidence that is expressed in the voice of the speaker (e.g., Jiang and Pell, <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2017" title="Jiang X, Pell MD (2017) The sound of confidence and doubt. Speech Commun 88:106–126. 
                  https://doi.org/10.1016/j.specom.2017.01.011
                  
                " href=#ref-CR37 id=ref-link-section-d166511592e10385>2017</a>; Rigoulout et al., <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2013" title="Rigoulot S, Wassiliwizky E, Pell MD (2013) Feeling backwards? How temporal order in speech affects the time course of vocal emotion recognition. Front Psychol 4:367. 
                  https://doi.org/10.3389/fpsyg.2013.00367
                  
                " href=#ref-CR77 id=ref-link-section-d166511592e10388>2013</a>; Pell and Kotz, <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2011" title="Pell MD, Kotz SA (2011) On the time course of vocal emotion recognition. PLoS ONE 6(11):e27256. 
                  https://doi.org/10.1371/journal.pone.0027256
                  
                " href=#ref-CR70 id=ref-link-section-d166511592e10391>2011</a>), our findings highlight that listeners confidence judgements are predicted by distinct sets of acoustic parameters. Of note, in shorter stimuli (i.e., affect bursts, nouns) <i>duration</i> was a significant predictor of listeners’ confidence judgements, while for longer stimuli (pseudo-, lexical- and neutral sentences) the <i>fundamental frequency</i> parameters explained the largest amount of variance in confidence ratings.<h3 class=c-article__sub-heading id=Sec21>Stimuli- and emotion comparisons/confidence predicted by correct emotion recognition</h3><p>Results from both logistic and linear models, implemented in our study, revealed that most of the acoustic predictors and vocal stimuli types had a significant influence on listeners’ recognition of emotions and their confidence judgements. Moreover, our findings showed that correct judgements of emotions elicited confident interpretations. These findings will be discussed in detail in the following.<p>The pattern of results revealed in our study clearly indicated that listeners were significantly more accurate and confident at judging emotions from non-speech sounds (i.e., affect bursts) than speech-embedded stimuli (i.e., sentences/pseudo-sentences). This finding adds to previous research which demonstrated that affect bursts are decoded more accurately than speech-embedded prosody (Hawk et al., <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2009" title="Hawk ST, van Kleef GA, Fischer AH et al. (2009) “Worth a thousand words”: absolute and relative decoding of nonlinguistic affect vocalizations. Emotion 9:293–305. 
                  https://doi.org/10.1037/a0015178
                  
                " href=#ref-CR33 id=ref-link-section-d166511592e10411>2009</a>). Further evidence comes from neurophysiological studies, showing that non-speech sounds facilitate early stages of perceptual processing in the form of decreased N1 amplitudes and enhanced P2 amplitudes in event-related brain potentials (Pell et al., <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2015" title="Pell MD, Rothermich K, Liu P et al. (2015) Preferential decoding of emotion from human non-linguistic vocalizations versus speech prosody. Biol Psychol 111:14–25. 
                  https://doi.org/10.1016/j.biopsycho.2015.08.008
                  
                " href=#ref-CR69 id=ref-link-section-d166511592e10414>2015</a>; Liu et al., <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2012" title="Liu T, Pinheiro AP, Deng G et al. (2012) Electrophysiological insights into processing nonverbal emotional vocalizations. Neuroreport 23:108–112. 
                  https://doi.org/10.1097/WNR.0b013e32834ea757
                  
                " href=#ref-CR56 id=ref-link-section-d166511592e10417>2012</a>). In other words, affect bursts, as evolutionary fundamental signals, seem to evoke a more rapid capture of attention than speech-embedded stimuli thought to involve more effortful cognitive processes and acoustic analysis (Pell et al., <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2015" title="Pell MD, Rothermich K, Liu P et al. (2015) Preferential decoding of emotion from human non-linguistic vocalizations versus speech prosody. Biol Psychol 111:14–25. 
                  https://doi.org/10.1016/j.biopsycho.2015.08.008
                  
                " href=#ref-CR69 id=ref-link-section-d166511592e10420>2015</a>) for successful recognition. Similar to recent findings (Schaerlaeken and Grandjean, <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2018" title="Schaerlaeken S, Grandjean D (2018) Unfolding and dynamics of affect bursts decoding in humans. PLoS ONE 13:e0206215. 
                  https://doi.org/10.1371/journal.pone.0206216
                  
                " href=#ref-CR81 id=ref-link-section-d166511592e10423>2018</a>), our data indicated that the accurate decoding of affect bursts led to greater confidence judgements. In light of the above-mentioned studies, one could argue that these types of stimuli carry more ecologically relevant information and, thus are given precedence by the neurocognitive system, allowing individuals to be more accurate and confident in their judgements of desirable/undesirable events in their environment. In addition, our results demonstrated that for stimuli with lexico-semantic content (i.e., nouns, lexical and neutral sentences) the accuracy of performance and confidence ratings is significantly higher than for stimuli devoid of meaning (i.e., pseudo-words; pseudo-sentences). Support for these findings comes from validation studies reporting greater accuracy for semantic compared to pseudo-speech utterances (e.g., Castro and Lima, <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2010" title="Castro SL, Lima CF (2010) Recognizing emotions in spoken language: a validated set of Portuguese sentences and pseudosentences for research on emotional prosody. Behav Res Methods 42:74–81. 
                  https://doi.org/10.3758/BRM.42.1.74
                  
                " href=#ref-CR15 id=ref-link-section-d166511592e10427>2010</a>) and from studies showing that congruent combinations of semantics and prosody, yield not only greater accuracy (e.g., Ben-David et al., <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2016" title="Ben-David BM, Multani N, Shakuf V et al. (2016) Prosody and semantics are separate but not separable channels in the perception of emotional speech: test for rating of emotions in speech. J Speech Lang Hear Res 59:1–18. 
                  https://doi.org/10.1044/2015_jslhr-h-14-0323
                  
                " href=#ref-CR9 id=ref-link-section-d166511592e10430>2016</a>) but, as shown by the patterns in our study, also higher confidence ratings compared to neutral sentences. When comparing stimuli with a semantic positive, negative and neutral content, no significant differences in recognition accuracy were observed, yet, listeners felt significantly more confident at detecting emotions in semantic negative than semantic positive or neutral single words (nouns). One explanation that has been put forth as to why such a negativity bias occurs in social judgements is that people may generally consider negative information to be more diagnostic than positive information in forming an overall impression (e.g., Hamilton and Huffman, <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 1971" title="Hamilton DL, Huffman LJ (1971) Generality of impression-formation processes for evaluative and nonevaluative judgments. J Pers Soc Psychol 20:200–207. 
                  https://doi.org/10.1037/h0031698
                  
                " href=#ref-CR30 id=ref-link-section-d166511592e10433>1971</a>). This is supported by studies showing that people consider negative information to be more important to impression formation and, when such information is available to them, they are subsequently more confident (e.g., Baumeister et al., <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2001" title="Baumeister RF, Bratslavsky E, Finkenauer C et al. (2001) Bad is stronger than good. Rev Gen Psychol 5:323–370. 
                  https://doi.org/10.1037//1089-2680.5.4.323
                  
                " href=#ref-CR5 id=ref-link-section-d166511592e10436>2001</a>; Hamilton and Zanna, <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 1972" title="Hamilton DL, Zanna MP (1972) Differential weighting of favorable and unfavorable attributes in impressions of personality. J Exp Res Pers 6:204–212" href=#ref-CR31 id=ref-link-section-d166511592e10439>1972</a>). Another possible explanation is that negative stimuli exert a stronger influence on people’s evaluations because these stimuli are more complex than positive ones and, thus, require greater attention and cognitive processing (e.g., Ito et al., <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 1998" title="Ito TA, Larsen JT, Smith NK et al. (1998) Negative information weighs more heavily on the brain: the negativity bias in evaluative categorizations. J Pers Soc Psychol 75:887–900. 
                  https://doi.org/10.1037/0022-3514.75.4.887
                  
                " href=#ref-CR35 id=ref-link-section-d166511592e10442>1998</a>; Peeters and Czapinski, <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 1990" title="Peeters G, Czapinski J (1990) Positive–negative asymmetry in evaluations: the distinction between affective and informational negativity effects. In: Stroebe W, Hewstone M (eds) European review of social psychology, vol. 1. Wiley, Chichester, pp. 33–60" href=#ref-CR74 id=ref-link-section-d166511592e10446>1990</a>; Abele, <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 1985" title="Abele A (1985) Thinking about thinking: causal, evaluative and finalistic cognitions about social situations. Eur J Soc Psychol 15:315–332. 
                  https://doi.org/10.1002/ejsp.2420150306
                  
                " href=#ref-CR1 id=ref-link-section-d166511592e10449>1985</a>). Correct recognition seemed to determine an increase of confidence judgements, however, when comparing Anna stimuli to other stimuli types, we observed that, when correctly categorizing the emotions for this stimulus type, listeners’ felt more confident regarding the correctness of their answer. As listeners’ decisions had to be made on the same item repeatedly presented (i.e., the name Anna), one could argue that this might have led to a familiarity effect with the stimulus. Studies on metacognition have shown that item familiarity leads to higher confidence ratings, because participants rest on the belief that more knowledge about the item means they are more accurate, although this has been shown to be an irrelevant factor (e.g., Koriat, <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2008" title="Koriat A (2008) When confidence in a choice is independent of which choice is made. Psychon Bull Rev 15:997–1001. 
                  https://doi.org/10.3758/PBR.15.5.997
                  
                " href=#ref-CR51 id=ref-link-section-d166511592e10452>2008</a>; Metcalfe et al., <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 1993" title="Metcalfe J, Schwartz BL, Joaquim SG (1993) The cue-familiarity heuristic in metacognition. J Exp Psychol Learn Mem Cogn 19:851–861. 
                  https://doi.org/10.1037//0278-7393.19.4.851
                  
                " href=#ref-CR57 id=ref-link-section-d166511592e10455>1993</a>).<p>Another important finding of the current study was that, for a vast majority of the stimuli, listeners’ performance accuracy and confidence ratings were significantly higher when spoken in an angry and neutral tone of voice than in any other emotional prosody. In contrast, recognition accuracy and confidence ratings were lowest for disgust, excepting affect bursts. One interesting pattern that emerged from our data was that listeners felt more confident at categorizing surprise than disgust and fear (i.e., pseudo- and lexical sentences) or happy (i.e., lexical sentences), despite the fact that their performance accuracy was significantly lower for the latter emotion. Moreover, we found that for nouns, fearful prosody led to higher accuracy rates and confidence ratings than sadness, although for other types of stimuli the exact opposite pattern was observed. Happiness yielded higher accuracy scores and confidence ratings in comparison to disgust, while when compared to other emotions this largely depended on the type of stimulus (for an overview, see Table <a data-track=click data-track-label=link data-track-action="table anchor" href=#Tab4>S4</a> in supplementary material). Previous studies on prosody revealed conflicting evidence regarding the differences between emotional and neutral prosody. They reported stimuli spoken in a neutral tone to be identified more accurately and with higher confidence ratings compared to other emotional prosodies, regardless of semantics (Cornew et al., <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2009" title="Cornew L, Carver L, Love T (2009) There’s more to emotion than meets the eye: a processing bias for neutral content in the domain of emotional prosody. Cogn Emot 24:1133–1152. 
                  https://doi.org/10.1080/02699930903247492
                  
                " href=#ref-CR20 id=ref-link-section-d166511592e10464>2009</a>; Schirmer and Kotz, <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2003" title="Schirmer A, Kotz SA (2003) ERP evidence for a sex-specific Stroop effect in emotional speech. J Cogn Neurosci 15:1135–1148. 
                  https://doi.org/10.1162/089892903322598102
                  
                " href=#ref-CR87 id=ref-link-section-d166511592e10467>2003</a>). Our findings converge with previous work on vocal emotion recognition, together indicating a general advantage for the recognition of neutral prosody, but also of angry expressions compared to other vocal expressions (e.g., Cowen et al., <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2019b" title="Cowen AS, Elfenbein HA, Laukka P et al. (2019b) Mapping 24 emotions conveyed by brief human vocalization. Am Psychol 74:698–712. 
                  https://doi.org/10.1037/amp0000399
                  
                " href=#ref-CR22 id=ref-link-section-d166511592e10470>2019b</a>; Chronaki et al., <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2018" title="Chronaki G, Wigelsworth M, Pell MD et al. (2018) The development of cross-cultural recognition of vocal emotions during childhood and adolescence. Sci Rep. 8:8659. 
                  https://doi.org/10.1038/s41598-018-26889-1
                  
                " href=#ref-CR16 id=ref-link-section-d166511592e10473>2018</a>; Paulmann and Uskul, <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2014" title="Paulmann S, Uskul AK (2014) Cross-cultural emotional prosody recognition: evidence from Chinese and British listeners. Cogn Emot 28:230–244. 
                  https://doi.org/10.1080/02699931.2013.812033
                  
                " href=#ref-CR66 id=ref-link-section-d166511592e10477>2014</a>; Pell et al., <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2009" title="Pell MD, Paulmann S, Dara C et al. (2009) Factors in the recognition of vocally expressed emotions: a comparison of four languages. J Phon 37:417–435. 
                  https://doi.org/10.1016/j.wocn.2009.07.005
                  
                " href=#ref-CR73 id=ref-link-section-d166511592e10480>2009</a>; Scherer et al., <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2001" title="Scherer KR, Banse R, Wallbott H (2001) Emotion inferences from vocal expression correlate across languages and cultures. J Cross Cult Psychol 32:76–92. 
                  https://doi.org/10.1177/0022022101032001009
                  
                " href=#ref-CR83 id=ref-link-section-d166511592e10483>2001</a>). This evidence is also compatible with evolutionary theories arguing that humans as well as other primates are biologically prepared to respond rapidly to vocal cues associated with threat or anger (e.g., Oehman, <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 1993" title="Oehman A (1993) Fear and anxiety as emotional phenomenon: clinical phenomenology, evolutionary perspectives, and information-processing mechanisms. In: Lewis M, Haviland JM (eds) Handbook of emotions. Guildford Press, New York, pp. 511–536" href=#ref-CR63 id=ref-link-section-d166511592e10486>1993</a>). It has been argued that particularly non-speech sounds (e.g. growls of anger, the laughter of happiness or cries of sadness) convey emotions clearer and faster than words (Pell et al., <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2015" title="Pell MD, Rothermich K, Liu P et al. (2015) Preferential decoding of emotion from human non-linguistic vocalizations versus speech prosody. Biol Psychol 111:14–25. 
                  https://doi.org/10.1016/j.biopsycho.2015.08.008
                  
                " href=#ref-CR69 id=ref-link-section-d166511592e10489>2015</a>). Our results, however, showed that when identifying emotions from affect bursts, listeners were less accurate and felt less confident at detecting anger when compared to other emotional prosodies. Previous findings report similar accuracy patterns when decoding anger from affect bursts (Pell et al., <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2015" title="Pell MD, Rothermich K, Liu P et al. (2015) Preferential decoding of emotion from human non-linguistic vocalizations versus speech prosody. Biol Psychol 111:14–25. 
                  https://doi.org/10.1016/j.biopsycho.2015.08.008
                  
                " href=#ref-CR69 id=ref-link-section-d166511592e10492>2015</a>; Belin et al., <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2008" title="Belin P, Fillion-Bilodeau S, Gosselin F (2008) The Montreal affective voices: a validated set of nonverbal affect bursts for research on auditory affective processing. Behav Res Methods 40:531–539. 
                  https://doi.org/10.3758/BRM.40.2.531
                  
                " href=#ref-CR8 id=ref-link-section-d166511592e10496>2008</a>). However, it remains unclear why this effect emerged. A possible explanation relates to the acoustics of acting anger sounds which might differ from natural ones. Anikin and Lima (<a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2018" title="Anikin A, Lima CF (2018) Perceptual and acoustic differences between authentic and acted nonverbal emotional vocalizations. Q J Exp Psychol 71:622–641. 
                  https://doi.org/10.1080/17470218.2016.1270976
                  
                " href=#ref-CR2 id=ref-link-section-d166511592e10499>2018</a>), for instance, reported that authentic vocalizations (e.g., anger, fear) differed from actor portrayals in a number of acoustic characteristics by showing a higher pitch and lower harmonicity, as well as, a less variable spectral slope and amplitude (but also see, Juergens et al., <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2015" title="Juergens R, Grass A, Drolet M et al. (2015) Effect of acting experience on emotion expression and recognition in voice: non-actors provide better stimuli than expected. J Nonverbal Behav 39:195–214. 
                  https://doi.org/10.1007/s10919-015-0209-5
                  
                " href=#ref-CR44 id=ref-link-section-d166511592e10502>2015</a>, <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2011" title="Juergens R, Hammerschmidt K, Fischer J (2011) Authentic and play-acted vocal emotion expressions reveal acoustic differences. Front Psychol 2:180. 
                  https://doi.org/10.3389/fpsyg.2011.00180
                  
                " href=#ref-CR46 id=ref-link-section-d166511592e10505>2011</a>, for a discussion on authentic vs. play acted expressions). Thus, it might be plausible that these acoustic characteristics of authenticity are hard-to-fake markers of a speaker’s emotional state and, thus, signal a distinction between honest communication and a bluff (Anikin and Lima, <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2018" title="Anikin A, Lima CF (2018) Perceptual and acoustic differences between authentic and acted nonverbal emotional vocalizations. Q J Exp Psychol 71:622–641. 
                  https://doi.org/10.1080/17470218.2016.1270976
                  
                " href=#ref-CR2 id=ref-link-section-d166511592e10508>2018</a>). The patterns obtained for disgust are also consistent with those of previous studies showing that, after surprise, it is the most difficult emotional expression to be recognized from speech-embedded stimuli (e.g., Paulmann and Uskul, <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2014" title="Paulmann S, Uskul AK (2014) Cross-cultural emotional prosody recognition: evidence from Chinese and British listeners. Cogn Emot 28:230–244. 
                  https://doi.org/10.1080/02699931.2013.812033
                  
                " href=#ref-CR66 id=ref-link-section-d166511592e10511>2014</a>) but not from non-speech sounds, presumably, because this emotion is merely expressed in affect bursts or short interjections (e.g., yuck) rather than in sentential context (Johnstone and Scherer, <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2000" title="Johnstone T, Scherer KR (2000) Vocal communication of emotion. In: Lewis M, Haviland J (eds) The handbook of emotion, 2nd edn. Guildford, New York, pp. 220–235" href=#ref-CR39 id=ref-link-section-d166511592e10515>2000</a>; Banse and Scherer, <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 1996" title="Banse R, Scherer KR (1996) Acoustic profiles in vocal emotion expression. J Pers Soc Psychol 70:614–636. 
                  https://doi.org/10.1037/0022-3514.70.3.614
                  
                " href=#ref-CR4 id=ref-link-section-d166511592e10518>1996</a>). In contrast, surprise yielded higher confidence scores than disgust or fear, which could be due to the fact that humans are more prone to notice and focus on surprising events and, therefore are more likely to attend to them (Wilson and Gilbert, <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2008" title="Wilson TD, Gilbert DT (2008) Explaining away: a model of affective adaptation. Perspect Psychol Sci 3:370–386. 
                  https://doi.org/10.1111/j.1745-6924.2008.00085.x
                  
                " href=#ref-CR95 id=ref-link-section-d166511592e10521>2008</a>). Following these arguments, one could speculate that similar to anger, surprise might also serve a functional and adaptive purpose as people might devote their energy to judging whether what is unfolding before them is a threat, a joke or a harmless event, thus, eliciting more confident evaluations. A similar argument may apply to our results regarding the comparisons between fear and sadness. Fear, as an expression that signals threat, might require less auditory input to be decoded accurately (i.e., shorter stimuli—in our case nouns), while identifying sadness from speech might activate additional social meanings that take more time to analyse and more careful post-message processing.<p>To summarize, this set of results extends previous findings from the facial domain (Kelly and Metcalfe, <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2011" title="Kelly KJ, Metcalfe J (2011) Metacognition of emotional face recognition. Emotion 11:896–906. 
                  https://doi.org/10.1037/a0023746
                  
                " href=#ref-CR47 id=ref-link-section-d166511592e10527>2011</a>) by showing that listeners who were better at recognizing vocal expressions of emotion were also more confident in their judgements. Although slight variations between emotion recognition accuracy and confidence ratings were observed for some stimuli types or emotion categories, overall our results support that the correct recognition of emotions increases confident judgements. Our findings suggest that individuals can predict and assess their performance for the recognition of emotional prosody.</p></div></div></section><section data-title="General discussion"><div class=c-article-section id=Sec22-section><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id=Sec22>General discussion</h2><div class=c-article-section__content id=Sec22-content><p>In the present investigation, we aimed to examine the influence of specific types of vocal stimuli and their acoustic parameters on emotional prosody recognition and retrospective confidence judgements. Previous research has suggested that when investigating the effects of prosody on emotion recognition it is critical to study a wide variety of stimuli to determine the full dimensionality of that space (e.g., Cowen et al., <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2019a" title="Cowen AS, Laukka P, Elfenbein HA et al. (2019a) The primacy of categories in the recognition of 12 emotions in speech prosody across two cultures. Nat Hum Behav 3:369–382. 
                  https://doi.org/10.1038/s41562-019-0533-6
                  
                " href=#ref-CR21 id=ref-link-section-d166511592e10539>2019a</a>; Sauter et al., <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2010" title="Sauter DA, Eisner F, Calder AJ et al. (2010) Perceptual cues in nonverbal vocal expressions of emotion. Q J Exp Psychol 63:2251–2272. 
                  https://doi.org/10.1080/17470211003721642
                  
                " href=#ref-CR78 id=ref-link-section-d166511592e10542>2010</a>). To cover the spectrum of materials used in emotional prosody research, we studied a vast array of vocal stimuli (i.e., for <i>speech</i>: words, lexical and neutral sentences; <i>pseudo-speech</i>: pseudo-words/sentences; for <i>non-speech</i>: vocalizations) from which we extracted a standard set of acoustic parameters considering previous findings that emphasized the importance of paralinguistic emotional content in verbal communication (e.g., Eyben et al., <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2016" title="Eyben F, Scherer KR, Schuller BW et al. (2016) The Geneva minimalistic acoustic parameter set (GeMAPS) for voice research and affective computing. IEEE Trans Affect Comput 7:190–202. 
                  https://doi.org/10.1109/TAFFC.2015.2457417
                  
                " href=#ref-CR27 id=ref-link-section-d166511592e10555>2016</a>).<p>In two studies, we examined (1) the extent to which listeners and classifiers use acoustic parameters as perceptual cues for identifying portrayed emotions and (2) whether listeners’ performance accuracy and confidence ratings vary across certain types of stimuli and specific emotion categories. To determine whether the set of extracted parameters provided sufficient information to successfully discriminate between stimuli from different emotional categories, discriminant analysis and RF were implemented (study 1). By employing a backward stepwise logistic regression analysis, we determined which of the acoustic predictors explained most of the deviance in listeners’ recognition rates (study 2). Results showed high cross-validation estimates of accuracy for both classification methods (study 1), indicating that the stimuli contained detectable acoustic contrasts which helped listeners to differentiate the portrayed emotions and that most, if not all, parameters explained a significant amount of variance in listeners’ recognition rates (study 2). This set of results corresponds to previous findings in the vocal emotion literature (e.g., Cowen et al., <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2019a" title="Cowen AS, Laukka P, Elfenbein HA et al. (2019a) The primacy of categories in the recognition of 12 emotions in speech prosody across two cultures. Nat Hum Behav 3:369–382. 
                  https://doi.org/10.1038/s41562-019-0533-6
                  
                " href=#ref-CR21 id=ref-link-section-d166511592e10561>2019a</a>; Hammerschmidt and Juergens, <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2007" title="Hammerschmidt K, Juergens U (2007) Acoustical correlates of affective prosody. J Voice 21:531–540. 
                  https://doi.org/10.1016/j.jvoice.2006.03.002
                  
                " href=#ref-CR32 id=ref-link-section-d166511592e10564>2007</a>; Banse and Scherer, <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 1996" title="Banse R, Scherer KR (1996) Acoustic profiles in vocal emotion expression. J Pers Soc Psychol 70:614–636. 
                  https://doi.org/10.1037/0022-3514.70.3.614
                  
                " href=#ref-CR4 id=ref-link-section-d166511592e10567>1996</a>) and, in analogy, they parallel research on visual signals of emotions which reported that statistical classification methods can successfully discriminate facial expressions of different emotions on the basis of their pixel intensities (e.g., Calder et al., <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2001" title="Calder AJ, Burton AM, Miller P et al. (2001) A principal component analysis of facial expressions. Vis Res 41:1179–1208. 
                  https://doi.org/10.1016/S0042-6989(01)00002-5
                  
                " href=#ref-CR14 id=ref-link-section-d166511592e10570>2001</a>). Thus, for both vocal and facial modalities, it is possible to classify emotional expressions on the basis of basic perceptual features in a manner that models human performance.<p>With respect to the comparisons between the types of stimuli and emotion categories, we uncovered that affect bursts are richer and more nuanced than typically thought (study 2). Although our data showed that emotion and confidence ratings could be reliably predicted from speech-embedded stimuli acoustic attributes, we found that in affect bursts the acoustic parameters explained the largest proportion of variance. Moreover, our study revealed that vocal bursts proved to be highly effective means of expressing specific emotions, such as disgust, happiness and sadness, in comparison to speech-embedded stimuli, with recognition accuracies above 80% (except for surprise). Results from cross-cultural studies corroborate these findings by reporting strong to moderate evidence for the universal recognizability of these emotions from vocal bursts (e.g., Cowen et al., <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2019a" title="Cowen AS, Laukka P, Elfenbein HA et al. (2019a) The primacy of categories in the recognition of 12 emotions in speech prosody across two cultures. Nat Hum Behav 3:369–382. 
                  https://doi.org/10.1038/s41562-019-0533-6
                  
                " href=#ref-CR21 id=ref-link-section-d166511592e10576>2019a</a>; Cordaro et al., <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2016" title="Cordaro DT, Keltner D, Tshering S et al. (2016) The voice conveys emotion in ten globalized cultures and one remote village in Bhutan. Emotion 16:117–128. 
                  https://doi.org/10.1037/emo0000100
                  
                " href=#ref-CR19 id=ref-link-section-d166511592e10579>2016</a>). Thus, one could ask whether there is a reason why these specific emotions are better recognized in this type of vocal stimuli? An argument that has been put forth is that vocal bursts are unique to some emotions (Goddard, <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2014" title="Goddard C (2014) Interjections and emotion (with special reference to “surprise” and “disgust”). Emot Rev 6:53–63. 
                  https://doi.org/10.1177/1754073913491843
                  
                " href=#ref-CR28 id=ref-link-section-d166511592e10582>2014</a>). For instance, laughter could be interpreted as a signal of happiness, crying as a signal of sadness, while interjections such as ‘argh’, ‘eek’ are typically indicative of disgust. Moreover, it has been suggested that people quite rarely vocalize disgust or surprise in the form of sentences (Schaerlaeken and Grandjean, <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2018" title="Schaerlaeken S, Grandjean D (2018) Unfolding and dynamics of affect bursts decoding in humans. PLoS ONE 13:e0206215. 
                  https://doi.org/10.1371/journal.pone.0206216
                  
                " href=#ref-CR81 id=ref-link-section-d166511592e10585>2018</a>; Banse and Scherer, <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 1996" title="Banse R, Scherer KR (1996) Acoustic profiles in vocal emotion expression. J Pers Soc Psychol 70:614–636. 
                  https://doi.org/10.1037/0022-3514.70.3.614
                  
                " href=#ref-CR4 id=ref-link-section-d166511592e10588>1996</a>). Since vocal bursts bear a heavy functional load in social interactions, as they are “so highly overlearned” and clearly attached to certain emotions (Goddard, <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2014" title="Goddard C (2014) Interjections and emotion (with special reference to “surprise” and “disgust”). Emot Rev 6:53–63. 
                  https://doi.org/10.1177/1754073913491843
                  
                " href=#ref-CR28 id=ref-link-section-d166511592e10592>2014</a>; Scherer, <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 1994" title="Scherer KR (1994) Affect bursts. In: van Goozen SHM, van de Poll NE, Sergeant JA (eds) Emotions: essays on emotion theory. Erlbaum, Hillsdale, pp. 161–193" href=#ref-CR84 id=ref-link-section-d166511592e10595>1994</a>), their accurate recognition might occur instantaneously and without conscious effort. Similar to previous interpretations (Cowen et al., <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2019a" title="Cowen AS, Laukka P, Elfenbein HA et al. (2019a) The primacy of categories in the recognition of 12 emotions in speech prosody across two cultures. Nat Hum Behav 3:369–382. 
                  https://doi.org/10.1038/s41562-019-0533-6
                  
                " href=#ref-CR21 id=ref-link-section-d166511592e10598>2019a</a>), our results regarding the efficiency of vocal bursts in emotion recognition may be explained by the innate psychological basicness of these signals (which might be universal to all humans).<h3 class=c-article__sub-heading id=Sec23>Limitations and future research</h3><p>Although the main groups of paralinguistic features and their acoustic parameters (Eyben et al., <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2016" title="Eyben F, Scherer KR, Schuller BW et al. (2016) The Geneva minimalistic acoustic parameter set (GeMAPS) for voice research and affective computing. IEEE Trans Affect Comput 7:190–202. 
                  https://doi.org/10.1109/TAFFC.2015.2457417
                  
                " href=#ref-CR27 id=ref-link-section-d166511592e10608>2016</a>; Juslin and Laukka, <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2003" title="Juslin PN, Laukka P (2003) Communication of emotions in vocal expression and music performance: different channels, same code? Psychol Bull 129:770–814. 
                  https://doi.org/10.1037/0033-2909.129.5.770
                  
                " href=#ref-CR41 id=ref-link-section-d166511592e10611>2003</a>), were covered in our study, not all relevant properties of vocal emotional expression have been considered. Thus, future research would profit by implementing, for instance, spectral parameters or prosodic contours, as they might index physiological changes in voice and are sensitive to emotional expressions (e.g., Eyben et al., <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2016" title="Eyben F, Scherer KR, Schuller BW et al. (2016) The Geneva minimalistic acoustic parameter set (GeMAPS) for voice research and affective computing. IEEE Trans Affect Comput 7:190–202. 
                  https://doi.org/10.1109/TAFFC.2015.2457417
                  
                " href=#ref-CR27 id=ref-link-section-d166511592e10614>2016</a>; Mozziconacci, <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2002" title="Mozziconacci S (2002) Prosody and emotions. In: Proceedings of speech prosody, pp. 1–9. 
                  https://www.isca-speech.org/archive/sp2002/
                  
                . Accessed 30 Nov 2018" href=#ref-CR59 id=ref-link-section-d166511592e10617>2002</a>). A limitation to this issue is the fact that parameters were extracted from the entire utterances. Although this is a common approach (Pichora-Fuller et al., <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2016" title="Pichora-Fuller MK, Dupuis K, Van Lieshout P (2016) Importance of F0 for predicting vocal emotion categorization. J Acoust Soc Am 140:3401–3401. 
                  https://doi.org/10.1121/1.4970917
                  
                " href=#ref-CR75 id=ref-link-section-d166511592e10620>2016</a>; Castro and Lima, <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2010" title="Castro SL, Lima CF (2010) Recognizing emotions in spoken language: a validated set of Portuguese sentences and pseudosentences for research on emotional prosody. Behav Res Methods 42:74–81. 
                  https://doi.org/10.3758/BRM.42.1.74
                  
                " href=#ref-CR15 id=ref-link-section-d166511592e10624>2010</a>; Paulmann et al., <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2008" title="Paulmann S, Pell MD, Kotz SA (2008) How aging affects the recognition of emotional speech. Brain Lang 104:262–269. 
                  https://doi.org/10.1016/j.bandl.2007.03.002
                  
                " href=#ref-CR68 id=ref-link-section-d166511592e10627>2008</a>), it has been supposed that it disregards the phonetic identity of speech segments in emotional expression (Goudbeek and Scherer, <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2010" title="Goudbeek M, Scherer KR (2010) Beyond arousal: valence and potency/control cues in the vocal expression of emotion. J Acoust Soc Am 128:1322–1336. 
                  https://doi.org/10.1121/1.3466853
                  
                " href=#ref-CR29 id=ref-link-section-d166511592e10630>2010</a>). There is also the proviso that gender of the speaker may have had an effect on the discrimination accuracy (Lausen and Schacht, <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2018" title="Lausen A, Schacht A (2018) Gender differences in the recognition of vocal emotions. Front Psychol 9:882. 
                  https://doi.org/10.3389/fpsyg.2018.00882
                  
                " href=#ref-CR54 id=ref-link-section-d166511592e10633>2018</a>). The present study, however, focused on the patterns of voice cues used to portray specific emotions, rather than on gender differences. By keeping in line with previous research in this area we extracted the acoustic parameters across both genders (e.g., Sauter et al., <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2010" title="Sauter DA, Eisner F, Calder AJ et al. (2010) Perceptual cues in nonverbal vocal expressions of emotion. Q J Exp Psychol 63:2251–2272. 
                  https://doi.org/10.1080/17470211003721642
                  
                " href=#ref-CR78 id=ref-link-section-d166511592e10636>2010</a>; Paulmann et al., <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2008" title="Paulmann S, Pell MD, Kotz SA (2008) How aging affects the recognition of emotional speech. Brain Lang 104:262–269. 
                  https://doi.org/10.1016/j.bandl.2007.03.002
                  
                " href=#ref-CR68 id=ref-link-section-d166511592e10639>2008</a>; Juslin and Laukka, <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2001" title="Juslin PN, Laukka P (2001) Impact of intended emotion intensity on cue utilization and decoding accuracy in vocal expression of emotion. Emotion 1:381–412. 
                  https://doi.org/10.1037//1528-3542.1.4.381
                  
                " href=#ref-CR42 id=ref-link-section-d166511592e10643>2001</a>). This does not rule out that gender might have had an effect, however, it should be noted that for the majority of the stimulus types the emotions were expressed by two speakers of different sexes (1 male, 1 female). Future work with a greater number of speakers would not only be able to establish the degree to which the acoustic factors in this study can be generalized but, would also help to explain the variation in these factors alongside speakers’ gender characteristics. A further limitation regards the absence of some emotional categories within our stimuli datasets. In comparison to the classification algorithms that categorized emotions based on the existing number of emotion categories, listeners were supposed to choose from a fixed set of given alternatives. This might explain why for certain emotion categories, listeners had higher error rates than the RF algorithm. Moreover, machine-learning efforts to decode emotion from speech stimuli will need to expand in focus beyond a small set of discrete categories. Finally, the sample in the current study was limited to a university-educated population and included predominantly young adults, which may limit the generalizability of the findings to the wider population.</p></div></div></section><section data-title=Conclusion><div class=c-article-section id=Sec24-section><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id=Sec24>Conclusion</h2><div class=c-article-section__content id=Sec24-content><p>The present study provided a systematic investigation of the influence prosodic parameters and stimulus types exert on vocal emotion recognition and confidence ratings. Addressing a timely issue with the abundance of vocal emotion corpora, our findings are essential both empirically and conceptually. First, they replicated earlier research by establishing that humans can infer emotion from vocal expression, based on differential acoustic patterning. Second, our results extend previous findings by indicating that emotional expressions are more accurately recognized and confidently judged from non-speech sounds than from semantically inflected. Finally, and most importantly, our findings yield data-driven hypotheses into important questions within emotion science (e.g., Sauter et al., <a data-track=click data-track-action="reference anchor" data-track-label=link data-test=citation-ref aria-label="Reference 2010" title="Sauter DA, Eisner F, Calder AJ et al. (2010) Perceptual cues in nonverbal vocal expressions of emotion. Q J Exp Psychol 63:2251–2272. 
                  https://doi.org/10.1080/17470211003721642
                  
                " href=#ref-CR78 id=ref-link-section-d166511592e10655>2010</a>) by showing that this pattern is not constant across all emotional categories and that listeners do not rely on the same acoustic cues when decoding emotions from speech and non-speech embedded sounds. While the current findings demonstrate that correct recognition of emotions promotes confident interpretations, more research is needed to uncover the underlying mechanisms of how individuals use this metacognitive knowledge.</p></div></div></section>
 
 <section data-title="Data availability"><div class=c-article-section id=data-availability-section><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id=data-availability>Data availability</h2><div class=c-article-section__content id=data-availability-content>
 
 <p>The datasets generated during and/or analysed during the current study are available in the Open Science Framework (OSF) repository: <a href="https://osf.io/j695t/?view_only=8e2b7a167737473faef357e9e34cb125">https://doi.org/10.17605/OSF.IO/J695T</a></p>
 </div></div></section><div id=MagazineFulltextArticleBodySuffix><section aria-labelledby=Bib1 data-title=References><div class=c-article-section id=Bib1-section><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id=Bib1>References</h2><div class=c-article-section__content id=Bib1-content><div data-container-section=references><ul class=c-article-references data-track-component="outbound reference"><li class="c-article-references__item js-c-reading-companion-references-item"><p class=c-article-references__text id=ref-CR1>Abele A (1985) Thinking about thinking: causal, evaluative and finalistic cognitions about social situations. Eur J Soc Psychol 15:315–332. <a href=https://doi.org/10.1002/ejsp.2420150306>https://doi.org/10.1002/ejsp.2420150306</a><p class="c-article-references__links u-hide-print"><a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="article reference" href=https://doi.org/10.1002%2Fejsp.2420150306 aria-label="Article reference 1">Article</a>&nbsp;
 <a data-track=click data-track-action="google scholar reference" data-track-label=link rel="nofollow noopener" aria-label="Google Scholar reference 1" href="http://scholar.google.com/scholar_lookup?&amp;title=Thinking%20about%20thinking%3A%20causal%2C%20evaluative%20and%20finalistic%20cognitions%20about%20social%20situations&amp;journal=Eur%20J%20Soc%20Psychol&amp;doi=10.1002%2Fejsp.2420150306&amp;volume=15&amp;pages=315-332&amp;publication_year=1985&amp;author=Abele%2CA">
 Google Scholar</a>&nbsp;
 </p><li class="c-article-references__item js-c-reading-companion-references-item"><p class=c-article-references__text id=ref-CR2>Anikin A, Lima CF (2018) Perceptual and acoustic differences between authentic and acted nonverbal emotional vocalizations. Q J Exp Psychol 71:622–641. <a href=https://doi.org/10.1080/17470218.2016.1270976>https://doi.org/10.1080/17470218.2016.1270976</a><p class="c-article-references__links u-hide-print"><a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="article reference" href=https://doi.org/10.1080%2F17470218.2016.1270976 aria-label="Article reference 2">Article</a>&nbsp;
 <a data-track=click data-track-action="google scholar reference" data-track-label=link rel="nofollow noopener" aria-label="Google Scholar reference 2" href="http://scholar.google.com/scholar_lookup?&amp;title=Perceptual%20and%20acoustic%20differences%20between%20authentic%20and%20acted%20nonverbal%20emotional%20vocalizations&amp;journal=Q%20J%20Exp%20Psychol&amp;doi=10.1080%2F17470218.2016.1270976&amp;volume=71&amp;pages=622-641&amp;publication_year=2018&amp;author=Anikin%2CA&amp;author=Lima%2CCF">
 Google Scholar</a>&nbsp;
 </p><li class="c-article-references__item js-c-reading-companion-references-item"><p class=c-article-references__text id=ref-CR3>Bąk HK (2016) The state of emotional prosody research—a meta-analysis. In: Bąk HK (ed) Emotional prosody processing for non-native English speakers, 1st edn. Springer International Publishing, pp. 79–112</p><li class="c-article-references__item js-c-reading-companion-references-item"><p class=c-article-references__text id=ref-CR4>Banse R, Scherer KR (1996) Acoustic profiles in vocal emotion expression. J Pers Soc Psychol 70:614–636. <a href=https://doi.org/10.1037/0022-3514.70.3.614>https://doi.org/10.1037/0022-3514.70.3.614</a><p class="c-article-references__links u-hide-print"><a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="cas reference" href=https://www.nature.com/articles/cas-redirect/1:STN:280:DyaK2s%2FgsVCgtg%3D%3D aria-label="CAS reference 4">CAS</a>&nbsp;
 <a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="article reference" href=https://doi.org/10.1037%2F0022-3514.70.3.614 aria-label="Article reference 4">Article</a>&nbsp;
 <a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=8851745" aria-label="PubMed reference 4">PubMed</a>&nbsp;
 <a data-track=click data-track-action="google scholar reference" data-track-label=link rel="nofollow noopener" aria-label="Google Scholar reference 4" href="http://scholar.google.com/scholar_lookup?&amp;title=Acoustic%20profiles%20in%20vocal%20emotion%20expression&amp;journal=J%20Pers%20Soc%20Psychol&amp;doi=10.1037%2F0022-3514.70.3.614&amp;volume=70&amp;pages=614-636&amp;publication_year=1996&amp;author=Banse%2CR&amp;author=Scherer%2CKR">
 Google Scholar</a>&nbsp;
 </p><li class="c-article-references__item js-c-reading-companion-references-item"><p class=c-article-references__text id=ref-CR5>Baumeister RF, Bratslavsky E, Finkenauer C et al. (2001) Bad is stronger than good. Rev Gen Psychol 5:323–370. <a href=https://doi.org/10.1037//1089-2680.5.4.323>https://doi.org/10.1037//1089-2680.5.4.323</a><p class="c-article-references__links u-hide-print"><a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="article reference" href=https://doi.org/10.1037%2F%2F1089-2680.5.4.323 aria-label="Article reference 5">Article</a>&nbsp;
 <a data-track=click data-track-action="google scholar reference" data-track-label=link rel="nofollow noopener" aria-label="Google Scholar reference 5" href="http://scholar.google.com/scholar_lookup?&amp;title=Bad%20is%20stronger%20than%20good&amp;journal=Rev%20Gen%20Psychol&amp;doi=10.1037%2F%2F1089-2680.5.4.323&amp;volume=5&amp;pages=323-370&amp;publication_year=2001&amp;author=Baumeister%2CRF&amp;author=Bratslavsky%2CE&amp;author=Finkenauer%2CC">
 Google Scholar</a>&nbsp;
 </p><li class="c-article-references__item js-c-reading-companion-references-item"><p class=c-article-references__text id=ref-CR6>Baenziger T, Scherer KR (2005) The role of intonation in emotional expressions. Speech Commun 46:252–267. <a href=https://doi.org/10.1016/j.specom.2005.02.016>https://doi.org/10.1016/j.specom.2005.02.016</a><p class="c-article-references__links u-hide-print"><a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="article reference" href=https://doi.org/10.1016%2Fj.specom.2005.02.016 aria-label="Article reference 6">Article</a>&nbsp;
 <a data-track=click data-track-action="google scholar reference" data-track-label=link rel="nofollow noopener" aria-label="Google Scholar reference 6" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20role%20of%20intonation%20in%20emotional%20expressions&amp;journal=Speech%20Commun&amp;doi=10.1016%2Fj.specom.2005.02.016&amp;volume=46&amp;pages=252-267&amp;publication_year=2005&amp;author=Baenziger%2CT&amp;author=Scherer%2CKR">
 Google Scholar</a>&nbsp;
 </p><li class="c-article-references__item js-c-reading-companion-references-item"><p class=c-article-references__text id=ref-CR7>Bègue I, Vaessen M, Hofmeister J et al. (2019) Confidence of emotion expression recognition recruits brain regions outside the face perception network. Soc Cogn Affect Neurosci 4:81–95. <a href=https://doi.org/10.1093/scan/nsy102>https://doi.org/10.1093/scan/nsy102</a><p class="c-article-references__links u-hide-print"><a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="article reference" href=https://doi.org/10.1093%2Fscan%2Fnsy102 aria-label="Article reference 7">Article</a>&nbsp;
 <a data-track=click data-track-action="google scholar reference" data-track-label=link rel="nofollow noopener" aria-label="Google Scholar reference 7" href="http://scholar.google.com/scholar_lookup?&amp;title=Confidence%20of%20emotion%20expression%20recognition%20recruits%20brain%20regions%20outside%20the%20face%20perception%20network&amp;journal=Soc%20Cogn%20Affect%20Neurosci&amp;doi=10.1093%2Fscan%2Fnsy102&amp;volume=4&amp;pages=81-95&amp;publication_year=2019&amp;author=B%C3%A8gue%2CI&amp;author=Vaessen%2CM&amp;author=Hofmeister%2CJ">
 Google Scholar</a>&nbsp;
 </p><li class="c-article-references__item js-c-reading-companion-references-item"><p class=c-article-references__text id=ref-CR8>Belin P, Fillion-Bilodeau S, Gosselin F (2008) The Montreal affective voices: a validated set of nonverbal affect bursts for research on auditory affective processing. Behav Res Methods 40:531–539. <a href=https://doi.org/10.3758/BRM.40.2.531>https://doi.org/10.3758/BRM.40.2.531</a><p class="c-article-references__links u-hide-print"><a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="article reference" href=https://doi.org/10.3758%2FBRM.40.2.531 aria-label="Article reference 8">Article</a>&nbsp;
 <a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=18522064" aria-label="PubMed reference 8">PubMed</a>&nbsp;
 <a data-track=click data-track-action="google scholar reference" data-track-label=link rel="nofollow noopener" aria-label="Google Scholar reference 8" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20Montreal%20affective%20voices%3A%20a%20validated%20set%20of%20nonverbal%20affect%20bursts%20for%20research%20on%20auditory%20affective%20processing&amp;journal=Behav%20Res%20Methods&amp;doi=10.3758%2FBRM.40.2.531&amp;volume=40&amp;pages=531-539&amp;publication_year=2008&amp;author=Belin%2CP&amp;author=Fillion-Bilodeau%2CS&amp;author=Gosselin%2CF">
 Google Scholar</a>&nbsp;
 </p><li class="c-article-references__item js-c-reading-companion-references-item"><p class=c-article-references__text id=ref-CR9>Ben-David BM, Multani N, Shakuf V et al. (2016) Prosody and semantics are separate but not separable channels in the perception of emotional speech: test for rating of emotions in speech. J Speech Lang Hear Res 59:1–18. <a href=https://doi.org/10.1044/2015_jslhr-h-14-0323>https://doi.org/10.1044/2015_jslhr-h-14-0323</a><p class="c-article-references__links u-hide-print"><a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="article reference" href=https://doi.org/10.1044%2F2015_jslhr-h-14-0323 aria-label="Article reference 9">Article</a>&nbsp;
 <a data-track=click data-track-action="google scholar reference" data-track-label=link rel="nofollow noopener" aria-label="Google Scholar reference 9" href="http://scholar.google.com/scholar_lookup?&amp;title=Prosody%20and%20semantics%20are%20separate%20but%20not%20separable%20channels%20in%20the%20perception%20of%20emotional%20speech%3A%20test%20for%20rating%20of%20emotions%20in%20speech&amp;journal=J%20Speech%20Lang%20Hear%20Res&amp;doi=10.1044%2F2015_jslhr-h-14-0323&amp;volume=59&amp;pages=1-18&amp;publication_year=2016&amp;author=Ben-David%2CBM&amp;author=Multani%2CN&amp;author=Shakuf%2CV">
 Google Scholar</a>&nbsp;
 </p><li class="c-article-references__item js-c-reading-companion-references-item"><p class=c-article-references__text id=ref-CR10>Bostanov V, Kotchoubey B (2004) Recognition of affective prosody: continuous wavelet measures of event-related brain potentials to emotional exclamations. Psychophysiology 41:259–268. <a href=https://doi.org/10.1111/j.1469-8986.2003.00142.x>https://doi.org/10.1111/j.1469-8986.2003.00142.x</a><p class="c-article-references__links u-hide-print"><a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="article reference" href=https://doi.org/10.1111%2Fj.1469-8986.2003.00142.x aria-label="Article reference 10">Article</a>&nbsp;
 <a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=15032991" aria-label="PubMed reference 10">PubMed</a>&nbsp;
 <a data-track=click data-track-action="google scholar reference" data-track-label=link rel="nofollow noopener" aria-label="Google Scholar reference 10" href="http://scholar.google.com/scholar_lookup?&amp;title=Recognition%20of%20affective%20prosody%3A%20continuous%20wavelet%20measures%20of%20event-related%20brain%20potentials%20to%20emotional%20exclamations&amp;journal=Psychophysiology&amp;doi=10.1111%2Fj.1469-8986.2003.00142.x&amp;volume=41&amp;pages=259-268&amp;publication_year=2004&amp;author=Bostanov%2CV&amp;author=Kotchoubey%2CB">
 Google Scholar</a>&nbsp;
 </p><li class="c-article-references__item js-c-reading-companion-references-item"><p class=c-article-references__text id=ref-CR11>Breiman L (2001) Random forests. Mach Learn 45:5–32. <a href=https://doi.org/10.1023/A:1010933404324>https://doi.org/10.1023/A:1010933404324</a><p class="c-article-references__links u-hide-print"><a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="article reference" href=https://doi.org/10.1023%2FA%3A1010933404324 aria-label="Article reference 11">Article</a>&nbsp;
 <a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="math reference" href=http://www.emis.de/MATH-item?1007.68152 aria-label="MATH reference 11">MATH</a>&nbsp;
 <a data-track=click data-track-action="google scholar reference" data-track-label=link rel="nofollow noopener" aria-label="Google Scholar reference 11" href="http://scholar.google.com/scholar_lookup?&amp;title=Random%20forests&amp;journal=Mach%20Learn&amp;doi=10.1023%2FA%3A1010933404324&amp;volume=45&amp;pages=5-32&amp;publication_year=2001&amp;author=Breiman%2CL">
 Google Scholar</a>&nbsp;
 </p><li class="c-article-references__item js-c-reading-companion-references-item"><p class=c-article-references__text id=ref-CR13>Burkhardt F, Paeschke A, Rolfes M et al. (2005) A database of German emotional speech. In: European conference on speech and language processing, Lisbon, Portugal, pp. 1517–1520. <a href=https://www.researchgate.net/publication/221491017_A_database_of_German_emotional_speech>https://www.researchgate.net/publication/221491017_A_database_of_German_emotional_speech</a>. Accessed 10 Nov 2015</p><li class="c-article-references__item js-c-reading-companion-references-item"><p class=c-article-references__text id=ref-CR14>Calder AJ, Burton AM, Miller P et al. (2001) A principal component analysis of facial expressions. Vis Res 41:1179–1208. <a href=https://doi.org/10.1016/S0042-6989(01)00002-5>https://doi.org/10.1016/S0042-6989(01)00002-5</a><p class="c-article-references__links u-hide-print"><a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="cas reference" href=https://www.nature.com/articles/cas-redirect/1:STN:280:DC%2BD3MzisVyltQ%3D%3D aria-label="CAS reference 13">CAS</a>&nbsp;
 <a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="article reference" href=https://doi.org/10.1016%2FS0042-6989%2801%2900002-5 aria-label="Article reference 13">Article</a>&nbsp;
 <a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=11292507" aria-label="PubMed reference 13">PubMed</a>&nbsp;
 <a data-track=click data-track-action="google scholar reference" data-track-label=link rel="nofollow noopener" aria-label="Google Scholar reference 13" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20principal%20component%20analysis%20of%20facial%20expressions&amp;journal=Vis%20Res&amp;doi=10.1016%2FS0042-6989%2801%2900002-5&amp;volume=41&amp;pages=1179-1208&amp;publication_year=2001&amp;author=Calder%2CAJ&amp;author=Burton%2CAM&amp;author=Miller%2CP">
 Google Scholar</a>&nbsp;
 </p><li class="c-article-references__item js-c-reading-companion-references-item"><p class=c-article-references__text id=ref-CR15>Castro SL, Lima CF (2010) Recognizing emotions in spoken language: a validated set of Portuguese sentences and pseudosentences for research on emotional prosody. Behav Res Methods 42:74–81. <a href=https://doi.org/10.3758/BRM.42.1.74>https://doi.org/10.3758/BRM.42.1.74</a><p class="c-article-references__links u-hide-print"><a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="article reference" href=https://doi.org/10.3758%2FBRM.42.1.74 aria-label="Article reference 14">Article</a>&nbsp;
 <a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=20160287" aria-label="PubMed reference 14">PubMed</a>&nbsp;
 <a data-track=click data-track-action="google scholar reference" data-track-label=link rel="nofollow noopener" aria-label="Google Scholar reference 14" href="http://scholar.google.com/scholar_lookup?&amp;title=Recognizing%20emotions%20in%20spoken%20language%3A%20a%20validated%20set%20of%20Portuguese%20sentences%20and%20pseudosentences%20for%20research%20on%20emotional%20prosody&amp;journal=Behav%20Res%20Methods&amp;doi=10.3758%2FBRM.42.1.74&amp;volume=42&amp;pages=74-81&amp;publication_year=2010&amp;author=Castro%2CSL&amp;author=Lima%2CCF">
 Google Scholar</a>&nbsp;
 </p><li class="c-article-references__item js-c-reading-companion-references-item"><p class=c-article-references__text id=ref-CR16>Chronaki G, Wigelsworth M, Pell MD et al. (2018) The development of cross-cultural recognition of vocal emotions during childhood and adolescence. Sci Rep. 8:8659. <a href=https://doi.org/10.1038/s41598-018-26889-1>https://doi.org/10.1038/s41598-018-26889-1</a><p class="c-article-references__links u-hide-print"><a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="ads reference" href="http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT&amp;bibcode=2018NatSR...8.8659C" aria-label="ADS reference 15">ADS</a>&nbsp;
 <a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="cas reference" href=https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BC1cXhvVCjsb3J aria-label="CAS reference 15">CAS</a>&nbsp;
 <a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="article reference" href=https://doi.org/10.1038%2Fs41598-018-26889-1 aria-label="Article reference 15">Article</a>&nbsp;
 <a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=29904120" aria-label="PubMed reference 15">PubMed</a>&nbsp;
 <a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="pubmed central reference" href=http://www.ncbi.nlm.nih.gov/pmc/articles/PMC6002529 aria-label="PubMed Central reference 15">PubMed Central</a>&nbsp;
 <a data-track=click data-track-action="google scholar reference" data-track-label=link rel="nofollow noopener" aria-label="Google Scholar reference 15" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20development%20of%20cross-cultural%20recognition%20of%20vocal%20emotions%20during%20childhood%20and%20adolescence&amp;journal=Sci%20Rep.&amp;doi=10.1038%2Fs41598-018-26889-1&amp;volume=8&amp;publication_year=2018&amp;author=Chronaki%2CG&amp;author=Wigelsworth%2CM&amp;author=Pell%2CMD">
 Google Scholar</a>&nbsp;
 </p><li class="c-article-references__item js-c-reading-companion-references-item"><p class=c-article-references__text id=ref-CR19>Cordaro DT, Keltner D, Tshering S et al. (2016) The voice conveys emotion in ten globalized cultures and one remote village in Bhutan. Emotion 16:117–128. <a href=https://doi.org/10.1037/emo0000100>https://doi.org/10.1037/emo0000100</a><p class="c-article-references__links u-hide-print"><a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="article reference" href=https://doi.org/10.1037%2Femo0000100 aria-label="Article reference 16">Article</a>&nbsp;
 <a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=26389648" aria-label="PubMed reference 16">PubMed</a>&nbsp;
 <a data-track=click data-track-action="google scholar reference" data-track-label=link rel="nofollow noopener" aria-label="Google Scholar reference 16" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20voice%20conveys%20emotion%20in%20ten%20globalized%20cultures%20and%20one%20remote%20village%20in%20Bhutan&amp;journal=Emotion&amp;doi=10.1037%2Femo0000100&amp;volume=16&amp;pages=117-128&amp;publication_year=2016&amp;author=Cordaro%2CDT&amp;author=Keltner%2CD&amp;author=Tshering%2CS">
 Google Scholar</a>&nbsp;
 </p><li class="c-article-references__item js-c-reading-companion-references-item"><p class=c-article-references__text id=ref-CR20>Cornew L, Carver L, Love T (2009) There’s more to emotion than meets the eye: a processing bias for neutral content in the domain of emotional prosody. Cogn Emot 24:1133–1152. <a href=https://doi.org/10.1080/02699930903247492>https://doi.org/10.1080/02699930903247492</a><p class="c-article-references__links u-hide-print"><a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="article reference" href=https://doi.org/10.1080%2F02699930903247492 aria-label="Article reference 17">Article</a>&nbsp;
 <a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=21552425" aria-label="PubMed reference 17">PubMed</a>&nbsp;
 <a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="pubmed central reference" href=http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3088090 aria-label="PubMed Central reference 17">PubMed Central</a>&nbsp;
 <a data-track=click data-track-action="google scholar reference" data-track-label=link rel="nofollow noopener" aria-label="Google Scholar reference 17" href="http://scholar.google.com/scholar_lookup?&amp;title=There%E2%80%99s%20more%20to%20emotion%20than%20meets%20the%20eye%3A%20a%20processing%20bias%20for%20neutral%20content%20in%20the%20domain%20of%20emotional%20prosody&amp;journal=Cogn%20Emot&amp;doi=10.1080%2F02699930903247492&amp;volume=24&amp;pages=1133-1152&amp;publication_year=2009&amp;author=Cornew%2CL&amp;author=Carver%2CL&amp;author=Love%2CT">
 Google Scholar</a>&nbsp;
 </p><li class="c-article-references__item js-c-reading-companion-references-item"><p class=c-article-references__text id=ref-CR21>Cowen AS, Laukka P, Elfenbein HA et al. (2019a) The primacy of categories in the recognition of 12 emotions in speech prosody across two cultures. Nat Hum Behav 3:369–382. <a href=https://doi.org/10.1038/s41562-019-0533-6>https://doi.org/10.1038/s41562-019-0533-6</a><p class="c-article-references__links u-hide-print"><a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="article reference" href=https://doi.org/10.1038%2Fs41562-019-0533-6 aria-label="Article reference 18">Article</a>&nbsp;
 <a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=30971794" aria-label="PubMed reference 18">PubMed</a>&nbsp;
 <a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="pubmed central reference" href=http://www.ncbi.nlm.nih.gov/pmc/articles/PMC6687085 aria-label="PubMed Central reference 18">PubMed Central</a>&nbsp;
 <a data-track=click data-track-action="google scholar reference" data-track-label=link rel="nofollow noopener" aria-label="Google Scholar reference 18" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20primacy%20of%20categories%20in%20the%20recognition%20of%2012%20emotions%20in%20speech%20prosody%20across%20two%20cultures&amp;journal=Nat%20Hum%20Behav&amp;doi=10.1038%2Fs41562-019-0533-6&amp;volume=3&amp;pages=369-382&amp;publication_year=2019&amp;author=Cowen%2CAS&amp;author=Laukka%2CP&amp;author=Elfenbein%2CHA">
 Google Scholar</a>&nbsp;
 </p><li class="c-article-references__item js-c-reading-companion-references-item"><p class=c-article-references__text id=ref-CR22>Cowen AS, Elfenbein HA, Laukka P et al. (2019b) Mapping 24 emotions conveyed by brief human vocalization. Am Psychol 74:698–712. <a href=https://doi.org/10.1037/amp0000399>https://doi.org/10.1037/amp0000399</a><p class="c-article-references__links u-hide-print"><a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="article reference" href=https://doi.org/10.1037%2Famp0000399 aria-label="Article reference 19">Article</a>&nbsp;
 <a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=30570267" aria-label="PubMed reference 19">PubMed</a>&nbsp;
 <a data-track=click data-track-action="google scholar reference" data-track-label=link rel="nofollow noopener" aria-label="Google Scholar reference 19" href="http://scholar.google.com/scholar_lookup?&amp;title=Mapping%2024%20emotions%20conveyed%20by%20brief%20human%20vocalization&amp;journal=Am%20Psychol&amp;doi=10.1037%2Famp0000399&amp;volume=74&amp;pages=698-712&amp;publication_year=2019&amp;author=Cowen%2CAS&amp;author=Elfenbein%2CHA&amp;author=Laukka%2CP">
 Google Scholar</a>&nbsp;
 </p><li class="c-article-references__item js-c-reading-companion-references-item"><p class=c-article-references__text id=ref-CR23>Cox DR, Snell EJ (1989) Analysis of binary data, 2nd edn. Chapman &amp; Hall</p><li class="c-article-references__item js-c-reading-companion-references-item"><p class=c-article-references__text id=ref-CR25>Dixon SJ, Brereton RG (2009) Comparison of performance of five common classifiers represented as boundary methods: Euclidean distance to centroids, linear discriminant analysis, quadratic discriminant analysis, learning vector quantization and support vector machines, as dependent on data structure. Chemom Intell Lab Syst 95:1–17. <a href=https://doi.org/10.1016/j.chemolab.2008.07.010>https://doi.org/10.1016/j.chemolab.2008.07.010</a><p class="c-article-references__links u-hide-print"><a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="cas reference" href=https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BD1cXhsFagur7L aria-label="CAS reference 21">CAS</a>&nbsp;
 <a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="article reference" href=https://doi.org/10.1016%2Fj.chemolab.2008.07.010 aria-label="Article reference 21">Article</a>&nbsp;
 <a data-track=click data-track-action="google scholar reference" data-track-label=link rel="nofollow noopener" aria-label="Google Scholar reference 21" href="http://scholar.google.com/scholar_lookup?&amp;title=Comparison%20of%20performance%20of%20five%20common%20classifiers%20represented%20as%20boundary%20methods%3A%20Euclidean%20distance%20to%20centroids%2C%20linear%20discriminant%20analysis%2C%20quadratic%20discriminant%20analysis%2C%20learning%20vector%20quantization%20and%20support%20vector%20machines%2C%20as%20dependent%20on%20data%20structure&amp;journal=Chemom%20Intell%20Lab%20Syst&amp;doi=10.1016%2Fj.chemolab.2008.07.010&amp;volume=95&amp;pages=1-17&amp;publication_year=2009&amp;author=Dixon%2CSJ&amp;author=Brereton%2CRG">
 Google Scholar</a>&nbsp;
 </p><li class="c-article-references__item js-c-reading-companion-references-item"><p class=c-article-references__text id=ref-CR26>Dunlosky J, Metcalfe J (2009) Confidence judgements. In: Dunlosky J, Metcalfe J (eds) Metacognition, 1st edn. Sage Publications, Washington, pp. 118–139<p class="c-article-references__links u-hide-print"><a data-track=click data-track-action="google scholar reference" data-track-label=link rel="nofollow noopener" aria-label="Google Scholar reference 22" href="http://scholar.google.com/scholar_lookup?&amp;title=Confidence%20judgements&amp;pages=118-139&amp;publication_year=2009&amp;author=Dunlosky%2CJ&amp;author=Metcalfe%2CJ">
 Google Scholar</a>&nbsp;
 </p><li class="c-article-references__item js-c-reading-companion-references-item"><p class=c-article-references__text id=ref-CR27>Eyben F, Scherer KR, Schuller BW et al. (2016) The Geneva minimalistic acoustic parameter set (GeMAPS) for voice research and affective computing. IEEE Trans Affect Comput 7:190–202. <a href=https://doi.org/10.1109/TAFFC.2015.2457417>https://doi.org/10.1109/TAFFC.2015.2457417</a><p class="c-article-references__links u-hide-print"><a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="article reference" href=https://doi.org/10.1109%2FTAFFC.2015.2457417 aria-label="Article reference 23">Article</a>&nbsp;
 <a data-track=click data-track-action="google scholar reference" data-track-label=link rel="nofollow noopener" aria-label="Google Scholar reference 23" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20Geneva%20minimalistic%20acoustic%20parameter%20set%20%28GeMAPS%29%20for%20voice%20research%20and%20affective%20computing&amp;journal=IEEE%20Trans%20Affect%20Comput&amp;doi=10.1109%2FTAFFC.2015.2457417&amp;volume=7&amp;pages=190-202&amp;publication_year=2016&amp;author=Eyben%2CF&amp;author=Scherer%2CKR&amp;author=Schuller%2CBW">
 Google Scholar</a>&nbsp;
 </p><li class="c-article-references__item js-c-reading-companion-references-item"><p class=c-article-references__text id=ref-CR28>Goddard C (2014) Interjections and emotion (with special reference to “surprise” and “disgust”). Emot Rev 6:53–63. <a href=https://doi.org/10.1177/1754073913491843>https://doi.org/10.1177/1754073913491843</a><p class="c-article-references__links u-hide-print"><a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="article reference" href=https://doi.org/10.1177%2F1754073913491843 aria-label="Article reference 24">Article</a>&nbsp;
 <a data-track=click data-track-action="google scholar reference" data-track-label=link rel="nofollow noopener" aria-label="Google Scholar reference 24" href="http://scholar.google.com/scholar_lookup?&amp;title=Interjections%20and%20emotion%20%28with%20special%20reference%20to%20%E2%80%9Csurprise%E2%80%9D%20and%20%E2%80%9Cdisgust%E2%80%9D%29&amp;journal=Emot%20Rev&amp;doi=10.1177%2F1754073913491843&amp;volume=6&amp;pages=53-63&amp;publication_year=2014&amp;author=Goddard%2CC">
 Google Scholar</a>&nbsp;
 </p><li class="c-article-references__item js-c-reading-companion-references-item"><p class=c-article-references__text id=ref-CR29>Goudbeek M, Scherer KR (2010) Beyond arousal: valence and potency/control cues in the vocal expression of emotion. J Acoust Soc Am 128:1322–1336. <a href=https://doi.org/10.1121/1.3466853>https://doi.org/10.1121/1.3466853</a><p class="c-article-references__links u-hide-print"><a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="ads reference" href="http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT&amp;bibcode=2010ASAJ..128.1322G" aria-label="ADS reference 25">ADS</a>&nbsp;
 <a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="article reference" href=https://doi.org/10.1121%2F1.3466853 aria-label="Article reference 25">Article</a>&nbsp;
 <a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=20815467" aria-label="PubMed reference 25">PubMed</a>&nbsp;
 <a data-track=click data-track-action="google scholar reference" data-track-label=link rel="nofollow noopener" aria-label="Google Scholar reference 25" href="http://scholar.google.com/scholar_lookup?&amp;title=Beyond%20arousal%3A%20valence%20and%20potency%2Fcontrol%20cues%20in%20the%20vocal%20expression%20of%20emotion&amp;journal=J%20Acoust%20Soc%20Am&amp;doi=10.1121%2F1.3466853&amp;volume=128&amp;pages=1322-1336&amp;publication_year=2010&amp;author=Goudbeek%2CM&amp;author=Scherer%2CKR">
 Google Scholar</a>&nbsp;
 </p><li class="c-article-references__item js-c-reading-companion-references-item"><p class=c-article-references__text id=ref-CR30>Hamilton DL, Huffman LJ (1971) Generality of impression-formation processes for evaluative and nonevaluative judgments. J Pers Soc Psychol 20:200–207. <a href=https://doi.org/10.1037/h0031698>https://doi.org/10.1037/h0031698</a><p class="c-article-references__links u-hide-print"><a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="article reference" href=https://doi.org/10.1037%2Fh0031698 aria-label="Article reference 26">Article</a>&nbsp;
 <a data-track=click data-track-action="google scholar reference" data-track-label=link rel="nofollow noopener" aria-label="Google Scholar reference 26" href="http://scholar.google.com/scholar_lookup?&amp;title=Generality%20of%20impression-formation%20processes%20for%20evaluative%20and%20nonevaluative%20judgments&amp;journal=J%20Pers%20Soc%20Psychol&amp;doi=10.1037%2Fh0031698&amp;volume=20&amp;pages=200-207&amp;publication_year=1971&amp;author=Hamilton%2CDL&amp;author=Huffman%2CLJ">
 Google Scholar</a>&nbsp;
 </p><li class="c-article-references__item js-c-reading-companion-references-item"><p class=c-article-references__text id=ref-CR31>Hamilton DL, Zanna MP (1972) Differential weighting of favorable and unfavorable attributes in impressions of personality. J Exp Res Pers 6:204–212<p class="c-article-references__links u-hide-print"><a data-track=click data-track-action="google scholar reference" data-track-label=link rel="nofollow noopener" aria-label="Google Scholar reference 27" href="http://scholar.google.com/scholar_lookup?&amp;title=Differential%20weighting%20of%20favorable%20and%20unfavorable%20attributes%20in%20impressions%20of%20personality&amp;journal=J%20Exp%20Res%20Pers&amp;volume=6&amp;pages=204-212&amp;publication_year=1972&amp;author=Hamilton%2CDL&amp;author=Zanna%2CMP">
 Google Scholar</a>&nbsp;
 </p><li class="c-article-references__item js-c-reading-companion-references-item"><p class=c-article-references__text id=ref-CR32>Hammerschmidt K, Juergens U (2007) Acoustical correlates of affective prosody. J Voice 21:531–540. <a href=https://doi.org/10.1016/j.jvoice.2006.03.002>https://doi.org/10.1016/j.jvoice.2006.03.002</a><p class="c-article-references__links u-hide-print"><a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="article reference" href=https://doi.org/10.1016%2Fj.jvoice.2006.03.002 aria-label="Article reference 28">Article</a>&nbsp;
 <a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=16647247" aria-label="PubMed reference 28">PubMed</a>&nbsp;
 <a data-track=click data-track-action="google scholar reference" data-track-label=link rel="nofollow noopener" aria-label="Google Scholar reference 28" href="http://scholar.google.com/scholar_lookup?&amp;title=Acoustical%20correlates%20of%20affective%20prosody&amp;journal=J%20Voice&amp;doi=10.1016%2Fj.jvoice.2006.03.002&amp;volume=21&amp;pages=531-540&amp;publication_year=2007&amp;author=Hammerschmidt%2CK&amp;author=Juergens%2CU">
 Google Scholar</a>&nbsp;
 </p><li class="c-article-references__item js-c-reading-companion-references-item"><p class=c-article-references__text id=ref-CR33>Hawk ST, van Kleef GA, Fischer AH et al. (2009) “Worth a thousand words”: absolute and relative decoding of nonlinguistic affect vocalizations. Emotion 9:293–305. <a href=https://doi.org/10.1037/a0015178>https://doi.org/10.1037/a0015178</a><p class="c-article-references__links u-hide-print"><a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="article reference" href=https://doi.org/10.1037%2Fa0015178 aria-label="Article reference 29">Article</a>&nbsp;
 <a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=19485607" aria-label="PubMed reference 29">PubMed</a>&nbsp;
 <a data-track=click data-track-action="google scholar reference" data-track-label=link rel="nofollow noopener" aria-label="Google Scholar reference 29" href="http://scholar.google.com/scholar_lookup?&amp;title=%E2%80%9CWorth%20a%20thousand%20words%E2%80%9D%3A%20absolute%20and%20relative%20decoding%20of%20nonlinguistic%20affect%20vocalizations&amp;journal=Emotion&amp;doi=10.1037%2Fa0015178&amp;volume=9&amp;pages=293-305&amp;publication_year=2009&amp;author=Hawk%2CST&amp;author=Kleef%2CGA&amp;author=Fischer%2CAH">
 Google Scholar</a>&nbsp;
 </p><li class="c-article-references__item js-c-reading-companion-references-item"><p class=c-article-references__text id=ref-CR34>Hothorn T, Bretz F, Westfall P (2008) Simultaneous inference in general parametric models. Biom J 50:346–363. <a href=https://doi.org/10.1002/bimj.200810425>https://doi.org/10.1002/bimj.200810425</a><p class="c-article-references__links u-hide-print"><a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="mathscinet reference" href="http://www.ams.org/mathscinet-getitem?mr=2521547" aria-label="MathSciNet reference 30">MathSciNet</a>&nbsp;
 <a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="article reference" href=https://doi.org/10.1002%2Fbimj.200810425 aria-label="Article reference 30">Article</a>&nbsp;
 <a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=18481363" aria-label="PubMed reference 30">PubMed</a>&nbsp;
 <a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="math reference" href=http://www.emis.de/MATH-item?1442.62415 aria-label="MATH reference 30">MATH</a>&nbsp;
 <a data-track=click data-track-action="google scholar reference" data-track-label=link rel="nofollow noopener" aria-label="Google Scholar reference 30" href="http://scholar.google.com/scholar_lookup?&amp;title=Simultaneous%20inference%20in%20general%20parametric%20models&amp;journal=Biom%20J&amp;doi=10.1002%2Fbimj.200810425&amp;volume=50&amp;pages=346-363&amp;publication_year=2008&amp;author=Hothorn%2CT&amp;author=Bretz%2CF&amp;author=Westfall%2CP">
 Google Scholar</a>&nbsp;
 </p><li class="c-article-references__item js-c-reading-companion-references-item"><p class=c-article-references__text id=ref-CR35>Ito TA, Larsen JT, Smith NK et al. (1998) Negative information weighs more heavily on the brain: the negativity bias in evaluative categorizations. J Pers Soc Psychol 75:887–900. <a href=https://doi.org/10.1037/0022-3514.75.4.887>https://doi.org/10.1037/0022-3514.75.4.887</a><p class="c-article-references__links u-hide-print"><a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="cas reference" href=https://www.nature.com/articles/cas-redirect/1:STN:280:DyaK1M%2Fkt12ltQ%3D%3D aria-label="CAS reference 31">CAS</a>&nbsp;
 <a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="article reference" href=https://doi.org/10.1037%2F0022-3514.75.4.887 aria-label="Article reference 31">Article</a>&nbsp;
 <a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=9825526" aria-label="PubMed reference 31">PubMed</a>&nbsp;
 <a data-track=click data-track-action="google scholar reference" data-track-label=link rel="nofollow noopener" aria-label="Google Scholar reference 31" href="http://scholar.google.com/scholar_lookup?&amp;title=Negative%20information%20weighs%20more%20heavily%20on%20the%20brain%3A%20the%20negativity%20bias%20in%20evaluative%20categorizations&amp;journal=J%20Pers%20Soc%20Psychol&amp;doi=10.1037%2F0022-3514.75.4.887&amp;volume=75&amp;pages=887-900&amp;publication_year=1998&amp;author=Ito%2CTA&amp;author=Larsen%2CJT&amp;author=Smith%2CNK">
 Google Scholar</a>&nbsp;
 </p><li class="c-article-references__item js-c-reading-companion-references-item"><p class=c-article-references__text id=ref-CR36>James G, Witten D, Hastie T et al. (2013) An introduction to statistical learning with applications in R. In: Cassella G, Fienberg S, Olkin I (eds) Springer texts in statistics. Springer, New York, pp. 303–332<p class="c-article-references__links u-hide-print"><a data-track=click data-track-action="google scholar reference" data-track-label=link rel="nofollow noopener" aria-label="Google Scholar reference 32" href="http://scholar.google.com/scholar_lookup?&amp;title=An%20introduction%20to%20statistical%20learning%20with%20applications%20in%20R&amp;pages=303-332&amp;publication_year=2013&amp;author=James%2CG&amp;author=Witten%2CD&amp;author=Hastie%2CT">
 Google Scholar</a>&nbsp;
 </p><li class="c-article-references__item js-c-reading-companion-references-item"><p class=c-article-references__text id=ref-CR37>Jiang X, Pell MD (2017) The sound of confidence and doubt. Speech Commun 88:106–126. <a href=https://doi.org/10.1016/j.specom.2017.01.011>https://doi.org/10.1016/j.specom.2017.01.011</a><p class="c-article-references__links u-hide-print"><a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="article reference" href=https://doi.org/10.1016%2Fj.specom.2017.01.011 aria-label="Article reference 33">Article</a>&nbsp;
 <a data-track=click data-track-action="google scholar reference" data-track-label=link rel="nofollow noopener" aria-label="Google Scholar reference 33" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20sound%20of%20confidence%20and%20doubt&amp;journal=Speech%20Commun&amp;doi=10.1016%2Fj.specom.2017.01.011&amp;volume=88&amp;pages=106-126&amp;publication_year=2017&amp;author=Jiang%2CX&amp;author=Pell%2CMD">
 Google Scholar</a>&nbsp;
 </p><li class="c-article-references__item js-c-reading-companion-references-item"><p class=c-article-references__text id=ref-CR38>Jiang X, Pell DM (2014) Encoding and decoding confidence information in speech. In: Proceedings of the 7th international conference in speech prosody (social and linguistic speech prosody). pp. 573–576. <a href=http://fastnet.netsoc.ie/sp7/sp7book.pdf>http://fastnet.netsoc.ie/sp7/sp7book.pdf</a>. Accessed 30 Nov 2018</p><li class="c-article-references__item js-c-reading-companion-references-item"><p class=c-article-references__text id=ref-CR39>Johnstone T, Scherer KR (2000) Vocal communication of emotion. In: Lewis M, Haviland J (eds) The handbook of emotion, 2nd edn. Guildford, New York, pp. 220–235<p class="c-article-references__links u-hide-print"><a data-track=click data-track-action="google scholar reference" data-track-label=link rel="nofollow noopener" aria-label="Google Scholar reference 35" href="http://scholar.google.com/scholar_lookup?&amp;title=Vocal%20communication%20of%20emotion&amp;pages=220-235&amp;publication_year=2000&amp;author=Johnstone%2CT&amp;author=Scherer%2CKR">
 Google Scholar</a>&nbsp;
 </p><li class="c-article-references__item js-c-reading-companion-references-item"><p class=c-article-references__text id=ref-CR40>Juslin PN, Scherer KR (2005) Vocal expression of affect. In: Harrigan JA, Rosenthal R, Scherer KR (eds) The new handbook of methods in nonverbal behavior research, 1st edn. Oxford University Press, Oxford, pp. 65–135<p class="c-article-references__links u-hide-print"><a data-track=click data-track-action="google scholar reference" data-track-label=link rel="nofollow noopener" aria-label="Google Scholar reference 36" href="http://scholar.google.com/scholar_lookup?&amp;title=Vocal%20expression%20of%20affect&amp;publication_year=2005&amp;author=Juslin%2CPN&amp;author=Scherer%2CKR">
 Google Scholar</a>&nbsp;
 </p><li class="c-article-references__item js-c-reading-companion-references-item"><p class=c-article-references__text id=ref-CR41>Juslin PN, Laukka P (2003) Communication of emotions in vocal expression and music performance: different channels, same code? Psychol Bull 129:770–814. <a href=https://doi.org/10.1037/0033-2909.129.5.770>https://doi.org/10.1037/0033-2909.129.5.770</a><p class="c-article-references__links u-hide-print"><a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="article reference" href=https://doi.org/10.1037%2F0033-2909.129.5.770 aria-label="Article reference 37">Article</a>&nbsp;
 <a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=12956543" aria-label="PubMed reference 37">PubMed</a>&nbsp;
 <a data-track=click data-track-action="google scholar reference" data-track-label=link rel="nofollow noopener" aria-label="Google Scholar reference 37" href="http://scholar.google.com/scholar_lookup?&amp;title=Communication%20of%20emotions%20in%20vocal%20expression%20and%20music%20performance%3A%20different%20channels%2C%20same%20code%3F&amp;journal=Psychol%20Bull&amp;doi=10.1037%2F0033-2909.129.5.770&amp;volume=129&amp;pages=770-814&amp;publication_year=2003&amp;author=Juslin%2CPN&amp;author=Laukka%2CP">
 Google Scholar</a>&nbsp;
 </p><li class="c-article-references__item js-c-reading-companion-references-item"><p class=c-article-references__text id=ref-CR42>Juslin PN, Laukka P (2001) Impact of intended emotion intensity on cue utilization and decoding accuracy in vocal expression of emotion. Emotion 1:381–412. <a href=https://doi.org/10.1037//1528-3542.1.4.381>https://doi.org/10.1037//1528-3542.1.4.381</a><p class="c-article-references__links u-hide-print"><a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="cas reference" href=https://www.nature.com/articles/cas-redirect/1:STN:280:DC%2BD3szmslWgsw%3D%3D aria-label="CAS reference 38">CAS</a>&nbsp;
 <a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="article reference" href=https://doi.org/10.1037%2F%2F1528-3542.1.4.381 aria-label="Article reference 38">Article</a>&nbsp;
 <a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=12901399" aria-label="PubMed reference 38">PubMed</a>&nbsp;
 <a data-track=click data-track-action="google scholar reference" data-track-label=link rel="nofollow noopener" aria-label="Google Scholar reference 38" href="http://scholar.google.com/scholar_lookup?&amp;title=Impact%20of%20intended%20emotion%20intensity%20on%20cue%20utilization%20and%20decoding%20accuracy%20in%20vocal%20expression%20of%20emotion&amp;journal=Emotion&amp;doi=10.1037%2F%2F1528-3542.1.4.381&amp;volume=1&amp;pages=381-412&amp;publication_year=2001&amp;author=Juslin%2CPN&amp;author=Laukka%2CP">
 Google Scholar</a>&nbsp;
 </p><li class="c-article-references__item js-c-reading-companion-references-item"><p class=c-article-references__text id=ref-CR43>Juergens R, Fischer J, Schacht A (2018) Hot speech and exploding bombs: autonomic arousal during emotion classification of prosodic utterances and affective sounds. Front Psychol 9:228. <a href=https://doi.org/10.3389/fpsyg.2018.00228>https://doi.org/10.3389/fpsyg.2018.00228</a><p class="c-article-references__links u-hide-print"><a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="article reference" href=https://doi.org/10.3389%2Ffpsyg.2018.00228 aria-label="Article reference 39">Article</a>&nbsp;
 <a data-track=click data-track-action="google scholar reference" data-track-label=link rel="nofollow noopener" aria-label="Google Scholar reference 39" href="http://scholar.google.com/scholar_lookup?&amp;title=Hot%20speech%20and%20exploding%20bombs%3A%20autonomic%20arousal%20during%20emotion%20classification%20of%20prosodic%20utterances%20and%20affective%20sounds&amp;journal=Front%20Psychol&amp;doi=10.3389%2Ffpsyg.2018.00228&amp;volume=9&amp;publication_year=2018&amp;author=Juergens%2CR&amp;author=Fischer%2CJ&amp;author=Schacht%2CA">
 Google Scholar</a>&nbsp;
 </p><li class="c-article-references__item js-c-reading-companion-references-item"><p class=c-article-references__text id=ref-CR44>Juergens R, Grass A, Drolet M et al. (2015) Effect of acting experience on emotion expression and recognition in voice: non-actors provide better stimuli than expected. J Nonverbal Behav 39:195–214. <a href=https://doi.org/10.1007/s10919-015-0209-5>https://doi.org/10.1007/s10919-015-0209-5</a><p class="c-article-references__links u-hide-print"><a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="article reference" href=https://doi.org/10.1007%2Fs10919-015-0209-5 aria-label="Article reference 40">Article</a>&nbsp;
 <a data-track=click data-track-action="google scholar reference" data-track-label=link rel="nofollow noopener" aria-label="Google Scholar reference 40" href="http://scholar.google.com/scholar_lookup?&amp;title=Effect%20of%20acting%20experience%20on%20emotion%20expression%20and%20recognition%20in%20voice%3A%20non-actors%20provide%20better%20stimuli%20than%20expected&amp;journal=J%20Nonverbal%20Behav&amp;doi=10.1007%2Fs10919-015-0209-5&amp;volume=39&amp;pages=195-214&amp;publication_year=2015&amp;author=Juergens%2CR&amp;author=Grass%2CA&amp;author=Drolet%2CM">
 Google Scholar</a>&nbsp;
 </p><li class="c-article-references__item js-c-reading-companion-references-item"><p class=c-article-references__text id=ref-CR45>Juergens R, Drolet M, Pirow R et al. (2013) Encoding conditions affect recognition of vocally expressed emotions across cultures. Front Psychol 4:111. <a href=https://doi.org/10.3389/fpsyg.2013.00111>https://doi.org/10.3389/fpsyg.2013.00111</a><p class="c-article-references__links u-hide-print"><a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="article reference" href=https://doi.org/10.3389%2Ffpsyg.2013.00111 aria-label="Article reference 41">Article</a>&nbsp;
 <a data-track=click data-track-action="google scholar reference" data-track-label=link rel="nofollow noopener" aria-label="Google Scholar reference 41" href="http://scholar.google.com/scholar_lookup?&amp;title=Encoding%20conditions%20affect%20recognition%20of%20vocally%20expressed%20emotions%20across%20cultures&amp;journal=Front%20Psychol&amp;doi=10.3389%2Ffpsyg.2013.00111&amp;volume=4&amp;publication_year=2013&amp;author=Juergens%2CR&amp;author=Drolet%2CM&amp;author=Pirow%2CR">
 Google Scholar</a>&nbsp;
 </p><li class="c-article-references__item js-c-reading-companion-references-item"><p class=c-article-references__text id=ref-CR46>Juergens R, Hammerschmidt K, Fischer J (2011) Authentic and play-acted vocal emotion expressions reveal acoustic differences. Front Psychol 2:180. <a href=https://doi.org/10.3389/fpsyg.2011.00180>https://doi.org/10.3389/fpsyg.2011.00180</a><p class="c-article-references__links u-hide-print"><a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="article reference" href=https://doi.org/10.3389%2Ffpsyg.2011.00180 aria-label="Article reference 42">Article</a>&nbsp;
 <a data-track=click data-track-action="google scholar reference" data-track-label=link rel="nofollow noopener" aria-label="Google Scholar reference 42" href="http://scholar.google.com/scholar_lookup?&amp;title=Authentic%20and%20play-acted%20vocal%20emotion%20expressions%20reveal%20acoustic%20differences&amp;journal=Front%20Psychol&amp;doi=10.3389%2Ffpsyg.2011.00180&amp;volume=2&amp;publication_year=2011&amp;author=Juergens%2CR&amp;author=Hammerschmidt%2CK&amp;author=Fischer%2CJ">
 Google Scholar</a>&nbsp;
 </p><li class="c-article-references__item js-c-reading-companion-references-item"><p class=c-article-references__text id=ref-CR47>Kelly KJ, Metcalfe J (2011) Metacognition of emotional face recognition. Emotion 11:896–906. <a href=https://doi.org/10.1037/a0023746>https://doi.org/10.1037/a0023746</a><p class="c-article-references__links u-hide-print"><a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="article reference" href=https://doi.org/10.1037%2Fa0023746 aria-label="Article reference 43">Article</a>&nbsp;
 <a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=21859205" aria-label="PubMed reference 43">PubMed</a>&nbsp;
 <a data-track=click data-track-action="google scholar reference" data-track-label=link rel="nofollow noopener" aria-label="Google Scholar reference 43" href="http://scholar.google.com/scholar_lookup?&amp;title=Metacognition%20of%20emotional%20face%20recognition&amp;journal=Emotion&amp;doi=10.1037%2Fa0023746&amp;volume=11&amp;pages=896-906&amp;publication_year=2011&amp;author=Kelly%2CKJ&amp;author=Metcalfe%2CJ">
 Google Scholar</a>&nbsp;
 </p><li class="c-article-references__item js-c-reading-companion-references-item"><p class=c-article-references__text id=ref-CR48>Kimble C, Seidel S (1991) Vocal signs of confidence. J Nonverbal Behav 15:99–105. <a href=https://doi.org/10.1007/BF00998265>https://doi.org/10.1007/BF00998265</a><p class="c-article-references__links u-hide-print"><a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="article reference" href=https://doi.org/10.1007%2FBF00998265 aria-label="Article reference 44">Article</a>&nbsp;
 <a data-track=click data-track-action="google scholar reference" data-track-label=link rel="nofollow noopener" aria-label="Google Scholar reference 44" href="http://scholar.google.com/scholar_lookup?&amp;title=Vocal%20signs%20of%20confidence&amp;journal=J%20Nonverbal%20Behav&amp;doi=10.1007%2FBF00998265&amp;volume=15&amp;pages=99-105&amp;publication_year=1991&amp;author=Kimble%2CC&amp;author=Seidel%2CS">
 Google Scholar</a>&nbsp;
 </p><li class="c-article-references__item js-c-reading-companion-references-item"><p class=c-article-references__text id=ref-CR49>Kitayama S, Ishii K (2002) Word and voice: spontaneous attention to emotional speech in two cultures. Cogn Emot 16:29–59. <a href=https://doi.org/10.1080/0269993943000121>https://doi.org/10.1080/0269993943000121</a><p class="c-article-references__links u-hide-print"><a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="article reference" href=https://doi.org/10.1080%2F0269993943000121 aria-label="Article reference 45">Article</a>&nbsp;
 <a data-track=click data-track-action="google scholar reference" data-track-label=link rel="nofollow noopener" aria-label="Google Scholar reference 45" href="http://scholar.google.com/scholar_lookup?&amp;title=Word%20and%20voice%3A%20spontaneous%20attention%20to%20emotional%20speech%20in%20two%20cultures&amp;journal=Cogn%20Emot&amp;doi=10.1080%2F0269993943000121&amp;volume=16&amp;pages=29-59&amp;publication_year=2002&amp;author=Kitayama%2CS&amp;author=Ishii%2CK">
 Google Scholar</a>&nbsp;
 </p><li class="c-article-references__item js-c-reading-companion-references-item"><p class=c-article-references__text id=ref-CR50>Kohler KJ (2008) ‘Speech-smile’, ‘speech-laugh’, ‘laughter’ and their sequencing in dialogic interaction. Phonetica 65:1–18. <a href=https://doi.org/10.1159/000130013>https://doi.org/10.1159/000130013</a><p class="c-article-references__links u-hide-print"><a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="article reference" href=https://doi.org/10.1159%2F000130013 aria-label="Article reference 46">Article</a>&nbsp;
 <a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=18523364" aria-label="PubMed reference 46">PubMed</a>&nbsp;
 <a data-track=click data-track-action="google scholar reference" data-track-label=link rel="nofollow noopener" aria-label="Google Scholar reference 46" href="http://scholar.google.com/scholar_lookup?&amp;title=%E2%80%98Speech-smile%E2%80%99%2C%20%E2%80%98speech-laugh%E2%80%99%2C%20%E2%80%98laughter%E2%80%99%20and%20their%20sequencing%20in%20dialogic%20interaction&amp;journal=Phonetica&amp;doi=10.1159%2F000130013&amp;volume=65&amp;pages=1-18&amp;publication_year=2008&amp;author=Kohler%2CKJ">
 Google Scholar</a>&nbsp;
 </p><li class="c-article-references__item js-c-reading-companion-references-item"><p class=c-article-references__text id=ref-CR51>Koriat A (2008) When confidence in a choice is independent of which choice is made. Psychon Bull Rev 15:997–1001. <a href=https://doi.org/10.3758/PBR.15.5.997>https://doi.org/10.3758/PBR.15.5.997</a><p class="c-article-references__links u-hide-print"><a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="article reference" href=https://doi.org/10.3758%2FPBR.15.5.997 aria-label="Article reference 47">Article</a>&nbsp;
 <a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=18926995" aria-label="PubMed reference 47">PubMed</a>&nbsp;
 <a data-track=click data-track-action="google scholar reference" data-track-label=link rel="nofollow noopener" aria-label="Google Scholar reference 47" href="http://scholar.google.com/scholar_lookup?&amp;title=When%20confidence%20in%20a%20choice%20is%20independent%20of%20which%20choice%20is%20made&amp;journal=Psychon%20Bull%20Rev&amp;doi=10.3758%2FPBR.15.5.997&amp;volume=15&amp;pages=997-1001&amp;publication_year=2008&amp;author=Koriat%2CA">
 Google Scholar</a>&nbsp;
 </p><li class="c-article-references__item js-c-reading-companion-references-item"><p class=c-article-references__text id=ref-CR52>Kotz SA, Paulmann S (2007) When emotional prosody and semantics dance cheek to cheek: ERP evidence. Brain Res 1151:107–118. <a href=https://doi.org/10.1016/j.brainres.2007.03.015>https://doi.org/10.1016/j.brainres.2007.03.015</a><p class="c-article-references__links u-hide-print"><a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="cas reference" href=https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BD2sXlt1CjsrY%3D aria-label="CAS reference 48">CAS</a>&nbsp;
 <a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="article reference" href=https://doi.org/10.1016%2Fj.brainres.2007.03.015 aria-label="Article reference 48">Article</a>&nbsp;
 <a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=17445783" aria-label="PubMed reference 48">PubMed</a>&nbsp;
 <a data-track=click data-track-action="google scholar reference" data-track-label=link rel="nofollow noopener" aria-label="Google Scholar reference 48" href="http://scholar.google.com/scholar_lookup?&amp;title=When%20emotional%20prosody%20and%20semantics%20dance%20cheek%20to%20cheek%3A%20ERP%20evidence&amp;journal=Brain%20Res&amp;doi=10.1016%2Fj.brainres.2007.03.015&amp;volume=1151&amp;pages=107-118&amp;publication_year=2007&amp;author=Kotz%2CSA&amp;author=Paulmann%2CS">
 Google Scholar</a>&nbsp;
 </p><li class="c-article-references__item js-c-reading-companion-references-item"><p class=c-article-references__text id=ref-CR53>Krumhuber EG, Scherer KR (2011) Affect bursts: dynamic patterns of facial expression. Emotion 11:825–841. <a href=https://doi.org/10.1037/a0023856>https://doi.org/10.1037/a0023856</a><p class="c-article-references__links u-hide-print"><a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="article reference" href=https://doi.org/10.1037%2Fa0023856 aria-label="Article reference 49">Article</a>&nbsp;
 <a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=21707163" aria-label="PubMed reference 49">PubMed</a>&nbsp;
 <a data-track=click data-track-action="google scholar reference" data-track-label=link rel="nofollow noopener" aria-label="Google Scholar reference 49" href="http://scholar.google.com/scholar_lookup?&amp;title=Affect%20bursts%3A%20dynamic%20patterns%20of%20facial%20expression&amp;journal=Emotion&amp;doi=10.1037%2Fa0023856&amp;volume=11&amp;pages=825-841&amp;publication_year=2011&amp;author=Krumhuber%2CEG&amp;author=Scherer%2CKR">
 Google Scholar</a>&nbsp;
 </p><li class="c-article-references__item js-c-reading-companion-references-item"><p class=c-article-references__text id=ref-CR54>Lausen A, Schacht A (2018) Gender differences in the recognition of vocal emotions. Front Psychol 9:882. <a href=https://doi.org/10.3389/fpsyg.2018.00882>https://doi.org/10.3389/fpsyg.2018.00882</a><p class="c-article-references__links u-hide-print"><a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="article reference" href=https://doi.org/10.3389%2Ffpsyg.2018.00882 aria-label="Article reference 50">Article</a>&nbsp;
 <a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=29922202" aria-label="PubMed reference 50">PubMed</a>&nbsp;
 <a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="pubmed central reference" href=http://www.ncbi.nlm.nih.gov/pmc/articles/PMC5996252 aria-label="PubMed Central reference 50">PubMed Central</a>&nbsp;
 <a data-track=click data-track-action="google scholar reference" data-track-label=link rel="nofollow noopener" aria-label="Google Scholar reference 50" href="http://scholar.google.com/scholar_lookup?&amp;title=Gender%20differences%20in%20the%20recognition%20of%20vocal%20emotions&amp;journal=Front%20Psychol&amp;doi=10.3389%2Ffpsyg.2018.00882&amp;volume=9&amp;publication_year=2018&amp;author=Lausen%2CA&amp;author=Schacht%2CA">
 Google Scholar</a>&nbsp;
 </p><li class="c-article-references__item js-c-reading-companion-references-item"><p class=c-article-references__text id=ref-CR55>Lima CF, Castro SL, Scott SK (2013) When voices get emotional: a corpus of nonverbal vocalizations for research on emotion processing. Behav Res Methods 45:1234–1245. <a href=https://doi.org/10.3758/s13428-013-0324-3>https://doi.org/10.3758/s13428-013-0324-3</a><p class="c-article-references__links u-hide-print"><a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="article reference" href=https://doi.org/10.3758%2Fs13428-013-0324-3 aria-label="Article reference 51">Article</a>&nbsp;
 <a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=23444120" aria-label="PubMed reference 51">PubMed</a>&nbsp;
 <a data-track=click data-track-action="google scholar reference" data-track-label=link rel="nofollow noopener" aria-label="Google Scholar reference 51" href="http://scholar.google.com/scholar_lookup?&amp;title=When%20voices%20get%20emotional%3A%20a%20corpus%20of%20nonverbal%20vocalizations%20for%20research%20on%20emotion%20processing&amp;journal=Behav%20Res%20Methods&amp;doi=10.3758%2Fs13428-013-0324-3&amp;volume=45&amp;pages=1234-1245&amp;publication_year=2013&amp;author=Lima%2CCF&amp;author=Castro%2CSL&amp;author=Scott%2CSK">
 Google Scholar</a>&nbsp;
 </p><li class="c-article-references__item js-c-reading-companion-references-item"><p class=c-article-references__text id=ref-CR56>Liu T, Pinheiro AP, Deng G et al. (2012) Electrophysiological insights into processing nonverbal emotional vocalizations. Neuroreport 23:108–112. <a href=https://doi.org/10.1097/WNR.0b013e32834ea757>https://doi.org/10.1097/WNR.0b013e32834ea757</a><p class="c-article-references__links u-hide-print"><a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="article reference" href=https://doi.org/10.1097%2FWNR.0b013e32834ea757 aria-label="Article reference 52">Article</a>&nbsp;
 <a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=22134115" aria-label="PubMed reference 52">PubMed</a>&nbsp;
 <a data-track=click data-track-action="google scholar reference" data-track-label=link rel="nofollow noopener" aria-label="Google Scholar reference 52" href="http://scholar.google.com/scholar_lookup?&amp;title=Electrophysiological%20insights%20into%20processing%20nonverbal%20emotional%20vocalizations&amp;journal=Neuroreport&amp;doi=10.1097%2FWNR.0b013e32834ea757&amp;volume=23&amp;pages=108-112&amp;publication_year=2012&amp;author=Liu%2CT&amp;author=Pinheiro%2CAP&amp;author=Deng%2CG">
 Google Scholar</a>&nbsp;
 </p><li class="c-article-references__item js-c-reading-companion-references-item"><p class=c-article-references__text id=ref-CR57>Metcalfe J, Schwartz BL, Joaquim SG (1993) The cue-familiarity heuristic in metacognition. J Exp Psychol Learn Mem Cogn 19:851–861. <a href=https://doi.org/10.1037//0278-7393.19.4.851>https://doi.org/10.1037//0278-7393.19.4.851</a><p class="c-article-references__links u-hide-print"><a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="cas reference" href=https://www.nature.com/articles/cas-redirect/1:STN:280:DyaK3szks12gtg%3D%3D aria-label="CAS reference 53">CAS</a>&nbsp;
 <a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="article reference" href=https://doi.org/10.1037%2F%2F0278-7393.19.4.851 aria-label="Article reference 53">Article</a>&nbsp;
 <a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=8345327" aria-label="PubMed reference 53">PubMed</a>&nbsp;
 <a data-track=click data-track-action="google scholar reference" data-track-label=link rel="nofollow noopener" aria-label="Google Scholar reference 53" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20cue-familiarity%20heuristic%20in%20metacognition&amp;journal=J%20Exp%20Psychol%20Learn%20Mem%20Cogn&amp;doi=10.1037%2F%2F0278-7393.19.4.851&amp;volume=19&amp;pages=851-861&amp;publication_year=1993&amp;author=Metcalfe%2CJ&amp;author=Schwartz%2CBL&amp;author=Joaquim%2CSG">
 Google Scholar</a>&nbsp;
 </p><li class="c-article-references__item js-c-reading-companion-references-item"><p class=c-article-references__text id=ref-CR58>Mitchell RLC, Elliott R, Barry M et al. (2003) The neural response to emotional prosody, as revealed by functional magnetic resonance imaging. Neuropsychologia 41:1410–1421. <a href=https://doi.org/10.1016/S0028-3932(03)00017-4>https://doi.org/10.1016/S0028-3932(03)00017-4</a><p class="c-article-references__links u-hide-print"><a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="article reference" href=https://doi.org/10.1016%2FS0028-3932%2803%2900017-4 aria-label="Article reference 54">Article</a>&nbsp;
 <a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=12757912" aria-label="PubMed reference 54">PubMed</a>&nbsp;
 <a data-track=click data-track-action="google scholar reference" data-track-label=link rel="nofollow noopener" aria-label="Google Scholar reference 54" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20neural%20response%20to%20emotional%20prosody%2C%20as%20revealed%20by%20functional%20magnetic%20resonance%20imaging&amp;journal=Neuropsychologia&amp;doi=10.1016%2FS0028-3932%2803%2900017-4&amp;volume=41&amp;pages=1410-1421&amp;publication_year=2003&amp;author=Mitchell%2CRLC&amp;author=Elliott%2CR&amp;author=Barry%2CM">
 Google Scholar</a>&nbsp;
 </p><li class="c-article-references__item js-c-reading-companion-references-item"><p class=c-article-references__text id=ref-CR59>Mozziconacci S (2002) Prosody and emotions. In: Proceedings of speech prosody, pp. 1–9. <a href=https://www.isca-speech.org/archive/sp2002/>https://www.isca-speech.org/archive/sp2002/</a>. Accessed 30 Nov 2018</p><li class="c-article-references__item js-c-reading-companion-references-item"><p class=c-article-references__text id=ref-CR60>Noroozi F, Sapiński T, Kamińska D et al. (2017) Vocal-based emotion recognition using random forests and decision tree. Int J Speech Technol 20:239–246. <a href=https://doi.org/10.1007/s10772-017-9396-2>https://doi.org/10.1007/s10772-017-9396-2</a><p class="c-article-references__links u-hide-print"><a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="article reference" href=https://doi.org/10.1007%2Fs10772-017-9396-2 aria-label="Article reference 56">Article</a>&nbsp;
 <a data-track=click data-track-action="google scholar reference" data-track-label=link rel="nofollow noopener" aria-label="Google Scholar reference 56" href="http://scholar.google.com/scholar_lookup?&amp;title=Vocal-based%20emotion%20recognition%20using%20random%20forests%20and%20decision%20tree&amp;journal=Int%20J%20Speech%20Technol&amp;doi=10.1007%2Fs10772-017-9396-2&amp;volume=20&amp;pages=239-246&amp;publication_year=2017&amp;author=Noroozi%2CF&amp;author=Sapi%C5%84ski%2CT&amp;author=Kami%C5%84ska%2CD">
 Google Scholar</a>&nbsp;
 </p><li class="c-article-references__item js-c-reading-companion-references-item"><p class=c-article-references__text id=ref-CR97>Norsonic Nor140 (2017) Instruction manual. Lierskogen, Norway. <a href=https://www.campbell-associates.co.uk/norsonic-140-sound-level-meter>https://www.campbell-associates.co.uk/norsonic-140-sound-level-meter</a></p><li class="c-article-references__item js-c-reading-companion-references-item"><p class=c-article-references__text id=ref-CR61>Nygaard L, Lunders E (2002) Resolution of lexical ambiguity by emotional tone of voice. Mem Cogn 30:583–593. <a href=https://doi.org/10.3758/BF03194959>https://doi.org/10.3758/BF03194959</a><p class="c-article-references__links u-hide-print"><a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="article reference" href=https://doi.org/10.3758%2FBF03194959 aria-label="Article reference 58">Article</a>&nbsp;
 <a data-track=click data-track-action="google scholar reference" data-track-label=link rel="nofollow noopener" aria-label="Google Scholar reference 58" href="http://scholar.google.com/scholar_lookup?&amp;title=Resolution%20of%20lexical%20ambiguity%20by%20emotional%20tone%20of%20voice&amp;journal=Mem%20Cogn&amp;doi=10.3758%2FBF03194959&amp;volume=30&amp;pages=583-593&amp;publication_year=2002&amp;author=Nygaard%2CL&amp;author=Lunders%2CE">
 Google Scholar</a>&nbsp;
 </p><li class="c-article-references__item js-c-reading-companion-references-item"><p class=c-article-references__text id=ref-CR62>Owren MJ (2008) GSU Praat tools: scripts for modifying and analyzing sounds using Praat acoustics software. Behav Res Methods 40:822–829. <a href=https://doi.org/10.3758/BRM.40.3.822>https://doi.org/10.3758/BRM.40.3.822</a><p class="c-article-references__links u-hide-print"><a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="article reference" href=https://doi.org/10.3758%2FBRM.40.3.822 aria-label="Article reference 59">Article</a>&nbsp;
 <a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=18697678" aria-label="PubMed reference 59">PubMed</a>&nbsp;
 <a data-track=click data-track-action="google scholar reference" data-track-label=link rel="nofollow noopener" aria-label="Google Scholar reference 59" href="http://scholar.google.com/scholar_lookup?&amp;title=GSU%20Praat%20tools%3A%20scripts%20for%20modifying%20and%20analyzing%20sounds%20using%20Praat%20acoustics%20software&amp;journal=Behav%20Res%20Methods&amp;doi=10.3758%2FBRM.40.3.822&amp;volume=40&amp;pages=822-829&amp;publication_year=2008&amp;author=Owren%2CMJ">
 Google Scholar</a>&nbsp;
 </p><li class="c-article-references__item js-c-reading-companion-references-item"><p class=c-article-references__text id=ref-CR63>Oehman A (1993) Fear and anxiety as emotional phenomenon: clinical phenomenology, evolutionary perspectives, and information-processing mechanisms. In: Lewis M, Haviland JM (eds) Handbook of emotions. Guildford Press, New York, pp. 511–536<p class="c-article-references__links u-hide-print"><a data-track=click data-track-action="google scholar reference" data-track-label=link rel="nofollow noopener" aria-label="Google Scholar reference 60" href="http://scholar.google.com/scholar_lookup?&amp;title=Fear%20and%20anxiety%20as%20emotional%20phenomenon%3A%20clinical%20phenomenology%2C%20evolutionary%20perspectives%2C%20and%20information-processing%20mechanisms&amp;pages=511-536&amp;publication_year=1993&amp;author=Oehman%2CA">
 Google Scholar</a>&nbsp;
 </p><li class="c-article-references__item js-c-reading-companion-references-item"><p class=c-article-references__text id=ref-CR64>Parsons CE, Young KS, Craske MG et al. (2014) Introducing the Oxford Vocal (OxVoc) Sounds database: a validated set of non-acted affective sounds from human infants, adults, and domestic animals. Front Psychol 5:562. <a href=https://doi.org/10.3389/fpsyg.2014.00562>https://doi.org/10.3389/fpsyg.2014.00562</a><p class="c-article-references__links u-hide-print"><a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="article reference" href=https://doi.org/10.3389%2Ffpsyg.2014.00562 aria-label="Article reference 61">Article</a>&nbsp;
 <a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=25009511" aria-label="PubMed reference 61">PubMed</a>&nbsp;
 <a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="pubmed central reference" href=http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4068198 aria-label="PubMed Central reference 61">PubMed Central</a>&nbsp;
 <a data-track=click data-track-action="google scholar reference" data-track-label=link rel="nofollow noopener" aria-label="Google Scholar reference 61" href="http://scholar.google.com/scholar_lookup?&amp;title=Introducing%20the%20Oxford%20Vocal%20%28OxVoc%29%20Sounds%20database%3A%20a%20validated%20set%20of%20non-acted%20affective%20sounds%20from%20human%20infants%2C%20adults%2C%20and%20domestic%20animals&amp;journal=Front%20Psychol&amp;doi=10.3389%2Ffpsyg.2014.00562&amp;volume=5&amp;publication_year=2014&amp;author=Parsons%2CCE&amp;author=Young%2CKS&amp;author=Craske%2CMG">
 Google Scholar</a>&nbsp;
 </p><li class="c-article-references__item js-c-reading-companion-references-item"><p class=c-article-references__text id=ref-CR65>Paulmann S (2016) The neurocognition of prosody. In: Hickok G, Small S (eds) Neurobiology of language. Elsevier, San Diego, pp. 1109–1120<p class="c-article-references__links u-hide-print"><a data-track=click data-track-action="google scholar reference" data-track-label=link rel="nofollow noopener" aria-label="Google Scholar reference 62" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20neurocognition%20of%20prosody&amp;publication_year=2016&amp;author=Paulmann%2CS">
 Google Scholar</a>&nbsp;
 </p><li class="c-article-references__item js-c-reading-companion-references-item"><p class=c-article-references__text id=ref-CR66>Paulmann S, Uskul AK (2014) Cross-cultural emotional prosody recognition: evidence from Chinese and British listeners. Cogn Emot 28:230–244. <a href=https://doi.org/10.1080/02699931.2013.812033>https://doi.org/10.1080/02699931.2013.812033</a><p class="c-article-references__links u-hide-print"><a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="article reference" href=https://doi.org/10.1080%2F02699931.2013.812033 aria-label="Article reference 63">Article</a>&nbsp;
 <a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=23862740" aria-label="PubMed reference 63">PubMed</a>&nbsp;
 <a data-track=click data-track-action="google scholar reference" data-track-label=link rel="nofollow noopener" aria-label="Google Scholar reference 63" href="http://scholar.google.com/scholar_lookup?&amp;title=Cross-cultural%20emotional%20prosody%20recognition%3A%20evidence%20from%20Chinese%20and%20British%20listeners&amp;journal=Cogn%20Emot&amp;doi=10.1080%2F02699931.2013.812033&amp;volume=28&amp;pages=230-244&amp;publication_year=2014&amp;author=Paulmann%2CS&amp;author=Uskul%2CAK">
 Google Scholar</a>&nbsp;
 </p><li class="c-article-references__item js-c-reading-companion-references-item"><p class=c-article-references__text id=ref-CR67>Paulmann S, Kotz SA (2008) An ERP investigation on the temporal dynamics of emotional prosody and emotional semantics in pseudo- and lexical sentence context. Brain Lang 105:59–69. <a href=https://doi.org/10.1016/j.bandl.2007.11.005>https://doi.org/10.1016/j.bandl.2007.11.005</a><p class="c-article-references__links u-hide-print"><a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="article reference" href=https://doi.org/10.1016%2Fj.bandl.2007.11.005 aria-label="Article reference 64">Article</a>&nbsp;
 <a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=18177699" aria-label="PubMed reference 64">PubMed</a>&nbsp;
 <a data-track=click data-track-action="google scholar reference" data-track-label=link rel="nofollow noopener" aria-label="Google Scholar reference 64" href="http://scholar.google.com/scholar_lookup?&amp;title=An%20ERP%20investigation%20on%20the%20temporal%20dynamics%20of%20emotional%20prosody%20and%20emotional%20semantics%20in%20pseudo-%20and%20lexical%20sentence%20context&amp;journal=Brain%20Lang&amp;doi=10.1016%2Fj.bandl.2007.11.005&amp;volume=105&amp;pages=59-69&amp;publication_year=2008&amp;author=Paulmann%2CS&amp;author=Kotz%2CSA">
 Google Scholar</a>&nbsp;
 </p><li class="c-article-references__item js-c-reading-companion-references-item"><p class=c-article-references__text id=ref-CR68>Paulmann S, Pell MD, Kotz SA (2008) How aging affects the recognition of emotional speech. Brain Lang 104:262–269. <a href=https://doi.org/10.1016/j.bandl.2007.03.002>https://doi.org/10.1016/j.bandl.2007.03.002</a><p class="c-article-references__links u-hide-print"><a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="article reference" href=https://doi.org/10.1016%2Fj.bandl.2007.03.002 aria-label="Article reference 65">Article</a>&nbsp;
 <a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=17428529" aria-label="PubMed reference 65">PubMed</a>&nbsp;
 <a data-track=click data-track-action="google scholar reference" data-track-label=link rel="nofollow noopener" aria-label="Google Scholar reference 65" href="http://scholar.google.com/scholar_lookup?&amp;title=How%20aging%20affects%20the%20recognition%20of%20emotional%20speech&amp;journal=Brain%20Lang&amp;doi=10.1016%2Fj.bandl.2007.03.002&amp;volume=104&amp;pages=262-269&amp;publication_year=2008&amp;author=Paulmann%2CS&amp;author=Pell%2CMD&amp;author=Kotz%2CSA">
 Google Scholar</a>&nbsp;
 </p><li class="c-article-references__item js-c-reading-companion-references-item"><p class=c-article-references__text id=ref-CR69>Pell MD, Rothermich K, Liu P et al. (2015) Preferential decoding of emotion from human non-linguistic vocalizations versus speech prosody. Biol Psychol 111:14–25. <a href=https://doi.org/10.1016/j.biopsycho.2015.08.008>https://doi.org/10.1016/j.biopsycho.2015.08.008</a><p class="c-article-references__links u-hide-print"><a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="cas reference" href=https://www.nature.com/articles/cas-redirect/1:STN:280:DC%2BC287mtFKmsA%3D%3D aria-label="CAS reference 66">CAS</a>&nbsp;
 <a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="article reference" href=https://doi.org/10.1016%2Fj.biopsycho.2015.08.008 aria-label="Article reference 66">Article</a>&nbsp;
 <a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=26307467" aria-label="PubMed reference 66">PubMed</a>&nbsp;
 <a data-track=click data-track-action="google scholar reference" data-track-label=link rel="nofollow noopener" aria-label="Google Scholar reference 66" href="http://scholar.google.com/scholar_lookup?&amp;title=Preferential%20decoding%20of%20emotion%20from%20human%20non-linguistic%20vocalizations%20versus%20speech%20prosody&amp;journal=Biol%20Psychol&amp;doi=10.1016%2Fj.biopsycho.2015.08.008&amp;volume=111&amp;pages=14-25&amp;publication_year=2015&amp;author=Pell%2CMD&amp;author=Rothermich%2CK&amp;author=Liu%2CP">
 Google Scholar</a>&nbsp;
 </p><li class="c-article-references__item js-c-reading-companion-references-item"><p class=c-article-references__text id=ref-CR70>Pell MD, Kotz SA (2011) On the time course of vocal emotion recognition. PLoS ONE 6(11):e27256. <a href=https://doi.org/10.1371/journal.pone.0027256>https://doi.org/10.1371/journal.pone.0027256</a><p class="c-article-references__links u-hide-print"><a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="ads reference" href="http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT&amp;bibcode=2011PLoSO...627256P" aria-label="ADS reference 67">ADS</a>&nbsp;
 <a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="cas reference" href=https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BC3MXhsFemtr7J aria-label="CAS reference 67">CAS</a>&nbsp;
 <a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="article reference" href=https://doi.org/10.1371%2Fjournal.pone.0027256 aria-label="Article reference 67">Article</a>&nbsp;
 <a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=22087275" aria-label="PubMed reference 67">PubMed</a>&nbsp;
 <a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="pubmed central reference" href=http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3210149 aria-label="PubMed Central reference 67">PubMed Central</a>&nbsp;
 <a data-track=click data-track-action="google scholar reference" data-track-label=link rel="nofollow noopener" aria-label="Google Scholar reference 67" href="http://scholar.google.com/scholar_lookup?&amp;title=On%20the%20time%20course%20of%20vocal%20emotion%20recognition&amp;journal=PLoS%20ONE&amp;doi=10.1371%2Fjournal.pone.0027256&amp;volume=6&amp;issue=11&amp;publication_year=2011&amp;author=Pell%2CMD&amp;author=Kotz%2CSA">
 Google Scholar</a>&nbsp;
 </p><li class="c-article-references__item js-c-reading-companion-references-item"><p class=c-article-references__text id=ref-CR71>Pell MD, Jaywant A, Monetta L et al. (2011) Emotional speech processing: disentangling the effects of prosody and semantic cues. Cogn Emot 25:834–853. <a href=https://doi.org/10.1080/02699931.2010.516915>https://doi.org/10.1080/02699931.2010.516915</a><p class="c-article-references__links u-hide-print"><a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="article reference" href=https://doi.org/10.1080%2F02699931.2010.516915 aria-label="Article reference 68">Article</a>&nbsp;
 <a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=21824024" aria-label="PubMed reference 68">PubMed</a>&nbsp;
 <a data-track=click data-track-action="google scholar reference" data-track-label=link rel="nofollow noopener" aria-label="Google Scholar reference 68" href="http://scholar.google.com/scholar_lookup?&amp;title=Emotional%20speech%20processing%3A%20disentangling%20the%20effects%20of%20prosody%20and%20semantic%20cues&amp;journal=Cogn%20Emot&amp;doi=10.1080%2F02699931.2010.516915&amp;volume=25&amp;pages=834-853&amp;publication_year=2011&amp;author=Pell%2CMD&amp;author=Jaywant%2CA&amp;author=Monetta%2CL">
 Google Scholar</a>&nbsp;
 </p><li class="c-article-references__item js-c-reading-companion-references-item"><p class=c-article-references__text id=ref-CR72>Pell MD, Monetta L, Paulmann S et al. (2009) Recognizing emotions in a foreign language. J Nonverbal Behav 33:107–120. <a href=https://doi.org/10.1007/s10919-008-0065-7>https://doi.org/10.1007/s10919-008-0065-7</a><p class="c-article-references__links u-hide-print"><a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="article reference" href=https://doi.org/10.1007%2Fs10919-008-0065-7 aria-label="Article reference 69">Article</a>&nbsp;
 <a data-track=click data-track-action="google scholar reference" data-track-label=link rel="nofollow noopener" aria-label="Google Scholar reference 69" href="http://scholar.google.com/scholar_lookup?&amp;title=Recognizing%20emotions%20in%20a%20foreign%20language&amp;journal=J%20Nonverbal%20Behav&amp;doi=10.1007%2Fs10919-008-0065-7&amp;volume=33&amp;pages=107-120&amp;publication_year=2009&amp;author=Pell%2CMD&amp;author=Monetta%2CL&amp;author=Paulmann%2CS">
 Google Scholar</a>&nbsp;
 </p><li class="c-article-references__item js-c-reading-companion-references-item"><p class=c-article-references__text id=ref-CR73>Pell MD, Paulmann S, Dara C et al. (2009) Factors in the recognition of vocally expressed emotions: a comparison of four languages. J Phon 37:417–435. <a href=https://doi.org/10.1016/j.wocn.2009.07.005>https://doi.org/10.1016/j.wocn.2009.07.005</a><p class="c-article-references__links u-hide-print"><a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="article reference" href=https://doi.org/10.1016%2Fj.wocn.2009.07.005 aria-label="Article reference 70">Article</a>&nbsp;
 <a data-track=click data-track-action="google scholar reference" data-track-label=link rel="nofollow noopener" aria-label="Google Scholar reference 70" href="http://scholar.google.com/scholar_lookup?&amp;title=Factors%20in%20the%20recognition%20of%20vocally%20expressed%20emotions%3A%20a%20comparison%20of%20four%20languages&amp;journal=J%20Phon&amp;doi=10.1016%2Fj.wocn.2009.07.005&amp;volume=37&amp;pages=417-435&amp;publication_year=2009&amp;author=Pell%2CMD&amp;author=Paulmann%2CS&amp;author=Dara%2CC">
 Google Scholar</a>&nbsp;
 </p><li class="c-article-references__item js-c-reading-companion-references-item"><p class=c-article-references__text id=ref-CR74>Peeters G, Czapinski J (1990) Positive–negative asymmetry in evaluations: the distinction between affective and informational negativity effects. In: Stroebe W, Hewstone M (eds) European review of social psychology, vol. 1. Wiley, Chichester, pp. 33–60<p class="c-article-references__links u-hide-print"><a data-track=click data-track-action="google scholar reference" data-track-label=link rel="nofollow noopener" aria-label="Google Scholar reference 71" href="http://scholar.google.com/scholar_lookup?&amp;title=Positive%E2%80%93negative%20asymmetry%20in%20evaluations%3A%20the%20distinction%20between%20affective%20and%20informational%20negativity%20effects&amp;pages=33-60&amp;publication_year=1990&amp;author=Peeters%2CG&amp;author=Czapinski%2CJ">
 Google Scholar</a>&nbsp;
 </p><li class="c-article-references__item js-c-reading-companion-references-item"><p class=c-article-references__text id=ref-CR75>Pichora-Fuller MK, Dupuis K, Van Lieshout P (2016) Importance of F0 for predicting vocal emotion categorization. J Acoust Soc Am 140:3401–3401. <a href=https://doi.org/10.1121/1.4970917>https://doi.org/10.1121/1.4970917</a><p class="c-article-references__links u-hide-print"><a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="ads reference" href="http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT&amp;bibcode=2016ASAJ..140.3401P" aria-label="ADS reference 72">ADS</a>&nbsp;
 <a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="article reference" href=https://doi.org/10.1121%2F1.4970917 aria-label="Article reference 72">Article</a>&nbsp;
 <a data-track=click data-track-action="google scholar reference" data-track-label=link rel="nofollow noopener" aria-label="Google Scholar reference 72" href="http://scholar.google.com/scholar_lookup?&amp;title=Importance%20of%20F0%20for%20predicting%20vocal%20emotion%20categorization&amp;journal=J%20Acoust%20Soc%20Am&amp;doi=10.1121%2F1.4970917&amp;volume=140&amp;pages=3401-3401&amp;publication_year=2016&amp;author=Pichora-Fuller%2CMK&amp;author=Dupuis%2CK&amp;author=Lieshout%2CP">
 Google Scholar</a>&nbsp;
 </p><li class="c-article-references__item js-c-reading-companion-references-item"><p class=c-article-references__text id=ref-CR76>R Core Team (2017) R: a language and environment for statistical computing. R Foundation for Statistical Computing, Vienna<p class="c-article-references__links u-hide-print"><a data-track=click data-track-action="google scholar reference" data-track-label=link rel="nofollow noopener" aria-label="Google Scholar reference 73" href="http://scholar.google.com/scholar_lookup?&amp;title=R%3A%20a%20language%20and%20environment%20for%20statistical%20computing&amp;publication_year=2017">
 Google Scholar</a>&nbsp;
 </p><li class="c-article-references__item js-c-reading-companion-references-item"><p class=c-article-references__text id=ref-CR77>Rigoulot S, Wassiliwizky E, Pell MD (2013) Feeling backwards? How temporal order in speech affects the time course of vocal emotion recognition. Front Psychol 4:367. <a href=https://doi.org/10.3389/fpsyg.2013.00367>https://doi.org/10.3389/fpsyg.2013.00367</a><p class="c-article-references__links u-hide-print"><a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="article reference" href=https://doi.org/10.3389%2Ffpsyg.2013.00367 aria-label="Article reference 74">Article</a>&nbsp;
 <a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=23805115" aria-label="PubMed reference 74">PubMed</a>&nbsp;
 <a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="pubmed central reference" href=http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3690349 aria-label="PubMed Central reference 74">PubMed Central</a>&nbsp;
 <a data-track=click data-track-action="google scholar reference" data-track-label=link rel="nofollow noopener" aria-label="Google Scholar reference 74" href="http://scholar.google.com/scholar_lookup?&amp;title=Feeling%20backwards%3F%20How%20temporal%20order%20in%20speech%20affects%20the%20time%20course%20of%20vocal%20emotion%20recognition&amp;journal=Front%20Psychol&amp;doi=10.3389%2Ffpsyg.2013.00367&amp;volume=4&amp;publication_year=2013&amp;author=Rigoulot%2CS&amp;author=Wassiliwizky%2CE&amp;author=Pell%2CMD">
 Google Scholar</a>&nbsp;
 </p><li class="c-article-references__item js-c-reading-companion-references-item"><p class=c-article-references__text id=ref-CR78>Sauter DA, Eisner F, Calder AJ et al. (2010) Perceptual cues in nonverbal vocal expressions of emotion. Q J Exp Psychol 63:2251–2272. <a href=https://doi.org/10.1080/17470211003721642>https://doi.org/10.1080/17470211003721642</a><p class="c-article-references__links u-hide-print"><a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="article reference" href=https://doi.org/10.1080%2F17470211003721642 aria-label="Article reference 75">Article</a>&nbsp;
 <a data-track=click data-track-action="google scholar reference" data-track-label=link rel="nofollow noopener" aria-label="Google Scholar reference 75" href="http://scholar.google.com/scholar_lookup?&amp;title=Perceptual%20cues%20in%20nonverbal%20vocal%20expressions%20of%20emotion&amp;journal=Q%20J%20Exp%20Psychol&amp;doi=10.1080%2F17470211003721642&amp;volume=63&amp;pages=2251-2272&amp;publication_year=2010&amp;author=Sauter%2CDA&amp;author=Eisner%2CF&amp;author=Calder%2CAJ">
 Google Scholar</a>&nbsp;
 </p><li class="c-article-references__item js-c-reading-companion-references-item"><p class=c-article-references__text id=ref-CR79>Sauter DA (2006) An investigation into vocal expressions of emotions: the roles of valence, culture, and acoustic factors. Unpublished Ph.D. thesis, University College London</p><li class="c-article-references__item js-c-reading-companion-references-item"><p class=c-article-references__text id=ref-CR80>Sbattella L, Colombo L, Rinaldi C et al. (2014) Extracting emotions and communication styles from prosody. In: da Silva H, Holzinger A, Fairclough S, Majoe D (eds) Physiological computing systems, vol. 8908. Springer, Heidelberg, pp. 21–42<p class="c-article-references__links u-hide-print"><a data-track=click data-track-action="google scholar reference" data-track-label=link rel="nofollow noopener" aria-label="Google Scholar reference 77" href="http://scholar.google.com/scholar_lookup?&amp;title=Extracting%20emotions%20and%20communication%20styles%20from%20prosody&amp;publication_year=2014&amp;author=Sbattella%2CL&amp;author=Colombo%2CL&amp;author=Rinaldi%2CC">
 Google Scholar</a>&nbsp;
 </p><li class="c-article-references__item js-c-reading-companion-references-item"><p class=c-article-references__text id=ref-CR81>Schaerlaeken S, Grandjean D (2018) Unfolding and dynamics of affect bursts decoding in humans. PLoS ONE 13:e0206215. <a href=https://doi.org/10.1371/journal.pone.0206216>https://doi.org/10.1371/journal.pone.0206216</a><p class="c-article-references__links u-hide-print"><a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="cas reference" href=https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BC1MXjt1Olsb4%3D aria-label="CAS reference 78">CAS</a>&nbsp;
 <a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="article reference" href=https://doi.org/10.1371%2Fjournal.pone.0206216 aria-label="Article reference 78">Article</a>&nbsp;
 <a data-track=click data-track-action="google scholar reference" data-track-label=link rel="nofollow noopener" aria-label="Google Scholar reference 78" href="http://scholar.google.com/scholar_lookup?&amp;title=Unfolding%20and%20dynamics%20of%20affect%20bursts%20decoding%20in%20humans&amp;journal=PLoS%20ONE&amp;doi=10.1371%2Fjournal.pone.0206216&amp;volume=13&amp;publication_year=2018&amp;author=Schaerlaeken%2CS&amp;author=Grandjean%2CD">
 Google Scholar</a>&nbsp;
 </p><li class="c-article-references__item js-c-reading-companion-references-item"><p class=c-article-references__text id=ref-CR82>Scherer KR, Baenziger T (2004) Emotional expression in prosody: a review and an agenda for future research. In: Bel B, Marlien I (eds) Speech prosody, Nara, Japan, pp. 359–366</p><li class="c-article-references__item js-c-reading-companion-references-item"><p class=c-article-references__text id=ref-CR83>Scherer KR, Banse R, Wallbott H (2001) Emotion inferences from vocal expression correlate across languages and cultures. J Cross Cult Psychol 32:76–92. <a href=https://doi.org/10.1177/0022022101032001009>https://doi.org/10.1177/0022022101032001009</a><p class="c-article-references__links u-hide-print"><a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="article reference" href=https://doi.org/10.1177%2F0022022101032001009 aria-label="Article reference 80">Article</a>&nbsp;
 <a data-track=click data-track-action="google scholar reference" data-track-label=link rel="nofollow noopener" aria-label="Google Scholar reference 80" href="http://scholar.google.com/scholar_lookup?&amp;title=Emotion%20inferences%20from%20vocal%20expression%20correlate%20across%20languages%20and%20cultures&amp;journal=J%20Cross%20Cult%20Psychol&amp;doi=10.1177%2F0022022101032001009&amp;volume=32&amp;pages=76-92&amp;publication_year=2001&amp;author=Scherer%2CKR&amp;author=Banse%2CR&amp;author=Wallbott%2CH">
 Google Scholar</a>&nbsp;
 </p><li class="c-article-references__item js-c-reading-companion-references-item"><p class=c-article-references__text id=ref-CR84>Scherer KR (1994) Affect bursts. In: van Goozen SHM, van de Poll NE, Sergeant JA (eds) Emotions: essays on emotion theory. Erlbaum, Hillsdale, pp. 161–193<p class="c-article-references__links u-hide-print"><a data-track=click data-track-action="google scholar reference" data-track-label=link rel="nofollow noopener" aria-label="Google Scholar reference 81" href="http://scholar.google.com/scholar_lookup?&amp;title=Affect%20bursts&amp;pages=161-193&amp;publication_year=1994&amp;author=Scherer%2CKR">
 Google Scholar</a>&nbsp;
 </p><li class="c-article-references__item js-c-reading-companion-references-item"><p class=c-article-references__text id=ref-CR85>Scherer KR, London H, Wolf J (1973) The voice of confidence: Paralinguistic cues and audience evaluation. J Res Pers 7:31–44. <a href=https://doi.org/10.1016/0092-6566(73)90030-5>https://doi.org/10.1016/0092-6566(73)90030-5</a><p class="c-article-references__links u-hide-print"><a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="article reference" href=https://doi.org/10.1016%2F0092-6566%2873%2990030-5 aria-label="Article reference 82">Article</a>&nbsp;
 <a data-track=click data-track-action="google scholar reference" data-track-label=link rel="nofollow noopener" aria-label="Google Scholar reference 82" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20voice%20of%20confidence%3A%20Paralinguistic%20cues%20and%20audience%20evaluation&amp;journal=J%20Res%20Pers&amp;doi=10.1016%2F0092-6566%2873%2990030-5&amp;volume=7&amp;pages=31-44&amp;publication_year=1973&amp;author=Scherer%2CKR&amp;author=London%2CH&amp;author=Wolf%2CJ">
 Google Scholar</a>&nbsp;
 </p><li class="c-article-references__item js-c-reading-companion-references-item"><p class=c-article-references__text id=ref-CR86>Schirmer A (2010) Mark my words: tone of voice changes affective word representations in memory. PLoS ONE 5(2):e9080. <a href=https://doi.org/10.1371/journal.pone.0009080>https://doi.org/10.1371/journal.pone.0009080</a><p class="c-article-references__links u-hide-print"><a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="ads reference" href="http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT&amp;bibcode=2010PLoSO...5.9080S" aria-label="ADS reference 83">ADS</a>&nbsp;
 <a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="cas reference" href=https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BC3cXitFGns7g%3D aria-label="CAS reference 83">CAS</a>&nbsp;
 <a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="article reference" href=https://doi.org/10.1371%2Fjournal.pone.0009080 aria-label="Article reference 83">Article</a>&nbsp;
 <a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=20169154" aria-label="PubMed reference 83">PubMed</a>&nbsp;
 <a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="pubmed central reference" href=http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2821399 aria-label="PubMed Central reference 83">PubMed Central</a>&nbsp;
 <a data-track=click data-track-action="google scholar reference" data-track-label=link rel="nofollow noopener" aria-label="Google Scholar reference 83" href="http://scholar.google.com/scholar_lookup?&amp;title=Mark%20my%20words%3A%20tone%20of%20voice%20changes%20affective%20word%20representations%20in%20memory&amp;journal=PLoS%20ONE&amp;doi=10.1371%2Fjournal.pone.0009080&amp;volume=5&amp;issue=2&amp;publication_year=2010&amp;author=Schirmer%2CA">
 Google Scholar</a>&nbsp;
 </p><li class="c-article-references__item js-c-reading-companion-references-item"><p class=c-article-references__text id=ref-CR87>Schirmer A, Kotz SA (2003) ERP evidence for a sex-specific Stroop effect in emotional speech. J Cogn Neurosci 15:1135–1148. <a href=https://doi.org/10.1162/089892903322598102>https://doi.org/10.1162/089892903322598102</a><p class="c-article-references__links u-hide-print"><a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="article reference" href=https://doi.org/10.1162%2F089892903322598102 aria-label="Article reference 84">Article</a>&nbsp;
 <a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=14709232" aria-label="PubMed reference 84">PubMed</a>&nbsp;
 <a data-track=click data-track-action="google scholar reference" data-track-label=link rel="nofollow noopener" aria-label="Google Scholar reference 84" href="http://scholar.google.com/scholar_lookup?&amp;title=ERP%20evidence%20for%20a%20sex-specific%20Stroop%20effect%20in%20emotional%20speech&amp;journal=J%20Cogn%20Neurosci&amp;doi=10.1162%2F089892903322598102&amp;volume=15&amp;pages=1135-1148&amp;publication_year=2003&amp;author=Schirmer%2CA&amp;author=Kotz%2CSA">
 Google Scholar</a>&nbsp;
 </p><li class="c-article-references__item js-c-reading-companion-references-item"><p class=c-article-references__text id=ref-CR88>Scott SK, Sauter D, McGettigan C (2010) Brain mechanisms for processing perceived emotional vocalizations in humans. In: Brudzynski SM (ed), Handbook of behavioral neuroscience, Elsevier, pp. 187–197</p><li class="c-article-references__item js-c-reading-companion-references-item"><p class=c-article-references__text id=ref-CR89>Seber GAF (1984) Multivariate observations. John Wiley &amp; Sons</p><li class="c-article-references__item js-c-reading-companion-references-item"><p class=c-article-references__text id=ref-CR91>Thompson WF, Balkwill LL (2009) Cross-cultural similarities and differences. In: Juslin PN, Sloboda JA (eds) Handbook of music and emotion: theory, research, applications, 1st edn. Oxford University Press, New York, pp. 755–791. <a href=https://doi.org/10.1093/acprof:oso/9780199230143.003.0027>https://doi.org/10.1093/acprof:oso/9780199230143.003.0027</a></p><li class="c-article-references__item js-c-reading-companion-references-item"><p class=c-article-references__text id=ref-CR92>Toivanen J, Väyrynen E, Sepännen T (2004) Automatic discrimination of emotion from spoken Finnish. Lang Speech 47:383–412. <a href=https://doi.org/10.1177/00238309040470040301>https://doi.org/10.1177/00238309040470040301</a><p class="c-article-references__links u-hide-print"><a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="article reference" href=https://doi.org/10.1177%2F00238309040470040301 aria-label="Article reference 88">Article</a>&nbsp;
 <a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=16038449" aria-label="PubMed reference 88">PubMed</a>&nbsp;
 <a data-track=click data-track-action="google scholar reference" data-track-label=link rel="nofollow noopener" aria-label="Google Scholar reference 88" href="http://scholar.google.com/scholar_lookup?&amp;title=Automatic%20discrimination%20of%20emotion%20from%20spoken%20Finnish&amp;journal=Lang%20Speech&amp;doi=10.1177%2F00238309040470040301&amp;volume=47&amp;pages=383-412&amp;publication_year=2004&amp;author=Toivanen%2CJ&amp;author=V%C3%A4yrynen%2CE&amp;author=Sep%C3%A4nnen%2CT">
 Google Scholar</a>&nbsp;
 </p><li class="c-article-references__item js-c-reading-companion-references-item"><p class=c-article-references__text id=ref-CR93>Wagner HL (1993) On measuring performance in category judgement studies of nonverbal behaviour. J Nonverbal Behav 17:3–28. <a href=https://doi.org/10.1007/BF00987006>https://doi.org/10.1007/BF00987006</a><p class="c-article-references__links u-hide-print"><a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="article reference" href=https://doi.org/10.1007%2FBF00987006 aria-label="Article reference 89">Article</a>&nbsp;
 <a data-track=click data-track-action="google scholar reference" data-track-label=link rel="nofollow noopener" aria-label="Google Scholar reference 89" href="http://scholar.google.com/scholar_lookup?&amp;title=On%20measuring%20performance%20in%20category%20judgement%20studies%20of%20nonverbal%20behaviour&amp;journal=J%20Nonverbal%20Behav&amp;doi=10.1007%2FBF00987006&amp;volume=17&amp;pages=3-28&amp;publication_year=1993&amp;author=Wagner%2CHL">
 Google Scholar</a>&nbsp;
 </p><li class="c-article-references__item js-c-reading-companion-references-item"><p class=c-article-references__text id=ref-CR94>Wendt B, Scheich H (2002) The “Magdeburger Prosodie Korpus”—a spoken language corpus for fMRI-Studies. In: Bel B, Marlien I (eds) Speech prosody. Aix-en-Provence, SproSIG, pp. 699–701</p><li class="c-article-references__item js-c-reading-companion-references-item"><p class=c-article-references__text id=ref-CR95>Wilson TD, Gilbert DT (2008) Explaining away: a model of affective adaptation. Perspect Psychol Sci 3:370–386. <a href=https://doi.org/10.1111/j.1745-6924.2008.00085.x>https://doi.org/10.1111/j.1745-6924.2008.00085.x</a><p class="c-article-references__links u-hide-print"><a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="article reference" href=https://doi.org/10.1111%2Fj.1745-6924.2008.00085.x aria-label="Article reference 91">Article</a>&nbsp;
 <a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=26158955" aria-label="PubMed reference 91">PubMed</a>&nbsp;
 <a data-track=click data-track-action="google scholar reference" data-track-label=link rel="nofollow noopener" aria-label="Google Scholar reference 91" href="http://scholar.google.com/scholar_lookup?&amp;title=Explaining%20away%3A%20a%20model%20of%20affective%20adaptation&amp;journal=Perspect%20Psychol%20Sci&amp;doi=10.1111%2Fj.1745-6924.2008.00085.x&amp;volume=3&amp;pages=370-386&amp;publication_year=2008&amp;author=Wilson%2CTD&amp;author=Gilbert%2CDT">
 Google Scholar</a>&nbsp;
 </p><li class="c-article-references__item js-c-reading-companion-references-item"><p class=c-article-references__text id=ref-CR96>World Medical Association (2013) World Medical Association Declaration of Helsinki: ethical principles form medical research involving human subjects. JAMA 310:2191–2194. <a href=https://doi.org/10.1001/jama.2013.281053>https://doi.org/10.1001/jama.2013.281053</a><p class="c-article-references__links u-hide-print"><a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="cas reference" href=https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BC2cXmtVOmsA%3D%3D aria-label="CAS reference 92">CAS</a>&nbsp;
 <a data-track=click data-track-label=link rel="nofollow noopener" data-track-action="article reference" href=https://doi.org/10.1001%2Fjama.2013.281053 aria-label="Article reference 92">Article</a>&nbsp;
 <a data-track=click data-track-action="google scholar reference" data-track-label=link rel="nofollow noopener" aria-label="Google Scholar reference 92" href="http://scholar.google.com/scholar_lookup?&amp;title=World%20Medical%20Association%20Declaration%20of%20Helsinki%3A%20ethical%20principles%20form%20medical%20research%20involving%20human%20subjects&amp;journal=JAMA&amp;doi=10.1001%2Fjama.2013.281053&amp;volume=310&amp;pages=2191-2194&amp;publication_year=2013">
 Google Scholar</a>&nbsp;
 </p></ul><p class="c-article-references__download u-hide-print"><a data-track=click data-track-action="download citation references" data-track-label=link href="https://citation-needed.springer.com/v2/references/10.1057/s41599-020-0499-z?format=refman&amp;flavour=references">Download references<svg width=16 height=16 focusable=false role=img aria-hidden=true class=u-icon><use xmlns:xlink=http://www.w3.org/1999/xlink xlink:href=#global-icon-download></use></svg></a></p></div></div></div></section></div><section data-title=Acknowledgements><div class=c-article-section id=Ack1-section><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id=Ack1>Acknowledgements</h2><div class=c-article-section__content id=Ack1-content><p>This research was conducted through a project funded by German Research Foundation (Deutsche Forschungsgemeinschaft, DFG)—Project number 254142454/GRK 2070. The funder had no role in study design, data collection, and analysis, decision to publish, or preparation of the manuscript. We would like to thank Silke Paulmann for generously providing us with her stimuli sets, Julia Fischer for helpful suggestions on the manuscript, Carlotta Dove, Isabel Noethen and Christina Broering for help with data acquisition and all individuals who participated in the research presented here.</p></div></div></section><section aria-labelledby=author-information data-title="Author information"><div class=c-article-section id=author-information-section><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id=author-information>Author information</h2><div class=c-article-section__content id=author-information-content><h3 class=c-article__sub-heading id=affiliations>Authors and Affiliations</h3><ol class=c-article-author-affiliation__list><li id=Aff1><p class=c-article-author-affiliation__address>Department of Affective Neuroscience and Psychophysiology, Institute for Psychology, University of Goettingen, 37073, Goettingen, Germany<p class=c-article-author-affiliation__authors-list>Adi Lausen</p><li id=Aff2><p class=c-article-author-affiliation__address>Department of Mathematical Sciences, University of Essex, Colchester, CO4 3SQ, UK<p class=c-article-author-affiliation__authors-list>Adi Lausen</p><li id=Aff3><p class=c-article-author-affiliation__address>Cognitive Ethology Laboratory, German Primate Center, University of Goettingen, Kellnerweg 4, 37077, Goettingen, Germany<p class=c-article-author-affiliation__authors-list>Kurt Hammerschmidt</p><li id=Aff4><p class=c-article-author-affiliation__address>Leibniz ScienceCampus “Primate Cognition”, 37077, Goettingen, Germany<p class=c-article-author-affiliation__authors-list>Kurt Hammerschmidt</p></ol><div class="u-js-hide u-hide-print sf-hidden" data-test=author-info></div><h3 class=c-article__sub-heading id=corresponding-author>Corresponding author</h3><p id=corresponding-author-list>Correspondence to
 <a id=corresp-c1 href=mailto:adi.lausen@psych.uni-goettingen.de>Adi Lausen</a>.</p></div></div></section><section data-title="Ethics declarations"><div class=c-article-section id=ethics-section><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id=ethics>Ethics declarations</h2><div class=c-article-section__content id=ethics-content>
 
 <h3 class=c-article__sub-heading id=FPar1>Competing interests</h3>
 <p>The authors declare no competing interests.</p>
 
 </div></div></section><section data-title="Additional information"><div class=c-article-section id=additional-information-section><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id=additional-information>Additional information</h2><div class=c-article-section__content id=additional-information-content><p><b>Publisher’s note</b> Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></div></div></section><section data-title="Supplementary information"><div class=c-article-section id=Sec25-section><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id=Sec25>Supplementary information</h2><div class=c-article-section__content id=Sec25-content><div data-test=supplementary-info><div id=figshareContainer class=c-article-figshare-container data-test=figshare-container></div>
 
 <div class=c-article-supplementary__item data-test=supp-item id=MOESM1><h3 class="c-article-supplementary__title u-h3"><a class=print-link data-track=click data-track-action="view supplementary info" data-track-label=link data-test=supp-info-link href=https://static-content.springer.com/esm/art%3A10.1057%2Fs41599-020-0499-z/MediaObjects/41599_2020_499_MOESM1_ESM.pdf data-supp-info-image>Supplementary Materials</a></h3></div></div></div></div></section><section data-title="Rights and permissions"><div class=c-article-section id=rightslink-section><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id=rightslink>Rights and permissions</h2><div class=c-article-section__content id=rightslink-content>
 <p><b>Open Access</b> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this license, visit <a href=http://creativecommons.org/licenses/by/4.0/ rel=license>http://creativecommons.org/licenses/by/4.0/</a>.</p>
 <p class=c-article-rights><a data-track=click data-track-action="view rights and permissions" data-track-label=link href="https://s100.copyright.com/AppDispatchServlet?title=Emotion%20recognition%20and%20confidence%20ratings%20predicted%20by%20vocal%20stimulus%20type%20and%20prosodic%20parameters&amp;author=Adi%20Lausen%20et%20al&amp;contentID=10.1057%2Fs41599-020-0499-z&amp;copyright=The%20Author%28s%29&amp;publication=2662-9992&amp;publicationDate=2020-06-17&amp;publisherName=SpringerNature&amp;orderBeanReset=true&amp;oa=CC%20BY">Reprints and Permissions</a></p></div></div></section><section aria-labelledby=article-info data-title="About this article"><div class=c-article-section id=article-info-section><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id=article-info>About this article</h2><div class=c-article-section__content id=article-info-content><div class=c-bibliographic-information><div class="u-hide-print c-bibliographic-information__column c-bibliographic-information__column--border"><a data-crossmark=10.1057/s41599-020-0499-z target=_blank rel=noopener href="https://crossmark.crossref.org/dialog/?doi=10.1057/s41599-020-0499-z" data-track=click data-track-action="Click Crossmark" data-track-label=link data-test=crossmark><img width=57 height=81 alt="Verify currency and authenticity via CrossMark" src=data:image/svg+xml;base64,<svg height="81" width="57" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd"><path d="m17.35 35.45 21.3-14.2v-17.03h-21.3" fill="#989898"/><path d="m38.65 35.45-21.3-14.2v-17.03h21.3" fill="#747474"/><path d="m28 .5c-12.98 0-23.5 10.52-23.5 23.5s10.52 23.5 23.5 23.5 23.5-10.52 23.5-23.5c0-6.23-2.48-12.21-6.88-16.62-4.41-4.4-10.39-6.88-16.62-6.88zm0 41.25c-9.8 0-17.75-7.95-17.75-17.75s7.95-17.75 17.75-17.75 17.75 7.95 17.75 17.75c0 4.71-1.87 9.22-5.2 12.55s-7.84 5.2-12.55 5.2z" fill="#535353"/><path d="m41 36c-5.81 6.23-15.23 7.45-22.43 2.9-7.21-4.55-10.16-13.57-7.03-21.5l-4.92-3.11c-4.95 10.7-1.19 23.42 8.78 29.71 9.97 6.3 23.07 4.22 30.6-4.86z" fill="#9c9c9c"/><path d="m.2 58.45c0-.75.11-1.42.33-2.01s.52-1.09.91-1.5c.38-.41.83-.73 1.34-.94.51-.22 1.06-.32 1.65-.32.56 0 1.06.11 1.51.35.44.23.81.5 1.1.81l-.91 1.01c-.24-.24-.49-.42-.75-.56-.27-.13-.58-.2-.93-.2-.39 0-.73.08-1.05.23-.31.16-.58.37-.81.66-.23.28-.41.63-.53 1.04-.13.41-.19.88-.19 1.39 0 1.04.23 1.86.68 2.46.45.59 1.06.88 1.84.88.41 0 .77-.07 1.07-.23s.59-.39.85-.68l.91 1c-.38.43-.8.76-1.28.99-.47.22-1 .34-1.58.34-.59 0-1.13-.1-1.64-.31-.5-.2-.94-.51-1.31-.91-.38-.4-.67-.9-.88-1.48-.22-.59-.33-1.26-.33-2.02zm8.4-5.33h1.61v2.54l-.05 1.33c.29-.27.61-.51.96-.72s.76-.31 1.24-.31c.73 0 1.27.23 1.61.71.33.47.5 1.14.5 2.02v4.31h-1.61v-4.1c0-.57-.08-.97-.25-1.21-.17-.23-.45-.35-.83-.35-.3 0-.56.08-.79.22-.23.15-.49.36-.78.64v4.8h-1.61zm7.37 6.45c0-.56.09-1.06.26-1.51.18-.45.42-.83.71-1.14.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.36c.07.62.29 1.1.65 1.44.36.33.82.5 1.38.5.29 0 .57-.04.83-.13s.51-.21.76-.37l.55 1.01c-.33.21-.69.39-1.09.53-.41.14-.83.21-1.26.21-.48 0-.92-.08-1.34-.25-.41-.16-.76-.4-1.07-.7-.31-.31-.55-.69-.72-1.13-.18-.44-.26-.95-.26-1.52zm4.6-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.07.45-.31.29-.5.73-.58 1.3zm2.5.62c0-.57.09-1.08.28-1.53.18-.44.43-.82.75-1.13s.69-.54 1.1-.71c.42-.16.85-.24 1.31-.24.45 0 .84.08 1.17.23s.61.34.85.57l-.77 1.02c-.19-.16-.38-.28-.56-.37-.19-.09-.39-.14-.61-.14-.56 0-1.01.21-1.35.63-.35.41-.52.97-.52 1.67 0 .69.17 1.24.51 1.66.34.41.78.62 1.32.62.28 0 .54-.06.78-.17.24-.12.45-.26.64-.42l.67 1.03c-.33.29-.69.51-1.08.65-.39.15-.78.23-1.18.23-.46 0-.9-.08-1.31-.24-.4-.16-.75-.39-1.05-.7s-.53-.69-.7-1.13c-.17-.45-.25-.96-.25-1.53zm6.91-6.45h1.58v6.17h.05l2.54-3.16h1.77l-2.35 2.8 2.59 4.07h-1.75l-1.77-2.98-1.08 1.23v1.75h-1.58zm13.69 1.27c-.25-.11-.5-.17-.75-.17-.58 0-.87.39-.87 1.16v.75h1.34v1.27h-1.34v5.6h-1.61v-5.6h-.92v-1.2l.92-.07v-.72c0-.35.04-.68.13-.98.08-.31.21-.57.4-.79s.42-.39.71-.51c.28-.12.63-.18 1.04-.18.24 0 .48.02.69.07.22.05.41.1.57.17zm.48 5.18c0-.57.09-1.08.27-1.53.17-.44.41-.82.72-1.13.3-.31.65-.54 1.04-.71.39-.16.8-.24 1.23-.24s.84.08 1.24.24c.4.17.74.4 1.04.71s.54.69.72 1.13c.19.45.28.96.28 1.53s-.09 1.08-.28 1.53c-.18.44-.42.82-.72 1.13s-.64.54-1.04.7-.81.24-1.24.24-.84-.08-1.23-.24-.74-.39-1.04-.7c-.31-.31-.55-.69-.72-1.13-.18-.45-.27-.96-.27-1.53zm1.65 0c0 .69.14 1.24.43 1.66.28.41.68.62 1.18.62.51 0 .9-.21 1.19-.62.29-.42.44-.97.44-1.66 0-.7-.15-1.26-.44-1.67-.29-.42-.68-.63-1.19-.63-.5 0-.9.21-1.18.63-.29.41-.43.97-.43 1.67zm6.48-3.44h1.33l.12 1.21h.05c.24-.44.54-.79.88-1.02.35-.24.7-.36 1.07-.36.32 0 .59.05.78.14l-.28 1.4-.33-.09c-.11-.01-.23-.02-.38-.02-.27 0-.56.1-.86.31s-.55.58-.77 1.1v4.2h-1.61zm-47.87 15h1.61v4.1c0 .57.08.97.25 1.2.17.24.44.35.81.35.3 0 .57-.07.8-.22.22-.15.47-.39.73-.73v-4.7h1.61v6.87h-1.32l-.12-1.01h-.04c-.3.36-.63.64-.98.86-.35.21-.76.32-1.24.32-.73 0-1.27-.24-1.61-.71-.33-.47-.5-1.14-.5-2.02zm9.46 7.43v2.16h-1.61v-9.59h1.33l.12.72h.05c.29-.24.61-.45.97-.63.35-.17.72-.26 1.1-.26.43 0 .81.08 1.15.24.33.17.61.4.84.71.24.31.41.68.53 1.11.13.42.19.91.19 1.44 0 .59-.09 1.11-.25 1.57-.16.47-.38.85-.65 1.16-.27.32-.58.56-.94.73-.35.16-.72.25-1.1.25-.3 0-.6-.07-.9-.2s-.59-.31-.87-.56zm0-2.3c.26.22.5.37.73.45.24.09.46.13.66.13.46 0 .84-.2 1.15-.6.31-.39.46-.98.46-1.77 0-.69-.12-1.22-.35-1.61-.23-.38-.61-.57-1.13-.57-.49 0-.99.26-1.52.77zm5.87-1.69c0-.56.08-1.06.25-1.51.16-.45.37-.83.65-1.14.27-.3.58-.54.93-.71s.71-.25 1.08-.25c.39 0 .73.07 1 .2.27.14.54.32.81.55l-.06-1.1v-2.49h1.61v9.88h-1.33l-.11-.74h-.06c-.25.25-.54.46-.88.64-.33.18-.69.27-1.06.27-.87 0-1.56-.32-2.07-.95s-.76-1.51-.76-2.65zm1.67-.01c0 .74.13 1.31.4 1.7.26.38.65.58 1.15.58.51 0 .99-.26 1.44-.77v-3.21c-.24-.21-.48-.36-.7-.45-.23-.08-.46-.12-.7-.12-.45 0-.82.19-1.13.59-.31.39-.46.95-.46 1.68zm6.35 1.59c0-.73.32-1.3.97-1.71.64-.4 1.67-.68 3.08-.84 0-.17-.02-.34-.07-.51-.05-.16-.12-.3-.22-.43s-.22-.22-.38-.3c-.15-.06-.34-.1-.58-.1-.34 0-.68.07-1 .2s-.63.29-.93.47l-.59-1.08c.39-.24.81-.45 1.28-.63.47-.17.99-.26 1.54-.26.86 0 1.51.25 1.93.76s.63 1.25.63 2.21v4.07h-1.32l-.12-.76h-.05c-.3.27-.63.48-.98.66s-.73.27-1.14.27c-.61 0-1.1-.19-1.48-.56-.38-.36-.57-.85-.57-1.46zm1.57-.12c0 .3.09.53.27.67.19.14.42.21.71.21.28 0 .54-.07.77-.2s.48-.31.73-.56v-1.54c-.47.06-.86.13-1.18.23-.31.09-.57.19-.76.31s-.33.25-.41.4c-.09.15-.13.31-.13.48zm6.29-3.63h-.98v-1.2l1.06-.07.2-1.88h1.34v1.88h1.75v1.27h-1.75v3.28c0 .8.32 1.2.97 1.2.12 0 .24-.01.37-.04.12-.03.24-.07.34-.11l.28 1.19c-.19.06-.4.12-.64.17-.23.05-.49.08-.76.08-.4 0-.74-.06-1.02-.18-.27-.13-.49-.3-.67-.52-.17-.21-.3-.48-.37-.78-.08-.3-.12-.64-.12-1.01zm4.36 2.17c0-.56.09-1.06.27-1.51s.41-.83.71-1.14c.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.37c.08.62.29 1.1.65 1.44.36.33.82.5 1.38.5.3 0 .58-.04.84-.13.25-.09.51-.21.76-.37l.54 1.01c-.32.21-.69.39-1.09.53s-.82.21-1.26.21c-.47 0-.92-.08-1.33-.25-.41-.16-.77-.4-1.08-.7-.3-.31-.54-.69-.72-1.13-.17-.44-.26-.95-.26-1.52zm4.61-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.08.45-.31.29-.5.73-.57 1.3zm3.01 2.23c.31.24.61.43.92.57.3.13.63.2.98.2.38 0 .65-.08.83-.23s.27-.35.27-.6c0-.14-.05-.26-.13-.37-.08-.1-.2-.2-.34-.28-.14-.09-.29-.16-.47-.23l-.53-.22c-.23-.09-.46-.18-.69-.3-.23-.11-.44-.24-.62-.4s-.33-.35-.45-.55c-.12-.21-.18-.46-.18-.75 0-.61.23-1.1.68-1.49.44-.38 1.06-.57 1.83-.57.48 0 .91.08 1.29.25s.71.36.99.57l-.74.98c-.24-.17-.49-.32-.73-.42-.25-.11-.51-.16-.78-.16-.35 0-.6.07-.76.21-.17.15-.25.33-.25.54 0 .14.04.26.12.36s.18.18.31.26c.14.07.29.14.46.21l.54.19c.23.09.47.18.7.29s.44.24.64.4c.19.16.34.35.46.58.11.23.17.5.17.82 0 .3-.06.58-.17.83-.12.26-.29.48-.51.68-.23.19-.51.34-.84.45-.34.11-.72.17-1.15.17-.48 0-.95-.09-1.41-.27-.46-.19-.86-.41-1.2-.68z" fill="#535353"/></g></svg>></a></div><div class=c-bibliographic-information__column><h3 class=c-article__sub-heading id=citeas>Cite this article</h3><p class=c-bibliographic-information__citation>Lausen, A., Hammerschmidt, K. Emotion recognition and confidence ratings predicted by vocal stimulus type and prosodic parameters.
 <i>Humanit Soc Sci Commun</i> <b>7</b>, 2 (2020). https://doi.org/10.1057/s41599-020-0499-z<p class="c-bibliographic-information__download-citation u-hide-print"><a data-test=citation-link data-track=click data-track-action="download article citation" data-track-label=link data-track-external href="https://citation-needed.springer.com/v2/references/10.1057/s41599-020-0499-z?format=refman&amp;flavour=citation">Download citation<svg width=16 height=16 focusable=false role=img aria-hidden=true class=u-icon><use xmlns:xlink=http://www.w3.org/1999/xlink xlink:href=#global-icon-download></use></svg></a><ul class=c-bibliographic-information__list data-test=publication-history><li class=c-bibliographic-information__list-item><p>Received<span class="u-hide sf-hidden">: </span><span class=c-bibliographic-information__value><time datetime=2019-10-10>10 October 2019</time></span></p><li class=c-bibliographic-information__list-item><p>Accepted<span class="u-hide sf-hidden">: </span><span class=c-bibliographic-information__value><time datetime=2020-05-19>19 May 2020</time></span></p><li class=c-bibliographic-information__list-item><p>Published<span class="u-hide sf-hidden">: </span><span class=c-bibliographic-information__value><time datetime=2020-06-17>17 June 2020</time></span></p><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide sf-hidden">: </span><span class=c-bibliographic-information__value>https://doi.org/10.1057/s41599-020-0499-z</span></p></ul><div data-component=share-box><div class="c-article-share-box u-display-inline"><h3 class=c-article__sub-heading>Share this article</h3><p class=c-article-share-box__description>Anyone you share the following link with will be able to read this content:</p><button class="js-get-share-url c-article-share-box__button" id=get-share-url data-track=click data-track-label=button data-track-external data-track-action="get shareable link">Get shareable link</button><div class="js-no-share-url-container u-display-none sf-hidden" hidden></div><div class="js-share-url-container u-display-none sf-hidden" hidden></div><p class="js-c-article-share-box__additional-info c-article-share-box__additional-info">
 Provided by the Springer Nature SharedIt content-sharing initiative
 </p></div></div><div data-component=article-info-list>
 <h3 class=c-article__sub-heading>Subjects</h3>
 <ul class=c-article-subject-list>
 <li class=c-article-subject-list__subject><a href=https://www.nature.com/subjects/language-and-linguistics data-track=click data-track-action="view subject" data-track-label=link>Language and linguistics</a><li class=c-article-subject-list__subject><a href=https://www.nature.com/subjects/psychology data-track=click data-track-action="view subject" data-track-label=link>Psychology</a></li>
 </ul>
 </div></div></div></div></div></section>
 
 <section>
 <div class="c-article-section js-article-section" id=further-reading-section>
 <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id=further-reading>Further reading</h2>
 <div class="c-article-section__content js-collapsible-section" id=further-reading-content>
 <ul class=c-article-further-reading__list id=further-reading-list>
 
 <li class="c-article-further-reading__item js-ref-item">
 <h3 class=c-article-further-reading__title>
 <a class=print-link data-track=click data-track-action="view further reading article" data-track-label="link:Superior Communication of Positive Emotions Through Nonverbal Vocalisations Compared to Speech Prosody" href=https://doi.org/10.1007/s10919-021-00375-1>
 Superior Communication of Positive Emotions Through Nonverbal Vocalisations Compared to Speech Prosody
 </a>
 </h3>
 
 <ul data-test=author-list class="c-author-list c-author-list--compact c-author-list--truncated u-mb-4 u-mt-auto">
 
 <li>Roza G. Kamiloğlu</li>
 
 <li>George Boateng</li>
 
 <li>Disa A. Sauter</li>
 
 </ul>
 
 <p class=c-article-further-reading__journal-title><i>Journal of Nonverbal Behavior</i> (2021)</p>
 </li>
 
 </ul>
 </div>
 </div>
 </section>
 
 
 <span data-recommended=jobs></span>
</div>
</article>
</main>
<aside class="c-article-extras u-hide-print" aria-label="Article navigation" data-component-reading-companion data-container-type=reading-companion data-track-component="reading companion">
 <div class=js-context-bar-sticky-point-desktop>
 
 
 
 
 
 <div class="c-pdf-download u-clear-both js-pdf-download">
 <a href=https://www.nature.com/articles/s41599-020-0499-z.pdf class="u-button u-button--full-width u-button--primary u-justify-content-space-between c-pdf-download__link" data-article-pdf=true data-readcube-pdf-url=true data-test=download-pdf data-draft-ignore=true data-track=click data-track-action="download pdf" data-track-label=link data-track-external>
 <span class=c-pdf-download__text>Download PDF</span>
 <svg aria-hidden=true focusable=false width=16 height=16 class=u-icon><use xlink:href=#global-icon-download></use></svg>
 </a>
 </div>
 
 
 
 </div>
 
 
 
 
 <div class=c-reading-companion>
 <div class="c-reading-companion__sticky c-reading-companion__sticky--stuck" data-component=reading-companion-sticky data-test=reading-companion-sticky style=top:120px;width:389.375px>
 <ul class=c-reading-companion__tabs role=tablist><li role=presentation><button data-tab-target=sections role=tab id=tab-sections aria-controls=tabpanel-sections aria-selected=false class=c-reading-companion__tab data-track=click data-track-action="sections tab" data-track-label=tab tabindex=-1>Sections</button><li role=presentation><button data-tab-target=figures role=tab id=tab-figures aria-controls=tabpanel-figures aria-selected=false tabindex=-1 class=c-reading-companion__tab data-track=click data-track-action="figures tab" data-track-label=tab>Figures</button><li role=presentation><button data-tab-target=references role=tab id=tab-references aria-controls=tabpanel-references aria-selected=true class="c-reading-companion__tab c-reading-companion__tab--active" data-track=click data-track-action="references tab" data-track-label=tab>References</button></ul><div class="c-reading-companion__panel c-reading-companion__sections sf-hidden" id=tabpanel-sections aria-labelledby=tab-sections role=tabpanel>
 
 </div>
 <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width sf-hidden" id=tabpanel-figures aria-labelledby=tab-figures role=tabpanel><div class="c-reading-companion__scroll-pane sf-hidden"><ul class="c-reading-companion__figures-list sf-hidden"><li class="c-reading-companion__figure-item sf-hidden"><figure class=sf-hidden><picture class=sf-hidden><source type=image/webp data-srcset="https://media.springernature.com/lw685/springer-static/image/art%3A10.1057%2Fs41599-020-0499-z/MediaObjects/41599_2020_499_Fig1_HTML.png?as=webp"></picture></figure><li class="c-reading-companion__figure-item sf-hidden"><figure class=sf-hidden><picture class=sf-hidden><source type=image/webp data-srcset="https://media.springernature.com/lw685/springer-static/image/art%3A10.1057%2Fs41599-020-0499-z/MediaObjects/41599_2020_499_Fig2_HTML.png?as=webp"></picture></figure><li class="c-reading-companion__figure-item sf-hidden"><figure class=sf-hidden><picture class=sf-hidden><source type=image/webp data-srcset="https://media.springernature.com/lw685/springer-static/image/art%3A10.1057%2Fs41599-020-0499-z/MediaObjects/41599_2020_499_Fig3_HTML.png?as=webp"></picture></figure></ul></div></div>
 <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width c-reading-companion__panel--active" id=tabpanel-references aria-labelledby=tab-references role=tabpanel><div class=c-reading-companion__scroll-pane style=max-height:45342.6px><ol class=c-reading-companion__references-list><li class=c-reading-companion__reference-item><p class="c-reading-companion__reference-citation u-font-family-serif" id=rc-ref-CR1>Abele A (1985) Thinking about thinking: causal, evaluative and finalistic cognitions about social situations. Eur J Soc Psychol 15:315–332. <a href=https://doi.org/10.1002/ejsp.2420150306>https://doi.org/10.1002/ejsp.2420150306</a><p class=c-reading-companion__reference-links><a href=https://doi.org/10.1002%2Fejsp.2420150306 data-track=click data-track-action="article reference" data-track-label=link>Article</a>&nbsp;<a href="http://scholar.google.com/scholar_lookup?&amp;title=Thinking%20about%20thinking%3A%20causal%2C%20evaluative%20and%20finalistic%20cognitions%20about%20social%20situations&amp;journal=Eur%20J%20Soc%20Psychol&amp;doi=10.1002%2Fejsp.2420150306&amp;volume=15&amp;pages=315-332&amp;publication_year=1985&amp;author=Abele%2CA" data-track=click data-track-action="google scholar reference" data-track-label=link>
 Google Scholar</a>&nbsp;</p><li class=c-reading-companion__reference-item><p class="c-reading-companion__reference-citation u-font-family-serif" id=rc-ref-CR2>Anikin A, Lima CF (2018) Perceptual and acoustic differences between authentic and acted nonverbal emotional vocalizations. Q J Exp Psychol 71:622–641. <a href=https://doi.org/10.1080/17470218.2016.1270976>https://doi.org/10.1080/17470218.2016.1270976</a><p class=c-reading-companion__reference-links><a href=https://doi.org/10.1080%2F17470218.2016.1270976 data-track=click data-track-action="article reference" data-track-label=link>Article</a>&nbsp;<a href="http://scholar.google.com/scholar_lookup?&amp;title=Perceptual%20and%20acoustic%20differences%20between%20authentic%20and%20acted%20nonverbal%20emotional%20vocalizations&amp;journal=Q%20J%20Exp%20Psychol&amp;doi=10.1080%2F17470218.2016.1270976&amp;volume=71&amp;pages=622-641&amp;publication_year=2018&amp;author=Anikin%2CA&amp;author=Lima%2CCF" data-track=click data-track-action="google scholar reference" data-track-label=link>
 Google Scholar</a>&nbsp;</p><li class=c-reading-companion__reference-item><p class="c-reading-companion__reference-citation u-font-family-serif" id=rc-ref-CR3>Bąk HK (2016) The state of emotional prosody research—a meta-analysis. In: Bąk HK (ed) Emotional prosody processing for non-native English speakers, 1st edn. Springer International Publishing, pp. 79–112</p><li class=c-reading-companion__reference-item><p class="c-reading-companion__reference-citation u-font-family-serif" id=rc-ref-CR4>Banse R, Scherer KR (1996) Acoustic profiles in vocal emotion expression. J Pers Soc Psychol 70:614–636. <a href=https://doi.org/10.1037/0022-3514.70.3.614>https://doi.org/10.1037/0022-3514.70.3.614</a><p class=c-reading-companion__reference-links><a href=https://www.nature.com/articles/cas-redirect/1:STN:280:DyaK2s%2FgsVCgtg%3D%3D data-track=click data-track-action="cas reference" data-track-label=link>CAS</a>&nbsp;<a href=https://doi.org/10.1037%2F0022-3514.70.3.614 data-track=click data-track-action="article reference" data-track-label=link>Article</a>&nbsp;<a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=8851745" data-track=click data-track-action="pubmed reference" data-track-label=link>PubMed</a>&nbsp;<a href="http://scholar.google.com/scholar_lookup?&amp;title=Acoustic%20profiles%20in%20vocal%20emotion%20expression&amp;journal=J%20Pers%20Soc%20Psychol&amp;doi=10.1037%2F0022-3514.70.3.614&amp;volume=70&amp;pages=614-636&amp;publication_year=1996&amp;author=Banse%2CR&amp;author=Scherer%2CKR" data-track=click data-track-action="google scholar reference" data-track-label=link>
 Google Scholar</a>&nbsp;</p><li class=c-reading-companion__reference-item><p class="c-reading-companion__reference-citation u-font-family-serif" id=rc-ref-CR5>Baumeister RF, Bratslavsky E, Finkenauer C et al. (2001) Bad is stronger than good. Rev Gen Psychol 5:323–370. <a href=https://doi.org/10.1037//1089-2680.5.4.323>https://doi.org/10.1037//1089-2680.5.4.323</a><p class=c-reading-companion__reference-links><a href=https://doi.org/10.1037%2F%2F1089-2680.5.4.323 data-track=click data-track-action="article reference" data-track-label=link>Article</a>&nbsp;<a href="http://scholar.google.com/scholar_lookup?&amp;title=Bad%20is%20stronger%20than%20good&amp;journal=Rev%20Gen%20Psychol&amp;doi=10.1037%2F%2F1089-2680.5.4.323&amp;volume=5&amp;pages=323-370&amp;publication_year=2001&amp;author=Baumeister%2CRF&amp;author=Bratslavsky%2CE&amp;author=Finkenauer%2CC" data-track=click data-track-action="google scholar reference" data-track-label=link>
 Google Scholar</a>&nbsp;</p><li class=c-reading-companion__reference-item><p class="c-reading-companion__reference-citation u-font-family-serif" id=rc-ref-CR6>Baenziger T, Scherer KR (2005) The role of intonation in emotional expressions. Speech Commun 46:252–267. <a href=https://doi.org/10.1016/j.specom.2005.02.016>https://doi.org/10.1016/j.specom.2005.02.016</a><p class=c-reading-companion__reference-links><a href=https://doi.org/10.1016%2Fj.specom.2005.02.016 data-track=click data-track-action="article reference" data-track-label=link>Article</a>&nbsp;<a href="http://scholar.google.com/scholar_lookup?&amp;title=The%20role%20of%20intonation%20in%20emotional%20expressions&amp;journal=Speech%20Commun&amp;doi=10.1016%2Fj.specom.2005.02.016&amp;volume=46&amp;pages=252-267&amp;publication_year=2005&amp;author=Baenziger%2CT&amp;author=Scherer%2CKR" data-track=click data-track-action="google scholar reference" data-track-label=link>
 Google Scholar</a>&nbsp;</p><li class=c-reading-companion__reference-item><p class="c-reading-companion__reference-citation u-font-family-serif" id=rc-ref-CR7>Bègue I, Vaessen M, Hofmeister J et al. (2019) Confidence of emotion expression recognition recruits brain regions outside the face perception network. Soc Cogn Affect Neurosci 4:81–95. <a href=https://doi.org/10.1093/scan/nsy102>https://doi.org/10.1093/scan/nsy102</a><p class=c-reading-companion__reference-links><a href=https://doi.org/10.1093%2Fscan%2Fnsy102 data-track=click data-track-action="article reference" data-track-label=link>Article</a>&nbsp;<a href="http://scholar.google.com/scholar_lookup?&amp;title=Confidence%20of%20emotion%20expression%20recognition%20recruits%20brain%20regions%20outside%20the%20face%20perception%20network&amp;journal=Soc%20Cogn%20Affect%20Neurosci&amp;doi=10.1093%2Fscan%2Fnsy102&amp;volume=4&amp;pages=81-95&amp;publication_year=2019&amp;author=B%C3%A8gue%2CI&amp;author=Vaessen%2CM&amp;author=Hofmeister%2CJ" data-track=click data-track-action="google scholar reference" data-track-label=link>
 Google Scholar</a>&nbsp;</p><li class=c-reading-companion__reference-item><p class="c-reading-companion__reference-citation u-font-family-serif" id=rc-ref-CR8>Belin P, Fillion-Bilodeau S, Gosselin F (2008) The Montreal affective voices: a validated set of nonverbal affect bursts for research on auditory affective processing. Behav Res Methods 40:531–539. <a href=https://doi.org/10.3758/BRM.40.2.531>https://doi.org/10.3758/BRM.40.2.531</a><p class=c-reading-companion__reference-links><a href=https://doi.org/10.3758%2FBRM.40.2.531 data-track=click data-track-action="article reference" data-track-label=link>Article</a>&nbsp;<a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=18522064" data-track=click data-track-action="pubmed reference" data-track-label=link>PubMed</a>&nbsp;<a href="http://scholar.google.com/scholar_lookup?&amp;title=The%20Montreal%20affective%20voices%3A%20a%20validated%20set%20of%20nonverbal%20affect%20bursts%20for%20research%20on%20auditory%20affective%20processing&amp;journal=Behav%20Res%20Methods&amp;doi=10.3758%2FBRM.40.2.531&amp;volume=40&amp;pages=531-539&amp;publication_year=2008&amp;author=Belin%2CP&amp;author=Fillion-Bilodeau%2CS&amp;author=Gosselin%2CF" data-track=click data-track-action="google scholar reference" data-track-label=link>
 Google Scholar</a>&nbsp;</p><li class=c-reading-companion__reference-item><p class="c-reading-companion__reference-citation u-font-family-serif" id=rc-ref-CR9>Ben-David BM, Multani N, Shakuf V et al. (2016) Prosody and semantics are separate but not separable channels in the perception of emotional speech: test for rating of emotions in speech. J Speech Lang Hear Res 59:1–18. <a href=https://doi.org/10.1044/2015_jslhr-h-14-0323>https://doi.org/10.1044/2015_jslhr-h-14-0323</a><p class=c-reading-companion__reference-links><a href=https://doi.org/10.1044%2F2015_jslhr-h-14-0323 data-track=click data-track-action="article reference" data-track-label=link>Article</a>&nbsp;<a href="http://scholar.google.com/scholar_lookup?&amp;title=Prosody%20and%20semantics%20are%20separate%20but%20not%20separable%20channels%20in%20the%20perception%20of%20emotional%20speech%3A%20test%20for%20rating%20of%20emotions%20in%20speech&amp;journal=J%20Speech%20Lang%20Hear%20Res&amp;doi=10.1044%2F2015_jslhr-h-14-0323&amp;volume=59&amp;pages=1-18&amp;publication_year=2016&amp;author=Ben-David%2CBM&amp;author=Multani%2CN&amp;author=Shakuf%2CV" data-track=click data-track-action="google scholar reference" data-track-label=link>
 Google Scholar</a>&nbsp;</p><li class=c-reading-companion__reference-item><p class="c-reading-companion__reference-citation u-font-family-serif" id=rc-ref-CR10>Bostanov V, Kotchoubey B (2004) Recognition of affective prosody: continuous wavelet measures of event-related brain potentials to emotional exclamations. Psychophysiology 41:259–268. <a href=https://doi.org/10.1111/j.1469-8986.2003.00142.x>https://doi.org/10.1111/j.1469-8986.2003.00142.x</a><p class=c-reading-companion__reference-links><a href=https://doi.org/10.1111%2Fj.1469-8986.2003.00142.x data-track=click data-track-action="article reference" data-track-label=link>Article</a>&nbsp;<a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=15032991" data-track=click data-track-action="pubmed reference" data-track-label=link>PubMed</a>&nbsp;<a href="http://scholar.google.com/scholar_lookup?&amp;title=Recognition%20of%20affective%20prosody%3A%20continuous%20wavelet%20measures%20of%20event-related%20brain%20potentials%20to%20emotional%20exclamations&amp;journal=Psychophysiology&amp;doi=10.1111%2Fj.1469-8986.2003.00142.x&amp;volume=41&amp;pages=259-268&amp;publication_year=2004&amp;author=Bostanov%2CV&amp;author=Kotchoubey%2CB" data-track=click data-track-action="google scholar reference" data-track-label=link>
 Google Scholar</a>&nbsp;</p><li class=c-reading-companion__reference-item><p class="c-reading-companion__reference-citation u-font-family-serif" id=rc-ref-CR11>Breiman L (2001) Random forests. Mach Learn 45:5–32. <a href=https://doi.org/10.1023/A:1010933404324>https://doi.org/10.1023/A:1010933404324</a><p class=c-reading-companion__reference-links><a href=https://doi.org/10.1023%2FA%3A1010933404324 data-track=click data-track-action="article reference" data-track-label=link>Article</a>&nbsp;<a href=http://www.emis.de/MATH-item?1007.68152 data-track=click data-track-action="math reference" data-track-label=link>MATH</a>&nbsp;<a href="http://scholar.google.com/scholar_lookup?&amp;title=Random%20forests&amp;journal=Mach%20Learn&amp;doi=10.1023%2FA%3A1010933404324&amp;volume=45&amp;pages=5-32&amp;publication_year=2001&amp;author=Breiman%2CL" data-track=click data-track-action="google scholar reference" data-track-label=link>
 Google Scholar</a>&nbsp;</p><li class=c-reading-companion__reference-item><p class="c-reading-companion__reference-citation u-font-family-serif" id=rc-ref-CR13>Burkhardt F, Paeschke A, Rolfes M et al. (2005) A database of German emotional speech. In: European conference on speech and language processing, Lisbon, Portugal, pp. 1517–1520. <a href=https://www.researchgate.net/publication/221491017_A_database_of_German_emotional_speech>https://www.researchgate.net/publication/221491017_A_database_of_German_emotional_speech</a>. Accessed 10 Nov 2015</p><li class=c-reading-companion__reference-item><p class="c-reading-companion__reference-citation u-font-family-serif" id=rc-ref-CR14>Calder AJ, Burton AM, Miller P et al. (2001) A principal component analysis of facial expressions. Vis Res 41:1179–1208. <a href=https://doi.org/10.1016/S0042-6989(01)00002-5>https://doi.org/10.1016/S0042-6989(01)00002-5</a><p class=c-reading-companion__reference-links><a href=https://www.nature.com/articles/cas-redirect/1:STN:280:DC%2BD3MzisVyltQ%3D%3D data-track=click data-track-action="cas reference" data-track-label=link>CAS</a>&nbsp;<a href=https://doi.org/10.1016%2FS0042-6989%2801%2900002-5 data-track=click data-track-action="article reference" data-track-label=link>Article</a>&nbsp;<a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=11292507" data-track=click data-track-action="pubmed reference" data-track-label=link>PubMed</a>&nbsp;<a href="http://scholar.google.com/scholar_lookup?&amp;title=A%20principal%20component%20analysis%20of%20facial%20expressions&amp;journal=Vis%20Res&amp;doi=10.1016%2FS0042-6989%2801%2900002-5&amp;volume=41&amp;pages=1179-1208&amp;publication_year=2001&amp;author=Calder%2CAJ&amp;author=Burton%2CAM&amp;author=Miller%2CP" data-track=click data-track-action="google scholar reference" data-track-label=link>
 Google Scholar</a>&nbsp;</p><li class=c-reading-companion__reference-item><p class="c-reading-companion__reference-citation u-font-family-serif" id=rc-ref-CR15>Castro SL, Lima CF (2010) Recognizing emotions in spoken language: a validated set of Portuguese sentences and pseudosentences for research on emotional prosody. Behav Res Methods 42:74–81. <a href=https://doi.org/10.3758/BRM.42.1.74>https://doi.org/10.3758/BRM.42.1.74</a><p class=c-reading-companion__reference-links><a href=https://doi.org/10.3758%2FBRM.42.1.74 data-track=click data-track-action="article reference" data-track-label=link>Article</a>&nbsp;<a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=20160287" data-track=click data-track-action="pubmed reference" data-track-label=link>PubMed</a>&nbsp;<a href="http://scholar.google.com/scholar_lookup?&amp;title=Recognizing%20emotions%20in%20spoken%20language%3A%20a%20validated%20set%20of%20Portuguese%20sentences%20and%20pseudosentences%20for%20research%20on%20emotional%20prosody&amp;journal=Behav%20Res%20Methods&amp;doi=10.3758%2FBRM.42.1.74&amp;volume=42&amp;pages=74-81&amp;publication_year=2010&amp;author=Castro%2CSL&amp;author=Lima%2CCF" data-track=click data-track-action="google scholar reference" data-track-label=link>
 Google Scholar</a>&nbsp;</p><li class=c-reading-companion__reference-item><p class="c-reading-companion__reference-citation u-font-family-serif" id=rc-ref-CR16 tabindex=-1>Chronaki G, Wigelsworth M, Pell MD et al. (2018) The development of cross-cultural recognition of vocal emotions during childhood and adolescence. Sci Rep. 8:8659. <a href=https://doi.org/10.1038/s41598-018-26889-1>https://doi.org/10.1038/s41598-018-26889-1</a><a href=#ref-link-section-d166511592e391 class=c-reading-companion__return>Return to ref 2018 in article</a><p class=c-reading-companion__reference-links><a href="http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT&amp;bibcode=2018NatSR...8.8659C" data-track=click data-track-action="ads reference" data-track-label=link>ADS</a>&nbsp;<a href=https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BC1cXhvVCjsb3J data-track=click data-track-action="cas reference" data-track-label=link>CAS</a>&nbsp;<a href=https://doi.org/10.1038%2Fs41598-018-26889-1 data-track=click data-track-action="article reference" data-track-label=link>Article</a>&nbsp;<a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=29904120" data-track=click data-track-action="pubmed reference" data-track-label=link>PubMed</a>&nbsp;<a href=http://www.ncbi.nlm.nih.gov/pmc/articles/PMC6002529 data-track=click data-track-action="pubmed central reference" data-track-label=link>PubMed Central</a>&nbsp;<a href="http://scholar.google.com/scholar_lookup?&amp;title=The%20development%20of%20cross-cultural%20recognition%20of%20vocal%20emotions%20during%20childhood%20and%20adolescence&amp;journal=Sci%20Rep.&amp;doi=10.1038%2Fs41598-018-26889-1&amp;volume=8&amp;publication_year=2018&amp;author=Chronaki%2CG&amp;author=Wigelsworth%2CM&amp;author=Pell%2CMD" data-track=click data-track-action="google scholar reference" data-track-label=link>
 Google Scholar</a>&nbsp;</p><li class=c-reading-companion__reference-item><p class="c-reading-companion__reference-citation u-font-family-serif" id=rc-ref-CR19>Cordaro DT, Keltner D, Tshering S et al. (2016) The voice conveys emotion in ten globalized cultures and one remote village in Bhutan. Emotion 16:117–128. <a href=https://doi.org/10.1037/emo0000100>https://doi.org/10.1037/emo0000100</a><p class=c-reading-companion__reference-links><a href=https://doi.org/10.1037%2Femo0000100 data-track=click data-track-action="article reference" data-track-label=link>Article</a>&nbsp;<a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=26389648" data-track=click data-track-action="pubmed reference" data-track-label=link>PubMed</a>&nbsp;<a href="http://scholar.google.com/scholar_lookup?&amp;title=The%20voice%20conveys%20emotion%20in%20ten%20globalized%20cultures%20and%20one%20remote%20village%20in%20Bhutan&amp;journal=Emotion&amp;doi=10.1037%2Femo0000100&amp;volume=16&amp;pages=117-128&amp;publication_year=2016&amp;author=Cordaro%2CDT&amp;author=Keltner%2CD&amp;author=Tshering%2CS" data-track=click data-track-action="google scholar reference" data-track-label=link>
 Google Scholar</a>&nbsp;</p><li class=c-reading-companion__reference-item><p class="c-reading-companion__reference-citation u-font-family-serif" id=rc-ref-CR20>Cornew L, Carver L, Love T (2009) There’s more to emotion than meets the eye: a processing bias for neutral content in the domain of emotional prosody. Cogn Emot 24:1133–1152. <a href=https://doi.org/10.1080/02699930903247492>https://doi.org/10.1080/02699930903247492</a><p class=c-reading-companion__reference-links><a href=https://doi.org/10.1080%2F02699930903247492 data-track=click data-track-action="article reference" data-track-label=link>Article</a>&nbsp;<a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=21552425" data-track=click data-track-action="pubmed reference" data-track-label=link>PubMed</a>&nbsp;<a href=http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3088090 data-track=click data-track-action="pubmed central reference" data-track-label=link>PubMed Central</a>&nbsp;<a href="http://scholar.google.com/scholar_lookup?&amp;title=There%E2%80%99s%20more%20to%20emotion%20than%20meets%20the%20eye%3A%20a%20processing%20bias%20for%20neutral%20content%20in%20the%20domain%20of%20emotional%20prosody&amp;journal=Cogn%20Emot&amp;doi=10.1080%2F02699930903247492&amp;volume=24&amp;pages=1133-1152&amp;publication_year=2009&amp;author=Cornew%2CL&amp;author=Carver%2CL&amp;author=Love%2CT" data-track=click data-track-action="google scholar reference" data-track-label=link>
 Google Scholar</a>&nbsp;</p><li class=c-reading-companion__reference-item><p class="c-reading-companion__reference-citation u-font-family-serif" id=rc-ref-CR21>Cowen AS, Laukka P, Elfenbein HA et al. (2019a) The primacy of categories in the recognition of 12 emotions in speech prosody across two cultures. Nat Hum Behav 3:369–382. <a href=https://doi.org/10.1038/s41562-019-0533-6>https://doi.org/10.1038/s41562-019-0533-6</a><p class=c-reading-companion__reference-links><a href=https://doi.org/10.1038%2Fs41562-019-0533-6 data-track=click data-track-action="article reference" data-track-label=link>Article</a>&nbsp;<a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=30971794" data-track=click data-track-action="pubmed reference" data-track-label=link>PubMed</a>&nbsp;<a href=http://www.ncbi.nlm.nih.gov/pmc/articles/PMC6687085 data-track=click data-track-action="pubmed central reference" data-track-label=link>PubMed Central</a>&nbsp;<a href="http://scholar.google.com/scholar_lookup?&amp;title=The%20primacy%20of%20categories%20in%20the%20recognition%20of%2012%20emotions%20in%20speech%20prosody%20across%20two%20cultures&amp;journal=Nat%20Hum%20Behav&amp;doi=10.1038%2Fs41562-019-0533-6&amp;volume=3&amp;pages=369-382&amp;publication_year=2019&amp;author=Cowen%2CAS&amp;author=Laukka%2CP&amp;author=Elfenbein%2CHA" data-track=click data-track-action="google scholar reference" data-track-label=link>
 Google Scholar</a>&nbsp;</p><li class=c-reading-companion__reference-item><p class="c-reading-companion__reference-citation u-font-family-serif" id=rc-ref-CR22>Cowen AS, Elfenbein HA, Laukka P et al. (2019b) Mapping 24 emotions conveyed by brief human vocalization. Am Psychol 74:698–712. <a href=https://doi.org/10.1037/amp0000399>https://doi.org/10.1037/amp0000399</a><p class=c-reading-companion__reference-links><a href=https://doi.org/10.1037%2Famp0000399 data-track=click data-track-action="article reference" data-track-label=link>Article</a>&nbsp;<a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=30570267" data-track=click data-track-action="pubmed reference" data-track-label=link>PubMed</a>&nbsp;<a href="http://scholar.google.com/scholar_lookup?&amp;title=Mapping%2024%20emotions%20conveyed%20by%20brief%20human%20vocalization&amp;journal=Am%20Psychol&amp;doi=10.1037%2Famp0000399&amp;volume=74&amp;pages=698-712&amp;publication_year=2019&amp;author=Cowen%2CAS&amp;author=Elfenbein%2CHA&amp;author=Laukka%2CP" data-track=click data-track-action="google scholar reference" data-track-label=link>
 Google Scholar</a>&nbsp;</p><li class=c-reading-companion__reference-item><p class="c-reading-companion__reference-citation u-font-family-serif" id=rc-ref-CR23>Cox DR, Snell EJ (1989) Analysis of binary data, 2nd edn. Chapman &amp; Hall</p><li class=c-reading-companion__reference-item><p class="c-reading-companion__reference-citation u-font-family-serif" id=rc-ref-CR25>Dixon SJ, Brereton RG (2009) Comparison of performance of five common classifiers represented as boundary methods: Euclidean distance to centroids, linear discriminant analysis, quadratic discriminant analysis, learning vector quantization and support vector machines, as dependent on data structure. Chemom Intell Lab Syst 95:1–17. <a href=https://doi.org/10.1016/j.chemolab.2008.07.010>https://doi.org/10.1016/j.chemolab.2008.07.010</a><p class=c-reading-companion__reference-links><a href=https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BD1cXhsFagur7L data-track=click data-track-action="cas reference" data-track-label=link>CAS</a>&nbsp;<a href=https://doi.org/10.1016%2Fj.chemolab.2008.07.010 data-track=click data-track-action="article reference" data-track-label=link>Article</a>&nbsp;<a href="http://scholar.google.com/scholar_lookup?&amp;title=Comparison%20of%20performance%20of%20five%20common%20classifiers%20represented%20as%20boundary%20methods%3A%20Euclidean%20distance%20to%20centroids%2C%20linear%20discriminant%20analysis%2C%20quadratic%20discriminant%20analysis%2C%20learning%20vector%20quantization%20and%20support%20vector%20machines%2C%20as%20dependent%20on%20data%20structure&amp;journal=Chemom%20Intell%20Lab%20Syst&amp;doi=10.1016%2Fj.chemolab.2008.07.010&amp;volume=95&amp;pages=1-17&amp;publication_year=2009&amp;author=Dixon%2CSJ&amp;author=Brereton%2CRG" data-track=click data-track-action="google scholar reference" data-track-label=link>
 Google Scholar</a>&nbsp;</p><li class=c-reading-companion__reference-item><p class="c-reading-companion__reference-citation u-font-family-serif" id=rc-ref-CR26>Dunlosky J, Metcalfe J (2009) Confidence judgements. In: Dunlosky J, Metcalfe J (eds) Metacognition, 1st edn. Sage Publications, Washington, pp. 118–139<p class=c-reading-companion__reference-links><a href="http://scholar.google.com/scholar_lookup?&amp;title=Confidence%20judgements&amp;pages=118-139&amp;publication_year=2009&amp;author=Dunlosky%2CJ&amp;author=Metcalfe%2CJ" data-track=click data-track-action="google scholar reference" data-track-label=link>
 Google Scholar</a>&nbsp;</p><li class=c-reading-companion__reference-item><p class="c-reading-companion__reference-citation u-font-family-serif" id=rc-ref-CR27>Eyben F, Scherer KR, Schuller BW et al. (2016) The Geneva minimalistic acoustic parameter set (GeMAPS) for voice research and affective computing. IEEE Trans Affect Comput 7:190–202. <a href=https://doi.org/10.1109/TAFFC.2015.2457417>https://doi.org/10.1109/TAFFC.2015.2457417</a><p class=c-reading-companion__reference-links><a href=https://doi.org/10.1109%2FTAFFC.2015.2457417 data-track=click data-track-action="article reference" data-track-label=link>Article</a>&nbsp;<a href="http://scholar.google.com/scholar_lookup?&amp;title=The%20Geneva%20minimalistic%20acoustic%20parameter%20set%20%28GeMAPS%29%20for%20voice%20research%20and%20affective%20computing&amp;journal=IEEE%20Trans%20Affect%20Comput&amp;doi=10.1109%2FTAFFC.2015.2457417&amp;volume=7&amp;pages=190-202&amp;publication_year=2016&amp;author=Eyben%2CF&amp;author=Scherer%2CKR&amp;author=Schuller%2CBW" data-track=click data-track-action="google scholar reference" data-track-label=link>
 Google Scholar</a>&nbsp;</p><li class=c-reading-companion__reference-item><p class="c-reading-companion__reference-citation u-font-family-serif" id=rc-ref-CR28>Goddard C (2014) Interjections and emotion (with special reference to “surprise” and “disgust”). Emot Rev 6:53–63. <a href=https://doi.org/10.1177/1754073913491843>https://doi.org/10.1177/1754073913491843</a><p class=c-reading-companion__reference-links><a href=https://doi.org/10.1177%2F1754073913491843 data-track=click data-track-action="article reference" data-track-label=link>Article</a>&nbsp;<a href="http://scholar.google.com/scholar_lookup?&amp;title=Interjections%20and%20emotion%20%28with%20special%20reference%20to%20%E2%80%9Csurprise%E2%80%9D%20and%20%E2%80%9Cdisgust%E2%80%9D%29&amp;journal=Emot%20Rev&amp;doi=10.1177%2F1754073913491843&amp;volume=6&amp;pages=53-63&amp;publication_year=2014&amp;author=Goddard%2CC" data-track=click data-track-action="google scholar reference" data-track-label=link>
 Google Scholar</a>&nbsp;</p><li class=c-reading-companion__reference-item><p class="c-reading-companion__reference-citation u-font-family-serif" id=rc-ref-CR29>Goudbeek M, Scherer KR (2010) Beyond arousal: valence and potency/control cues in the vocal expression of emotion. J Acoust Soc Am 128:1322–1336. <a href=https://doi.org/10.1121/1.3466853>https://doi.org/10.1121/1.3466853</a><p class=c-reading-companion__reference-links><a href="http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT&amp;bibcode=2010ASAJ..128.1322G" data-track=click data-track-action="ads reference" data-track-label=link>ADS</a>&nbsp;<a href=https://doi.org/10.1121%2F1.3466853 data-track=click data-track-action="article reference" data-track-label=link>Article</a>&nbsp;<a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=20815467" data-track=click data-track-action="pubmed reference" data-track-label=link>PubMed</a>&nbsp;<a href="http://scholar.google.com/scholar_lookup?&amp;title=Beyond%20arousal%3A%20valence%20and%20potency%2Fcontrol%20cues%20in%20the%20vocal%20expression%20of%20emotion&amp;journal=J%20Acoust%20Soc%20Am&amp;doi=10.1121%2F1.3466853&amp;volume=128&amp;pages=1322-1336&amp;publication_year=2010&amp;author=Goudbeek%2CM&amp;author=Scherer%2CKR" data-track=click data-track-action="google scholar reference" data-track-label=link>
 Google Scholar</a>&nbsp;</p><li class=c-reading-companion__reference-item><p class="c-reading-companion__reference-citation u-font-family-serif" id=rc-ref-CR30>Hamilton DL, Huffman LJ (1971) Generality of impression-formation processes for evaluative and nonevaluative judgments. J Pers Soc Psychol 20:200–207. <a href=https://doi.org/10.1037/h0031698>https://doi.org/10.1037/h0031698</a><p class=c-reading-companion__reference-links><a href=https://doi.org/10.1037%2Fh0031698 data-track=click data-track-action="article reference" data-track-label=link>Article</a>&nbsp;<a href="http://scholar.google.com/scholar_lookup?&amp;title=Generality%20of%20impression-formation%20processes%20for%20evaluative%20and%20nonevaluative%20judgments&amp;journal=J%20Pers%20Soc%20Psychol&amp;doi=10.1037%2Fh0031698&amp;volume=20&amp;pages=200-207&amp;publication_year=1971&amp;author=Hamilton%2CDL&amp;author=Huffman%2CLJ" data-track=click data-track-action="google scholar reference" data-track-label=link>
 Google Scholar</a>&nbsp;</p><li class=c-reading-companion__reference-item><p class="c-reading-companion__reference-citation u-font-family-serif" id=rc-ref-CR31>Hamilton DL, Zanna MP (1972) Differential weighting of favorable and unfavorable attributes in impressions of personality. J Exp Res Pers 6:204–212<p class=c-reading-companion__reference-links><a href="http://scholar.google.com/scholar_lookup?&amp;title=Differential%20weighting%20of%20favorable%20and%20unfavorable%20attributes%20in%20impressions%20of%20personality&amp;journal=J%20Exp%20Res%20Pers&amp;volume=6&amp;pages=204-212&amp;publication_year=1972&amp;author=Hamilton%2CDL&amp;author=Zanna%2CMP" data-track=click data-track-action="google scholar reference" data-track-label=link>
 Google Scholar</a>&nbsp;</p><li class=c-reading-companion__reference-item><p class="c-reading-companion__reference-citation u-font-family-serif" id=rc-ref-CR32>Hammerschmidt K, Juergens U (2007) Acoustical correlates of affective prosody. J Voice 21:531–540. <a href=https://doi.org/10.1016/j.jvoice.2006.03.002>https://doi.org/10.1016/j.jvoice.2006.03.002</a><p class=c-reading-companion__reference-links><a href=https://doi.org/10.1016%2Fj.jvoice.2006.03.002 data-track=click data-track-action="article reference" data-track-label=link>Article</a>&nbsp;<a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=16647247" data-track=click data-track-action="pubmed reference" data-track-label=link>PubMed</a>&nbsp;<a href="http://scholar.google.com/scholar_lookup?&amp;title=Acoustical%20correlates%20of%20affective%20prosody&amp;journal=J%20Voice&amp;doi=10.1016%2Fj.jvoice.2006.03.002&amp;volume=21&amp;pages=531-540&amp;publication_year=2007&amp;author=Hammerschmidt%2CK&amp;author=Juergens%2CU" data-track=click data-track-action="google scholar reference" data-track-label=link>
 Google Scholar</a>&nbsp;</p><li class=c-reading-companion__reference-item><p class="c-reading-companion__reference-citation u-font-family-serif" id=rc-ref-CR33>Hawk ST, van Kleef GA, Fischer AH et al. (2009) “Worth a thousand words”: absolute and relative decoding of nonlinguistic affect vocalizations. Emotion 9:293–305. <a href=https://doi.org/10.1037/a0015178>https://doi.org/10.1037/a0015178</a><p class=c-reading-companion__reference-links><a href=https://doi.org/10.1037%2Fa0015178 data-track=click data-track-action="article reference" data-track-label=link>Article</a>&nbsp;<a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=19485607" data-track=click data-track-action="pubmed reference" data-track-label=link>PubMed</a>&nbsp;<a href="http://scholar.google.com/scholar_lookup?&amp;title=%E2%80%9CWorth%20a%20thousand%20words%E2%80%9D%3A%20absolute%20and%20relative%20decoding%20of%20nonlinguistic%20affect%20vocalizations&amp;journal=Emotion&amp;doi=10.1037%2Fa0015178&amp;volume=9&amp;pages=293-305&amp;publication_year=2009&amp;author=Hawk%2CST&amp;author=Kleef%2CGA&amp;author=Fischer%2CAH" data-track=click data-track-action="google scholar reference" data-track-label=link>
 Google Scholar</a>&nbsp;</p><li class=c-reading-companion__reference-item><p class="c-reading-companion__reference-citation u-font-family-serif" id=rc-ref-CR34>Hothorn T, Bretz F, Westfall P (2008) Simultaneous inference in general parametric models. Biom J 50:346–363. <a href=https://doi.org/10.1002/bimj.200810425>https://doi.org/10.1002/bimj.200810425</a><p class=c-reading-companion__reference-links><a href="http://www.ams.org/mathscinet-getitem?mr=2521547" data-track=click data-track-action="mathscinet reference" data-track-label=link>MathSciNet</a>&nbsp;<a href=https://doi.org/10.1002%2Fbimj.200810425 data-track=click data-track-action="article reference" data-track-label=link>Article</a>&nbsp;<a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=18481363" data-track=click data-track-action="pubmed reference" data-track-label=link>PubMed</a>&nbsp;<a href=http://www.emis.de/MATH-item?1442.62415 data-track=click data-track-action="math reference" data-track-label=link>MATH</a>&nbsp;<a href="http://scholar.google.com/scholar_lookup?&amp;title=Simultaneous%20inference%20in%20general%20parametric%20models&amp;journal=Biom%20J&amp;doi=10.1002%2Fbimj.200810425&amp;volume=50&amp;pages=346-363&amp;publication_year=2008&amp;author=Hothorn%2CT&amp;author=Bretz%2CF&amp;author=Westfall%2CP" data-track=click data-track-action="google scholar reference" data-track-label=link>
 Google Scholar</a>&nbsp;</p><li class=c-reading-companion__reference-item><p class="c-reading-companion__reference-citation u-font-family-serif" id=rc-ref-CR35>Ito TA, Larsen JT, Smith NK et al. (1998) Negative information weighs more heavily on the brain: the negativity bias in evaluative categorizations. J Pers Soc Psychol 75:887–900. <a href=https://doi.org/10.1037/0022-3514.75.4.887>https://doi.org/10.1037/0022-3514.75.4.887</a><p class=c-reading-companion__reference-links><a href=https://www.nature.com/articles/cas-redirect/1:STN:280:DyaK1M%2Fkt12ltQ%3D%3D data-track=click data-track-action="cas reference" data-track-label=link>CAS</a>&nbsp;<a href=https://doi.org/10.1037%2F0022-3514.75.4.887 data-track=click data-track-action="article reference" data-track-label=link>Article</a>&nbsp;<a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=9825526" data-track=click data-track-action="pubmed reference" data-track-label=link>PubMed</a>&nbsp;<a href="http://scholar.google.com/scholar_lookup?&amp;title=Negative%20information%20weighs%20more%20heavily%20on%20the%20brain%3A%20the%20negativity%20bias%20in%20evaluative%20categorizations&amp;journal=J%20Pers%20Soc%20Psychol&amp;doi=10.1037%2F0022-3514.75.4.887&amp;volume=75&amp;pages=887-900&amp;publication_year=1998&amp;author=Ito%2CTA&amp;author=Larsen%2CJT&amp;author=Smith%2CNK" data-track=click data-track-action="google scholar reference" data-track-label=link>
 Google Scholar</a>&nbsp;</p><li class=c-reading-companion__reference-item><p class="c-reading-companion__reference-citation u-font-family-serif" id=rc-ref-CR36>James G, Witten D, Hastie T et al. (2013) An introduction to statistical learning with applications in R. In: Cassella G, Fienberg S, Olkin I (eds) Springer texts in statistics. Springer, New York, pp. 303–332<p class=c-reading-companion__reference-links><a href="http://scholar.google.com/scholar_lookup?&amp;title=An%20introduction%20to%20statistical%20learning%20with%20applications%20in%20R&amp;pages=303-332&amp;publication_year=2013&amp;author=James%2CG&amp;author=Witten%2CD&amp;author=Hastie%2CT" data-track=click data-track-action="google scholar reference" data-track-label=link>
 Google Scholar</a>&nbsp;</p><li class=c-reading-companion__reference-item><p class="c-reading-companion__reference-citation u-font-family-serif" id=rc-ref-CR37>Jiang X, Pell MD (2017) The sound of confidence and doubt. Speech Commun 88:106–126. <a href=https://doi.org/10.1016/j.specom.2017.01.011>https://doi.org/10.1016/j.specom.2017.01.011</a><p class=c-reading-companion__reference-links><a href=https://doi.org/10.1016%2Fj.specom.2017.01.011 data-track=click data-track-action="article reference" data-track-label=link>Article</a>&nbsp;<a href="http://scholar.google.com/scholar_lookup?&amp;title=The%20sound%20of%20confidence%20and%20doubt&amp;journal=Speech%20Commun&amp;doi=10.1016%2Fj.specom.2017.01.011&amp;volume=88&amp;pages=106-126&amp;publication_year=2017&amp;author=Jiang%2CX&amp;author=Pell%2CMD" data-track=click data-track-action="google scholar reference" data-track-label=link>
 Google Scholar</a>&nbsp;</p><li class=c-reading-companion__reference-item><p class="c-reading-companion__reference-citation u-font-family-serif" id=rc-ref-CR38>Jiang X, Pell DM (2014) Encoding and decoding confidence information in speech. In: Proceedings of the 7th international conference in speech prosody (social and linguistic speech prosody). pp. 573–576. <a href=http://fastnet.netsoc.ie/sp7/sp7book.pdf>http://fastnet.netsoc.ie/sp7/sp7book.pdf</a>. Accessed 30 Nov 2018</p><li class=c-reading-companion__reference-item><p class="c-reading-companion__reference-citation u-font-family-serif" id=rc-ref-CR39>Johnstone T, Scherer KR (2000) Vocal communication of emotion. In: Lewis M, Haviland J (eds) The handbook of emotion, 2nd edn. Guildford, New York, pp. 220–235<p class=c-reading-companion__reference-links><a href="http://scholar.google.com/scholar_lookup?&amp;title=Vocal%20communication%20of%20emotion&amp;pages=220-235&amp;publication_year=2000&amp;author=Johnstone%2CT&amp;author=Scherer%2CKR" data-track=click data-track-action="google scholar reference" data-track-label=link>
 Google Scholar</a>&nbsp;</p><li class=c-reading-companion__reference-item><p class="c-reading-companion__reference-citation u-font-family-serif" id=rc-ref-CR40>Juslin PN, Scherer KR (2005) Vocal expression of affect. In: Harrigan JA, Rosenthal R, Scherer KR (eds) The new handbook of methods in nonverbal behavior research, 1st edn. Oxford University Press, Oxford, pp. 65–135<p class=c-reading-companion__reference-links><a href="http://scholar.google.com/scholar_lookup?&amp;title=Vocal%20expression%20of%20affect&amp;publication_year=2005&amp;author=Juslin%2CPN&amp;author=Scherer%2CKR" data-track=click data-track-action="google scholar reference" data-track-label=link>
 Google Scholar</a>&nbsp;</p><li class=c-reading-companion__reference-item><p class="c-reading-companion__reference-citation u-font-family-serif" id=rc-ref-CR41>Juslin PN, Laukka P (2003) Communication of emotions in vocal expression and music performance: different channels, same code? Psychol Bull 129:770–814. <a href=https://doi.org/10.1037/0033-2909.129.5.770>https://doi.org/10.1037/0033-2909.129.5.770</a><p class=c-reading-companion__reference-links><a href=https://doi.org/10.1037%2F0033-2909.129.5.770 data-track=click data-track-action="article reference" data-track-label=link>Article</a>&nbsp;<a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=12956543" data-track=click data-track-action="pubmed reference" data-track-label=link>PubMed</a>&nbsp;<a href="http://scholar.google.com/scholar_lookup?&amp;title=Communication%20of%20emotions%20in%20vocal%20expression%20and%20music%20performance%3A%20different%20channels%2C%20same%20code%3F&amp;journal=Psychol%20Bull&amp;doi=10.1037%2F0033-2909.129.5.770&amp;volume=129&amp;pages=770-814&amp;publication_year=2003&amp;author=Juslin%2CPN&amp;author=Laukka%2CP" data-track=click data-track-action="google scholar reference" data-track-label=link>
 Google Scholar</a>&nbsp;</p><li class=c-reading-companion__reference-item><p class="c-reading-companion__reference-citation u-font-family-serif" id=rc-ref-CR42>Juslin PN, Laukka P (2001) Impact of intended emotion intensity on cue utilization and decoding accuracy in vocal expression of emotion. Emotion 1:381–412. <a href=https://doi.org/10.1037//1528-3542.1.4.381>https://doi.org/10.1037//1528-3542.1.4.381</a><p class=c-reading-companion__reference-links><a href=https://www.nature.com/articles/cas-redirect/1:STN:280:DC%2BD3szmslWgsw%3D%3D data-track=click data-track-action="cas reference" data-track-label=link>CAS</a>&nbsp;<a href=https://doi.org/10.1037%2F%2F1528-3542.1.4.381 data-track=click data-track-action="article reference" data-track-label=link>Article</a>&nbsp;<a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=12901399" data-track=click data-track-action="pubmed reference" data-track-label=link>PubMed</a>&nbsp;<a href="http://scholar.google.com/scholar_lookup?&amp;title=Impact%20of%20intended%20emotion%20intensity%20on%20cue%20utilization%20and%20decoding%20accuracy%20in%20vocal%20expression%20of%20emotion&amp;journal=Emotion&amp;doi=10.1037%2F%2F1528-3542.1.4.381&amp;volume=1&amp;pages=381-412&amp;publication_year=2001&amp;author=Juslin%2CPN&amp;author=Laukka%2CP" data-track=click data-track-action="google scholar reference" data-track-label=link>
 Google Scholar</a>&nbsp;</p><li class=c-reading-companion__reference-item><p class="c-reading-companion__reference-citation u-font-family-serif" id=rc-ref-CR43>Juergens R, Fischer J, Schacht A (2018) Hot speech and exploding bombs: autonomic arousal during emotion classification of prosodic utterances and affective sounds. Front Psychol 9:228. <a href=https://doi.org/10.3389/fpsyg.2018.00228>https://doi.org/10.3389/fpsyg.2018.00228</a><p class=c-reading-companion__reference-links><a href=https://doi.org/10.3389%2Ffpsyg.2018.00228 data-track=click data-track-action="article reference" data-track-label=link>Article</a>&nbsp;<a href="http://scholar.google.com/scholar_lookup?&amp;title=Hot%20speech%20and%20exploding%20bombs%3A%20autonomic%20arousal%20during%20emotion%20classification%20of%20prosodic%20utterances%20and%20affective%20sounds&amp;journal=Front%20Psychol&amp;doi=10.3389%2Ffpsyg.2018.00228&amp;volume=9&amp;publication_year=2018&amp;author=Juergens%2CR&amp;author=Fischer%2CJ&amp;author=Schacht%2CA" data-track=click data-track-action="google scholar reference" data-track-label=link>
 Google Scholar</a>&nbsp;</p><li class=c-reading-companion__reference-item><p class="c-reading-companion__reference-citation u-font-family-serif" id=rc-ref-CR44>Juergens R, Grass A, Drolet M et al. (2015) Effect of acting experience on emotion expression and recognition in voice: non-actors provide better stimuli than expected. J Nonverbal Behav 39:195–214. <a href=https://doi.org/10.1007/s10919-015-0209-5>https://doi.org/10.1007/s10919-015-0209-5</a><p class=c-reading-companion__reference-links><a href=https://doi.org/10.1007%2Fs10919-015-0209-5 data-track=click data-track-action="article reference" data-track-label=link>Article</a>&nbsp;<a href="http://scholar.google.com/scholar_lookup?&amp;title=Effect%20of%20acting%20experience%20on%20emotion%20expression%20and%20recognition%20in%20voice%3A%20non-actors%20provide%20better%20stimuli%20than%20expected&amp;journal=J%20Nonverbal%20Behav&amp;doi=10.1007%2Fs10919-015-0209-5&amp;volume=39&amp;pages=195-214&amp;publication_year=2015&amp;author=Juergens%2CR&amp;author=Grass%2CA&amp;author=Drolet%2CM" data-track=click data-track-action="google scholar reference" data-track-label=link>
 Google Scholar</a>&nbsp;</p><li class=c-reading-companion__reference-item><p class="c-reading-companion__reference-citation u-font-family-serif" id=rc-ref-CR45>Juergens R, Drolet M, Pirow R et al. (2013) Encoding conditions affect recognition of vocally expressed emotions across cultures. Front Psychol 4:111. <a href=https://doi.org/10.3389/fpsyg.2013.00111>https://doi.org/10.3389/fpsyg.2013.00111</a><p class=c-reading-companion__reference-links><a href=https://doi.org/10.3389%2Ffpsyg.2013.00111 data-track=click data-track-action="article reference" data-track-label=link>Article</a>&nbsp;<a href="http://scholar.google.com/scholar_lookup?&amp;title=Encoding%20conditions%20affect%20recognition%20of%20vocally%20expressed%20emotions%20across%20cultures&amp;journal=Front%20Psychol&amp;doi=10.3389%2Ffpsyg.2013.00111&amp;volume=4&amp;publication_year=2013&amp;author=Juergens%2CR&amp;author=Drolet%2CM&amp;author=Pirow%2CR" data-track=click data-track-action="google scholar reference" data-track-label=link>
 Google Scholar</a>&nbsp;</p><li class=c-reading-companion__reference-item><p class="c-reading-companion__reference-citation u-font-family-serif" id=rc-ref-CR46>Juergens R, Hammerschmidt K, Fischer J (2011) Authentic and play-acted vocal emotion expressions reveal acoustic differences. Front Psychol 2:180. <a href=https://doi.org/10.3389/fpsyg.2011.00180>https://doi.org/10.3389/fpsyg.2011.00180</a><p class=c-reading-companion__reference-links><a href=https://doi.org/10.3389%2Ffpsyg.2011.00180 data-track=click data-track-action="article reference" data-track-label=link>Article</a>&nbsp;<a href="http://scholar.google.com/scholar_lookup?&amp;title=Authentic%20and%20play-acted%20vocal%20emotion%20expressions%20reveal%20acoustic%20differences&amp;journal=Front%20Psychol&amp;doi=10.3389%2Ffpsyg.2011.00180&amp;volume=2&amp;publication_year=2011&amp;author=Juergens%2CR&amp;author=Hammerschmidt%2CK&amp;author=Fischer%2CJ" data-track=click data-track-action="google scholar reference" data-track-label=link>
 Google Scholar</a>&nbsp;</p><li class=c-reading-companion__reference-item><p class="c-reading-companion__reference-citation u-font-family-serif" id=rc-ref-CR47>Kelly KJ, Metcalfe J (2011) Metacognition of emotional face recognition. Emotion 11:896–906. <a href=https://doi.org/10.1037/a0023746>https://doi.org/10.1037/a0023746</a><p class=c-reading-companion__reference-links><a href=https://doi.org/10.1037%2Fa0023746 data-track=click data-track-action="article reference" data-track-label=link>Article</a>&nbsp;<a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=21859205" data-track=click data-track-action="pubmed reference" data-track-label=link>PubMed</a>&nbsp;<a href="http://scholar.google.com/scholar_lookup?&amp;title=Metacognition%20of%20emotional%20face%20recognition&amp;journal=Emotion&amp;doi=10.1037%2Fa0023746&amp;volume=11&amp;pages=896-906&amp;publication_year=2011&amp;author=Kelly%2CKJ&amp;author=Metcalfe%2CJ" data-track=click data-track-action="google scholar reference" data-track-label=link>
 Google Scholar</a>&nbsp;</p><li class=c-reading-companion__reference-item><p class="c-reading-companion__reference-citation u-font-family-serif" id=rc-ref-CR48>Kimble C, Seidel S (1991) Vocal signs of confidence. J Nonverbal Behav 15:99–105. <a href=https://doi.org/10.1007/BF00998265>https://doi.org/10.1007/BF00998265</a><p class=c-reading-companion__reference-links><a href=https://doi.org/10.1007%2FBF00998265 data-track=click data-track-action="article reference" data-track-label=link>Article</a>&nbsp;<a href="http://scholar.google.com/scholar_lookup?&amp;title=Vocal%20signs%20of%20confidence&amp;journal=J%20Nonverbal%20Behav&amp;doi=10.1007%2FBF00998265&amp;volume=15&amp;pages=99-105&amp;publication_year=1991&amp;author=Kimble%2CC&amp;author=Seidel%2CS" data-track=click data-track-action="google scholar reference" data-track-label=link>
 Google Scholar</a>&nbsp;</p><li class=c-reading-companion__reference-item><p class="c-reading-companion__reference-citation u-font-family-serif" id=rc-ref-CR49>Kitayama S, Ishii K (2002) Word and voice: spontaneous attention to emotional speech in two cultures. Cogn Emot 16:29–59. <a href=https://doi.org/10.1080/0269993943000121>https://doi.org/10.1080/0269993943000121</a><p class=c-reading-companion__reference-links><a href=https://doi.org/10.1080%2F0269993943000121 data-track=click data-track-action="article reference" data-track-label=link>Article</a>&nbsp;<a href="http://scholar.google.com/scholar_lookup?&amp;title=Word%20and%20voice%3A%20spontaneous%20attention%20to%20emotional%20speech%20in%20two%20cultures&amp;journal=Cogn%20Emot&amp;doi=10.1080%2F0269993943000121&amp;volume=16&amp;pages=29-59&amp;publication_year=2002&amp;author=Kitayama%2CS&amp;author=Ishii%2CK" data-track=click data-track-action="google scholar reference" data-track-label=link>
 Google Scholar</a>&nbsp;</p><li class=c-reading-companion__reference-item><p class="c-reading-companion__reference-citation u-font-family-serif" id=rc-ref-CR50>Kohler KJ (2008) ‘Speech-smile’, ‘speech-laugh’, ‘laughter’ and their sequencing in dialogic interaction. Phonetica 65:1–18. <a href=https://doi.org/10.1159/000130013>https://doi.org/10.1159/000130013</a><p class=c-reading-companion__reference-links><a href=https://doi.org/10.1159%2F000130013 data-track=click data-track-action="article reference" data-track-label=link>Article</a>&nbsp;<a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=18523364" data-track=click data-track-action="pubmed reference" data-track-label=link>PubMed</a>&nbsp;<a href="http://scholar.google.com/scholar_lookup?&amp;title=%E2%80%98Speech-smile%E2%80%99%2C%20%E2%80%98speech-laugh%E2%80%99%2C%20%E2%80%98laughter%E2%80%99%20and%20their%20sequencing%20in%20dialogic%20interaction&amp;journal=Phonetica&amp;doi=10.1159%2F000130013&amp;volume=65&amp;pages=1-18&amp;publication_year=2008&amp;author=Kohler%2CKJ" data-track=click data-track-action="google scholar reference" data-track-label=link>
 Google Scholar</a>&nbsp;</p><li class=c-reading-companion__reference-item><p class="c-reading-companion__reference-citation u-font-family-serif" id=rc-ref-CR51>Koriat A (2008) When confidence in a choice is independent of which choice is made. Psychon Bull Rev 15:997–1001. <a href=https://doi.org/10.3758/PBR.15.5.997>https://doi.org/10.3758/PBR.15.5.997</a><p class=c-reading-companion__reference-links><a href=https://doi.org/10.3758%2FPBR.15.5.997 data-track=click data-track-action="article reference" data-track-label=link>Article</a>&nbsp;<a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=18926995" data-track=click data-track-action="pubmed reference" data-track-label=link>PubMed</a>&nbsp;<a href="http://scholar.google.com/scholar_lookup?&amp;title=When%20confidence%20in%20a%20choice%20is%20independent%20of%20which%20choice%20is%20made&amp;journal=Psychon%20Bull%20Rev&amp;doi=10.3758%2FPBR.15.5.997&amp;volume=15&amp;pages=997-1001&amp;publication_year=2008&amp;author=Koriat%2CA" data-track=click data-track-action="google scholar reference" data-track-label=link>
 Google Scholar</a>&nbsp;</p><li class=c-reading-companion__reference-item><p class="c-reading-companion__reference-citation u-font-family-serif" id=rc-ref-CR52>Kotz SA, Paulmann S (2007) When emotional prosody and semantics dance cheek to cheek: ERP evidence. Brain Res 1151:107–118. <a href=https://doi.org/10.1016/j.brainres.2007.03.015>https://doi.org/10.1016/j.brainres.2007.03.015</a><p class=c-reading-companion__reference-links><a href=https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BD2sXlt1CjsrY%3D data-track=click data-track-action="cas reference" data-track-label=link>CAS</a>&nbsp;<a href=https://doi.org/10.1016%2Fj.brainres.2007.03.015 data-track=click data-track-action="article reference" data-track-label=link>Article</a>&nbsp;<a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=17445783" data-track=click data-track-action="pubmed reference" data-track-label=link>PubMed</a>&nbsp;<a href="http://scholar.google.com/scholar_lookup?&amp;title=When%20emotional%20prosody%20and%20semantics%20dance%20cheek%20to%20cheek%3A%20ERP%20evidence&amp;journal=Brain%20Res&amp;doi=10.1016%2Fj.brainres.2007.03.015&amp;volume=1151&amp;pages=107-118&amp;publication_year=2007&amp;author=Kotz%2CSA&amp;author=Paulmann%2CS" data-track=click data-track-action="google scholar reference" data-track-label=link>
 Google Scholar</a>&nbsp;</p><li class=c-reading-companion__reference-item><p class="c-reading-companion__reference-citation u-font-family-serif" id=rc-ref-CR53>Krumhuber EG, Scherer KR (2011) Affect bursts: dynamic patterns of facial expression. Emotion 11:825–841. <a href=https://doi.org/10.1037/a0023856>https://doi.org/10.1037/a0023856</a><p class=c-reading-companion__reference-links><a href=https://doi.org/10.1037%2Fa0023856 data-track=click data-track-action="article reference" data-track-label=link>Article</a>&nbsp;<a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=21707163" data-track=click data-track-action="pubmed reference" data-track-label=link>PubMed</a>&nbsp;<a href="http://scholar.google.com/scholar_lookup?&amp;title=Affect%20bursts%3A%20dynamic%20patterns%20of%20facial%20expression&amp;journal=Emotion&amp;doi=10.1037%2Fa0023856&amp;volume=11&amp;pages=825-841&amp;publication_year=2011&amp;author=Krumhuber%2CEG&amp;author=Scherer%2CKR" data-track=click data-track-action="google scholar reference" data-track-label=link>
 Google Scholar</a>&nbsp;</p><li class=c-reading-companion__reference-item><p class="c-reading-companion__reference-citation u-font-family-serif" id=rc-ref-CR54>Lausen A, Schacht A (2018) Gender differences in the recognition of vocal emotions. Front Psychol 9:882. <a href=https://doi.org/10.3389/fpsyg.2018.00882>https://doi.org/10.3389/fpsyg.2018.00882</a><p class=c-reading-companion__reference-links><a href=https://doi.org/10.3389%2Ffpsyg.2018.00882 data-track=click data-track-action="article reference" data-track-label=link>Article</a>&nbsp;<a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=29922202" data-track=click data-track-action="pubmed reference" data-track-label=link>PubMed</a>&nbsp;<a href=http://www.ncbi.nlm.nih.gov/pmc/articles/PMC5996252 data-track=click data-track-action="pubmed central reference" data-track-label=link>PubMed Central</a>&nbsp;<a href="http://scholar.google.com/scholar_lookup?&amp;title=Gender%20differences%20in%20the%20recognition%20of%20vocal%20emotions&amp;journal=Front%20Psychol&amp;doi=10.3389%2Ffpsyg.2018.00882&amp;volume=9&amp;publication_year=2018&amp;author=Lausen%2CA&amp;author=Schacht%2CA" data-track=click data-track-action="google scholar reference" data-track-label=link>
 Google Scholar</a>&nbsp;</p><li class=c-reading-companion__reference-item><p class="c-reading-companion__reference-citation u-font-family-serif" id=rc-ref-CR55>Lima CF, Castro SL, Scott SK (2013) When voices get emotional: a corpus of nonverbal vocalizations for research on emotion processing. Behav Res Methods 45:1234–1245. <a href=https://doi.org/10.3758/s13428-013-0324-3>https://doi.org/10.3758/s13428-013-0324-3</a><p class=c-reading-companion__reference-links><a href=https://doi.org/10.3758%2Fs13428-013-0324-3 data-track=click data-track-action="article reference" data-track-label=link>Article</a>&nbsp;<a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=23444120" data-track=click data-track-action="pubmed reference" data-track-label=link>PubMed</a>&nbsp;<a href="http://scholar.google.com/scholar_lookup?&amp;title=When%20voices%20get%20emotional%3A%20a%20corpus%20of%20nonverbal%20vocalizations%20for%20research%20on%20emotion%20processing&amp;journal=Behav%20Res%20Methods&amp;doi=10.3758%2Fs13428-013-0324-3&amp;volume=45&amp;pages=1234-1245&amp;publication_year=2013&amp;author=Lima%2CCF&amp;author=Castro%2CSL&amp;author=Scott%2CSK" data-track=click data-track-action="google scholar reference" data-track-label=link>
 Google Scholar</a>&nbsp;</p><li class=c-reading-companion__reference-item><p class="c-reading-companion__reference-citation u-font-family-serif" id=rc-ref-CR56>Liu T, Pinheiro AP, Deng G et al. (2012) Electrophysiological insights into processing nonverbal emotional vocalizations. Neuroreport 23:108–112. <a href=https://doi.org/10.1097/WNR.0b013e32834ea757>https://doi.org/10.1097/WNR.0b013e32834ea757</a><p class=c-reading-companion__reference-links><a href=https://doi.org/10.1097%2FWNR.0b013e32834ea757 data-track=click data-track-action="article reference" data-track-label=link>Article</a>&nbsp;<a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=22134115" data-track=click data-track-action="pubmed reference" data-track-label=link>PubMed</a>&nbsp;<a href="http://scholar.google.com/scholar_lookup?&amp;title=Electrophysiological%20insights%20into%20processing%20nonverbal%20emotional%20vocalizations&amp;journal=Neuroreport&amp;doi=10.1097%2FWNR.0b013e32834ea757&amp;volume=23&amp;pages=108-112&amp;publication_year=2012&amp;author=Liu%2CT&amp;author=Pinheiro%2CAP&amp;author=Deng%2CG" data-track=click data-track-action="google scholar reference" data-track-label=link>
 Google Scholar</a>&nbsp;</p><li class=c-reading-companion__reference-item><p class="c-reading-companion__reference-citation u-font-family-serif" id=rc-ref-CR57>Metcalfe J, Schwartz BL, Joaquim SG (1993) The cue-familiarity heuristic in metacognition. J Exp Psychol Learn Mem Cogn 19:851–861. <a href=https://doi.org/10.1037//0278-7393.19.4.851>https://doi.org/10.1037//0278-7393.19.4.851</a><p class=c-reading-companion__reference-links><a href=https://www.nature.com/articles/cas-redirect/1:STN:280:DyaK3szks12gtg%3D%3D data-track=click data-track-action="cas reference" data-track-label=link>CAS</a>&nbsp;<a href=https://doi.org/10.1037%2F%2F0278-7393.19.4.851 data-track=click data-track-action="article reference" data-track-label=link>Article</a>&nbsp;<a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=8345327" data-track=click data-track-action="pubmed reference" data-track-label=link>PubMed</a>&nbsp;<a href="http://scholar.google.com/scholar_lookup?&amp;title=The%20cue-familiarity%20heuristic%20in%20metacognition&amp;journal=J%20Exp%20Psychol%20Learn%20Mem%20Cogn&amp;doi=10.1037%2F%2F0278-7393.19.4.851&amp;volume=19&amp;pages=851-861&amp;publication_year=1993&amp;author=Metcalfe%2CJ&amp;author=Schwartz%2CBL&amp;author=Joaquim%2CSG" data-track=click data-track-action="google scholar reference" data-track-label=link>
 Google Scholar</a>&nbsp;</p><li class=c-reading-companion__reference-item><p class="c-reading-companion__reference-citation u-font-family-serif" id=rc-ref-CR58>Mitchell RLC, Elliott R, Barry M et al. (2003) The neural response to emotional prosody, as revealed by functional magnetic resonance imaging. Neuropsychologia 41:1410–1421. <a href=https://doi.org/10.1016/S0028-3932(03)00017-4>https://doi.org/10.1016/S0028-3932(03)00017-4</a><p class=c-reading-companion__reference-links><a href=https://doi.org/10.1016%2FS0028-3932%2803%2900017-4 data-track=click data-track-action="article reference" data-track-label=link>Article</a>&nbsp;<a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=12757912" data-track=click data-track-action="pubmed reference" data-track-label=link>PubMed</a>&nbsp;<a href="http://scholar.google.com/scholar_lookup?&amp;title=The%20neural%20response%20to%20emotional%20prosody%2C%20as%20revealed%20by%20functional%20magnetic%20resonance%20imaging&amp;journal=Neuropsychologia&amp;doi=10.1016%2FS0028-3932%2803%2900017-4&amp;volume=41&amp;pages=1410-1421&amp;publication_year=2003&amp;author=Mitchell%2CRLC&amp;author=Elliott%2CR&amp;author=Barry%2CM" data-track=click data-track-action="google scholar reference" data-track-label=link>
 Google Scholar</a>&nbsp;</p><li class=c-reading-companion__reference-item><p class="c-reading-companion__reference-citation u-font-family-serif" id=rc-ref-CR59>Mozziconacci S (2002) Prosody and emotions. In: Proceedings of speech prosody, pp. 1–9. <a href=https://www.isca-speech.org/archive/sp2002/>https://www.isca-speech.org/archive/sp2002/</a>. Accessed 30 Nov 2018</p><li class=c-reading-companion__reference-item><p class="c-reading-companion__reference-citation u-font-family-serif" id=rc-ref-CR60>Noroozi F, Sapiński T, Kamińska D et al. (2017) Vocal-based emotion recognition using random forests and decision tree. Int J Speech Technol 20:239–246. <a href=https://doi.org/10.1007/s10772-017-9396-2>https://doi.org/10.1007/s10772-017-9396-2</a><p class=c-reading-companion__reference-links><a href=https://doi.org/10.1007%2Fs10772-017-9396-2 data-track=click data-track-action="article reference" data-track-label=link>Article</a>&nbsp;<a href="http://scholar.google.com/scholar_lookup?&amp;title=Vocal-based%20emotion%20recognition%20using%20random%20forests%20and%20decision%20tree&amp;journal=Int%20J%20Speech%20Technol&amp;doi=10.1007%2Fs10772-017-9396-2&amp;volume=20&amp;pages=239-246&amp;publication_year=2017&amp;author=Noroozi%2CF&amp;author=Sapi%C5%84ski%2CT&amp;author=Kami%C5%84ska%2CD" data-track=click data-track-action="google scholar reference" data-track-label=link>
 Google Scholar</a>&nbsp;</p><li class=c-reading-companion__reference-item><p class="c-reading-companion__reference-citation u-font-family-serif" id=rc-ref-CR97>Norsonic Nor140 (2017) Instruction manual. Lierskogen, Norway. <a href=https://www.campbell-associates.co.uk/norsonic-140-sound-level-meter>https://www.campbell-associates.co.uk/norsonic-140-sound-level-meter</a></p><li class=c-reading-companion__reference-item><p class="c-reading-companion__reference-citation u-font-family-serif" id=rc-ref-CR61>Nygaard L, Lunders E (2002) Resolution of lexical ambiguity by emotional tone of voice. Mem Cogn 30:583–593. <a href=https://doi.org/10.3758/BF03194959>https://doi.org/10.3758/BF03194959</a><p class=c-reading-companion__reference-links><a href=https://doi.org/10.3758%2FBF03194959 data-track=click data-track-action="article reference" data-track-label=link>Article</a>&nbsp;<a href="http://scholar.google.com/scholar_lookup?&amp;title=Resolution%20of%20lexical%20ambiguity%20by%20emotional%20tone%20of%20voice&amp;journal=Mem%20Cogn&amp;doi=10.3758%2FBF03194959&amp;volume=30&amp;pages=583-593&amp;publication_year=2002&amp;author=Nygaard%2CL&amp;author=Lunders%2CE" data-track=click data-track-action="google scholar reference" data-track-label=link>
 Google Scholar</a>&nbsp;</p><li class=c-reading-companion__reference-item><p class="c-reading-companion__reference-citation u-font-family-serif" id=rc-ref-CR62>Owren MJ (2008) GSU Praat tools: scripts for modifying and analyzing sounds using Praat acoustics software. Behav Res Methods 40:822–829. <a href=https://doi.org/10.3758/BRM.40.3.822>https://doi.org/10.3758/BRM.40.3.822</a><p class=c-reading-companion__reference-links><a href=https://doi.org/10.3758%2FBRM.40.3.822 data-track=click data-track-action="article reference" data-track-label=link>Article</a>&nbsp;<a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=18697678" data-track=click data-track-action="pubmed reference" data-track-label=link>PubMed</a>&nbsp;<a href="http://scholar.google.com/scholar_lookup?&amp;title=GSU%20Praat%20tools%3A%20scripts%20for%20modifying%20and%20analyzing%20sounds%20using%20Praat%20acoustics%20software&amp;journal=Behav%20Res%20Methods&amp;doi=10.3758%2FBRM.40.3.822&amp;volume=40&amp;pages=822-829&amp;publication_year=2008&amp;author=Owren%2CMJ" data-track=click data-track-action="google scholar reference" data-track-label=link>
 Google Scholar</a>&nbsp;</p><li class=c-reading-companion__reference-item><p class="c-reading-companion__reference-citation u-font-family-serif" id=rc-ref-CR63>Oehman A (1993) Fear and anxiety as emotional phenomenon: clinical phenomenology, evolutionary perspectives, and information-processing mechanisms. In: Lewis M, Haviland JM (eds) Handbook of emotions. Guildford Press, New York, pp. 511–536<p class=c-reading-companion__reference-links><a href="http://scholar.google.com/scholar_lookup?&amp;title=Fear%20and%20anxiety%20as%20emotional%20phenomenon%3A%20clinical%20phenomenology%2C%20evolutionary%20perspectives%2C%20and%20information-processing%20mechanisms&amp;pages=511-536&amp;publication_year=1993&amp;author=Oehman%2CA" data-track=click data-track-action="google scholar reference" data-track-label=link>
 Google Scholar</a>&nbsp;</p><li class=c-reading-companion__reference-item><p class="c-reading-companion__reference-citation u-font-family-serif" id=rc-ref-CR64>Parsons CE, Young KS, Craske MG et al. (2014) Introducing the Oxford Vocal (OxVoc) Sounds database: a validated set of non-acted affective sounds from human infants, adults, and domestic animals. Front Psychol 5:562. <a href=https://doi.org/10.3389/fpsyg.2014.00562>https://doi.org/10.3389/fpsyg.2014.00562</a><p class=c-reading-companion__reference-links><a href=https://doi.org/10.3389%2Ffpsyg.2014.00562 data-track=click data-track-action="article reference" data-track-label=link>Article</a>&nbsp;<a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=25009511" data-track=click data-track-action="pubmed reference" data-track-label=link>PubMed</a>&nbsp;<a href=http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4068198 data-track=click data-track-action="pubmed central reference" data-track-label=link>PubMed Central</a>&nbsp;<a href="http://scholar.google.com/scholar_lookup?&amp;title=Introducing%20the%20Oxford%20Vocal%20%28OxVoc%29%20Sounds%20database%3A%20a%20validated%20set%20of%20non-acted%20affective%20sounds%20from%20human%20infants%2C%20adults%2C%20and%20domestic%20animals&amp;journal=Front%20Psychol&amp;doi=10.3389%2Ffpsyg.2014.00562&amp;volume=5&amp;publication_year=2014&amp;author=Parsons%2CCE&amp;author=Young%2CKS&amp;author=Craske%2CMG" data-track=click data-track-action="google scholar reference" data-track-label=link>
 Google Scholar</a>&nbsp;</p><li class=c-reading-companion__reference-item><p class="c-reading-companion__reference-citation u-font-family-serif" id=rc-ref-CR65>Paulmann S (2016) The neurocognition of prosody. In: Hickok G, Small S (eds) Neurobiology of language. Elsevier, San Diego, pp. 1109–1120<p class=c-reading-companion__reference-links><a href="http://scholar.google.com/scholar_lookup?&amp;title=The%20neurocognition%20of%20prosody&amp;publication_year=2016&amp;author=Paulmann%2CS" data-track=click data-track-action="google scholar reference" data-track-label=link>
 Google Scholar</a>&nbsp;</p><li class=c-reading-companion__reference-item><p class="c-reading-companion__reference-citation u-font-family-serif" id=rc-ref-CR66>Paulmann S, Uskul AK (2014) Cross-cultural emotional prosody recognition: evidence from Chinese and British listeners. Cogn Emot 28:230–244. <a href=https://doi.org/10.1080/02699931.2013.812033>https://doi.org/10.1080/02699931.2013.812033</a><p class=c-reading-companion__reference-links><a href=https://doi.org/10.1080%2F02699931.2013.812033 data-track=click data-track-action="article reference" data-track-label=link>Article</a>&nbsp;<a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=23862740" data-track=click data-track-action="pubmed reference" data-track-label=link>PubMed</a>&nbsp;<a href="http://scholar.google.com/scholar_lookup?&amp;title=Cross-cultural%20emotional%20prosody%20recognition%3A%20evidence%20from%20Chinese%20and%20British%20listeners&amp;journal=Cogn%20Emot&amp;doi=10.1080%2F02699931.2013.812033&amp;volume=28&amp;pages=230-244&amp;publication_year=2014&amp;author=Paulmann%2CS&amp;author=Uskul%2CAK" data-track=click data-track-action="google scholar reference" data-track-label=link>
 Google Scholar</a>&nbsp;</p><li class=c-reading-companion__reference-item><p class="c-reading-companion__reference-citation u-font-family-serif" id=rc-ref-CR67>Paulmann S, Kotz SA (2008) An ERP investigation on the temporal dynamics of emotional prosody and emotional semantics in pseudo- and lexical sentence context. Brain Lang 105:59–69. <a href=https://doi.org/10.1016/j.bandl.2007.11.005>https://doi.org/10.1016/j.bandl.2007.11.005</a><p class=c-reading-companion__reference-links><a href=https://doi.org/10.1016%2Fj.bandl.2007.11.005 data-track=click data-track-action="article reference" data-track-label=link>Article</a>&nbsp;<a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=18177699" data-track=click data-track-action="pubmed reference" data-track-label=link>PubMed</a>&nbsp;<a href="http://scholar.google.com/scholar_lookup?&amp;title=An%20ERP%20investigation%20on%20the%20temporal%20dynamics%20of%20emotional%20prosody%20and%20emotional%20semantics%20in%20pseudo-%20and%20lexical%20sentence%20context&amp;journal=Brain%20Lang&amp;doi=10.1016%2Fj.bandl.2007.11.005&amp;volume=105&amp;pages=59-69&amp;publication_year=2008&amp;author=Paulmann%2CS&amp;author=Kotz%2CSA" data-track=click data-track-action="google scholar reference" data-track-label=link>
 Google Scholar</a>&nbsp;</p><li class=c-reading-companion__reference-item><p class="c-reading-companion__reference-citation u-font-family-serif" id=rc-ref-CR68>Paulmann S, Pell MD, Kotz SA (2008) How aging affects the recognition of emotional speech. Brain Lang 104:262–269. <a href=https://doi.org/10.1016/j.bandl.2007.03.002>https://doi.org/10.1016/j.bandl.2007.03.002</a><p class=c-reading-companion__reference-links><a href=https://doi.org/10.1016%2Fj.bandl.2007.03.002 data-track=click data-track-action="article reference" data-track-label=link>Article</a>&nbsp;<a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=17428529" data-track=click data-track-action="pubmed reference" data-track-label=link>PubMed</a>&nbsp;<a href="http://scholar.google.com/scholar_lookup?&amp;title=How%20aging%20affects%20the%20recognition%20of%20emotional%20speech&amp;journal=Brain%20Lang&amp;doi=10.1016%2Fj.bandl.2007.03.002&amp;volume=104&amp;pages=262-269&amp;publication_year=2008&amp;author=Paulmann%2CS&amp;author=Pell%2CMD&amp;author=Kotz%2CSA" data-track=click data-track-action="google scholar reference" data-track-label=link>
 Google Scholar</a>&nbsp;</p><li class=c-reading-companion__reference-item><p class="c-reading-companion__reference-citation u-font-family-serif" id=rc-ref-CR69>Pell MD, Rothermich K, Liu P et al. (2015) Preferential decoding of emotion from human non-linguistic vocalizations versus speech prosody. Biol Psychol 111:14–25. <a href=https://doi.org/10.1016/j.biopsycho.2015.08.008>https://doi.org/10.1016/j.biopsycho.2015.08.008</a><p class=c-reading-companion__reference-links><a href=https://www.nature.com/articles/cas-redirect/1:STN:280:DC%2BC287mtFKmsA%3D%3D data-track=click data-track-action="cas reference" data-track-label=link>CAS</a>&nbsp;<a href=https://doi.org/10.1016%2Fj.biopsycho.2015.08.008 data-track=click data-track-action="article reference" data-track-label=link>Article</a>&nbsp;<a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=26307467" data-track=click data-track-action="pubmed reference" data-track-label=link>PubMed</a>&nbsp;<a href="http://scholar.google.com/scholar_lookup?&amp;title=Preferential%20decoding%20of%20emotion%20from%20human%20non-linguistic%20vocalizations%20versus%20speech%20prosody&amp;journal=Biol%20Psychol&amp;doi=10.1016%2Fj.biopsycho.2015.08.008&amp;volume=111&amp;pages=14-25&amp;publication_year=2015&amp;author=Pell%2CMD&amp;author=Rothermich%2CK&amp;author=Liu%2CP" data-track=click data-track-action="google scholar reference" data-track-label=link>
 Google Scholar</a>&nbsp;</p><li class=c-reading-companion__reference-item><p class="c-reading-companion__reference-citation u-font-family-serif" id=rc-ref-CR70>Pell MD, Kotz SA (2011) On the time course of vocal emotion recognition. PLoS ONE 6(11):e27256. <a href=https://doi.org/10.1371/journal.pone.0027256>https://doi.org/10.1371/journal.pone.0027256</a><p class=c-reading-companion__reference-links><a href="http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT&amp;bibcode=2011PLoSO...627256P" data-track=click data-track-action="ads reference" data-track-label=link>ADS</a>&nbsp;<a href=https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BC3MXhsFemtr7J data-track=click data-track-action="cas reference" data-track-label=link>CAS</a>&nbsp;<a href=https://doi.org/10.1371%2Fjournal.pone.0027256 data-track=click data-track-action="article reference" data-track-label=link>Article</a>&nbsp;<a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=22087275" data-track=click data-track-action="pubmed reference" data-track-label=link>PubMed</a>&nbsp;<a href=http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3210149 data-track=click data-track-action="pubmed central reference" data-track-label=link>PubMed Central</a>&nbsp;<a href="http://scholar.google.com/scholar_lookup?&amp;title=On%20the%20time%20course%20of%20vocal%20emotion%20recognition&amp;journal=PLoS%20ONE&amp;doi=10.1371%2Fjournal.pone.0027256&amp;volume=6&amp;issue=11&amp;publication_year=2011&amp;author=Pell%2CMD&amp;author=Kotz%2CSA" data-track=click data-track-action="google scholar reference" data-track-label=link>
 Google Scholar</a>&nbsp;</p><li class=c-reading-companion__reference-item><p class="c-reading-companion__reference-citation u-font-family-serif" id=rc-ref-CR71>Pell MD, Jaywant A, Monetta L et al. (2011) Emotional speech processing: disentangling the effects of prosody and semantic cues. Cogn Emot 25:834–853. <a href=https://doi.org/10.1080/02699931.2010.516915>https://doi.org/10.1080/02699931.2010.516915</a><p class=c-reading-companion__reference-links><a href=https://doi.org/10.1080%2F02699931.2010.516915 data-track=click data-track-action="article reference" data-track-label=link>Article</a>&nbsp;<a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=21824024" data-track=click data-track-action="pubmed reference" data-track-label=link>PubMed</a>&nbsp;<a href="http://scholar.google.com/scholar_lookup?&amp;title=Emotional%20speech%20processing%3A%20disentangling%20the%20effects%20of%20prosody%20and%20semantic%20cues&amp;journal=Cogn%20Emot&amp;doi=10.1080%2F02699931.2010.516915&amp;volume=25&amp;pages=834-853&amp;publication_year=2011&amp;author=Pell%2CMD&amp;author=Jaywant%2CA&amp;author=Monetta%2CL" data-track=click data-track-action="google scholar reference" data-track-label=link>
 Google Scholar</a>&nbsp;</p><li class=c-reading-companion__reference-item><p class="c-reading-companion__reference-citation u-font-family-serif" id=rc-ref-CR72>Pell MD, Monetta L, Paulmann S et al. (2009) Recognizing emotions in a foreign language. J Nonverbal Behav 33:107–120. <a href=https://doi.org/10.1007/s10919-008-0065-7>https://doi.org/10.1007/s10919-008-0065-7</a><p class=c-reading-companion__reference-links><a href=https://doi.org/10.1007%2Fs10919-008-0065-7 data-track=click data-track-action="article reference" data-track-label=link>Article</a>&nbsp;<a href="http://scholar.google.com/scholar_lookup?&amp;title=Recognizing%20emotions%20in%20a%20foreign%20language&amp;journal=J%20Nonverbal%20Behav&amp;doi=10.1007%2Fs10919-008-0065-7&amp;volume=33&amp;pages=107-120&amp;publication_year=2009&amp;author=Pell%2CMD&amp;author=Monetta%2CL&amp;author=Paulmann%2CS" data-track=click data-track-action="google scholar reference" data-track-label=link>
 Google Scholar</a>&nbsp;</p><li class=c-reading-companion__reference-item><p class="c-reading-companion__reference-citation u-font-family-serif" id=rc-ref-CR73>Pell MD, Paulmann S, Dara C et al. (2009) Factors in the recognition of vocally expressed emotions: a comparison of four languages. J Phon 37:417–435. <a href=https://doi.org/10.1016/j.wocn.2009.07.005>https://doi.org/10.1016/j.wocn.2009.07.005</a><p class=c-reading-companion__reference-links><a href=https://doi.org/10.1016%2Fj.wocn.2009.07.005 data-track=click data-track-action="article reference" data-track-label=link>Article</a>&nbsp;<a href="http://scholar.google.com/scholar_lookup?&amp;title=Factors%20in%20the%20recognition%20of%20vocally%20expressed%20emotions%3A%20a%20comparison%20of%20four%20languages&amp;journal=J%20Phon&amp;doi=10.1016%2Fj.wocn.2009.07.005&amp;volume=37&amp;pages=417-435&amp;publication_year=2009&amp;author=Pell%2CMD&amp;author=Paulmann%2CS&amp;author=Dara%2CC" data-track=click data-track-action="google scholar reference" data-track-label=link>
 Google Scholar</a>&nbsp;</p><li class=c-reading-companion__reference-item><p class="c-reading-companion__reference-citation u-font-family-serif" id=rc-ref-CR74>Peeters G, Czapinski J (1990) Positive–negative asymmetry in evaluations: the distinction between affective and informational negativity effects. In: Stroebe W, Hewstone M (eds) European review of social psychology, vol. 1. Wiley, Chichester, pp. 33–60<p class=c-reading-companion__reference-links><a href="http://scholar.google.com/scholar_lookup?&amp;title=Positive%E2%80%93negative%20asymmetry%20in%20evaluations%3A%20the%20distinction%20between%20affective%20and%20informational%20negativity%20effects&amp;pages=33-60&amp;publication_year=1990&amp;author=Peeters%2CG&amp;author=Czapinski%2CJ" data-track=click data-track-action="google scholar reference" data-track-label=link>
 Google Scholar</a>&nbsp;</p><li class=c-reading-companion__reference-item><p class="c-reading-companion__reference-citation u-font-family-serif" id=rc-ref-CR75>Pichora-Fuller MK, Dupuis K, Van Lieshout P (2016) Importance of F0 for predicting vocal emotion categorization. J Acoust Soc Am 140:3401–3401. <a href=https://doi.org/10.1121/1.4970917>https://doi.org/10.1121/1.4970917</a><p class=c-reading-companion__reference-links><a href="http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT&amp;bibcode=2016ASAJ..140.3401P" data-track=click data-track-action="ads reference" data-track-label=link>ADS</a>&nbsp;<a href=https://doi.org/10.1121%2F1.4970917 data-track=click data-track-action="article reference" data-track-label=link>Article</a>&nbsp;<a href="http://scholar.google.com/scholar_lookup?&amp;title=Importance%20of%20F0%20for%20predicting%20vocal%20emotion%20categorization&amp;journal=J%20Acoust%20Soc%20Am&amp;doi=10.1121%2F1.4970917&amp;volume=140&amp;pages=3401-3401&amp;publication_year=2016&amp;author=Pichora-Fuller%2CMK&amp;author=Dupuis%2CK&amp;author=Lieshout%2CP" data-track=click data-track-action="google scholar reference" data-track-label=link>
 Google Scholar</a>&nbsp;</p><li class=c-reading-companion__reference-item><p class="c-reading-companion__reference-citation u-font-family-serif" id=rc-ref-CR76>R Core Team (2017) R: a language and environment for statistical computing. R Foundation for Statistical Computing, Vienna<p class=c-reading-companion__reference-links><a href="http://scholar.google.com/scholar_lookup?&amp;title=R%3A%20a%20language%20and%20environment%20for%20statistical%20computing&amp;publication_year=2017" data-track=click data-track-action="google scholar reference" data-track-label=link>
 Google Scholar</a>&nbsp;</p><li class=c-reading-companion__reference-item><p class="c-reading-companion__reference-citation u-font-family-serif" id=rc-ref-CR77>Rigoulot S, Wassiliwizky E, Pell MD (2013) Feeling backwards? How temporal order in speech affects the time course of vocal emotion recognition. Front Psychol 4:367. <a href=https://doi.org/10.3389/fpsyg.2013.00367>https://doi.org/10.3389/fpsyg.2013.00367</a><p class=c-reading-companion__reference-links><a href=https://doi.org/10.3389%2Ffpsyg.2013.00367 data-track=click data-track-action="article reference" data-track-label=link>Article</a>&nbsp;<a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=23805115" data-track=click data-track-action="pubmed reference" data-track-label=link>PubMed</a>&nbsp;<a href=http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3690349 data-track=click data-track-action="pubmed central reference" data-track-label=link>PubMed Central</a>&nbsp;<a href="http://scholar.google.com/scholar_lookup?&amp;title=Feeling%20backwards%3F%20How%20temporal%20order%20in%20speech%20affects%20the%20time%20course%20of%20vocal%20emotion%20recognition&amp;journal=Front%20Psychol&amp;doi=10.3389%2Ffpsyg.2013.00367&amp;volume=4&amp;publication_year=2013&amp;author=Rigoulot%2CS&amp;author=Wassiliwizky%2CE&amp;author=Pell%2CMD" data-track=click data-track-action="google scholar reference" data-track-label=link>
 Google Scholar</a>&nbsp;</p><li class=c-reading-companion__reference-item><p class="c-reading-companion__reference-citation u-font-family-serif" id=rc-ref-CR78>Sauter DA, Eisner F, Calder AJ et al. (2010) Perceptual cues in nonverbal vocal expressions of emotion. Q J Exp Psychol 63:2251–2272. <a href=https://doi.org/10.1080/17470211003721642>https://doi.org/10.1080/17470211003721642</a><p class=c-reading-companion__reference-links><a href=https://doi.org/10.1080%2F17470211003721642 data-track=click data-track-action="article reference" data-track-label=link>Article</a>&nbsp;<a href="http://scholar.google.com/scholar_lookup?&amp;title=Perceptual%20cues%20in%20nonverbal%20vocal%20expressions%20of%20emotion&amp;journal=Q%20J%20Exp%20Psychol&amp;doi=10.1080%2F17470211003721642&amp;volume=63&amp;pages=2251-2272&amp;publication_year=2010&amp;author=Sauter%2CDA&amp;author=Eisner%2CF&amp;author=Calder%2CAJ" data-track=click data-track-action="google scholar reference" data-track-label=link>
 Google Scholar</a>&nbsp;</p><li class=c-reading-companion__reference-item><p class="c-reading-companion__reference-citation u-font-family-serif" id=rc-ref-CR79>Sauter DA (2006) An investigation into vocal expressions of emotions: the roles of valence, culture, and acoustic factors. Unpublished Ph.D. thesis, University College London</p><li class=c-reading-companion__reference-item><p class="c-reading-companion__reference-citation u-font-family-serif" id=rc-ref-CR80>Sbattella L, Colombo L, Rinaldi C et al. (2014) Extracting emotions and communication styles from prosody. In: da Silva H, Holzinger A, Fairclough S, Majoe D (eds) Physiological computing systems, vol. 8908. Springer, Heidelberg, pp. 21–42<p class=c-reading-companion__reference-links><a href="http://scholar.google.com/scholar_lookup?&amp;title=Extracting%20emotions%20and%20communication%20styles%20from%20prosody&amp;publication_year=2014&amp;author=Sbattella%2CL&amp;author=Colombo%2CL&amp;author=Rinaldi%2CC" data-track=click data-track-action="google scholar reference" data-track-label=link>
 Google Scholar</a>&nbsp;</p><li class=c-reading-companion__reference-item><p class="c-reading-companion__reference-citation u-font-family-serif" id=rc-ref-CR81>Schaerlaeken S, Grandjean D (2018) Unfolding and dynamics of affect bursts decoding in humans. PLoS ONE 13:e0206215. <a href=https://doi.org/10.1371/journal.pone.0206216>https://doi.org/10.1371/journal.pone.0206216</a><p class=c-reading-companion__reference-links><a href=https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BC1MXjt1Olsb4%3D data-track=click data-track-action="cas reference" data-track-label=link>CAS</a>&nbsp;<a href=https://doi.org/10.1371%2Fjournal.pone.0206216 data-track=click data-track-action="article reference" data-track-label=link>Article</a>&nbsp;<a href="http://scholar.google.com/scholar_lookup?&amp;title=Unfolding%20and%20dynamics%20of%20affect%20bursts%20decoding%20in%20humans&amp;journal=PLoS%20ONE&amp;doi=10.1371%2Fjournal.pone.0206216&amp;volume=13&amp;publication_year=2018&amp;author=Schaerlaeken%2CS&amp;author=Grandjean%2CD" data-track=click data-track-action="google scholar reference" data-track-label=link>
 Google Scholar</a>&nbsp;</p><li class=c-reading-companion__reference-item><p class="c-reading-companion__reference-citation u-font-family-serif" id=rc-ref-CR82>Scherer KR, Baenziger T (2004) Emotional expression in prosody: a review and an agenda for future research. In: Bel B, Marlien I (eds) Speech prosody, Nara, Japan, pp. 359–366</p><li class=c-reading-companion__reference-item><p class="c-reading-companion__reference-citation u-font-family-serif" id=rc-ref-CR83>Scherer KR, Banse R, Wallbott H (2001) Emotion inferences from vocal expression correlate across languages and cultures. J Cross Cult Psychol 32:76–92. <a href=https://doi.org/10.1177/0022022101032001009>https://doi.org/10.1177/0022022101032001009</a><p class=c-reading-companion__reference-links><a href=https://doi.org/10.1177%2F0022022101032001009 data-track=click data-track-action="article reference" data-track-label=link>Article</a>&nbsp;<a href="http://scholar.google.com/scholar_lookup?&amp;title=Emotion%20inferences%20from%20vocal%20expression%20correlate%20across%20languages%20and%20cultures&amp;journal=J%20Cross%20Cult%20Psychol&amp;doi=10.1177%2F0022022101032001009&amp;volume=32&amp;pages=76-92&amp;publication_year=2001&amp;author=Scherer%2CKR&amp;author=Banse%2CR&amp;author=Wallbott%2CH" data-track=click data-track-action="google scholar reference" data-track-label=link>
 Google Scholar</a>&nbsp;</p><li class=c-reading-companion__reference-item><p class="c-reading-companion__reference-citation u-font-family-serif" id=rc-ref-CR84>Scherer KR (1994) Affect bursts. In: van Goozen SHM, van de Poll NE, Sergeant JA (eds) Emotions: essays on emotion theory. Erlbaum, Hillsdale, pp. 161–193<p class=c-reading-companion__reference-links><a href="http://scholar.google.com/scholar_lookup?&amp;title=Affect%20bursts&amp;pages=161-193&amp;publication_year=1994&amp;author=Scherer%2CKR" data-track=click data-track-action="google scholar reference" data-track-label=link>
 Google Scholar</a>&nbsp;</p><li class=c-reading-companion__reference-item><p class="c-reading-companion__reference-citation u-font-family-serif" id=rc-ref-CR85>Scherer KR, London H, Wolf J (1973) The voice of confidence: Paralinguistic cues and audience evaluation. J Res Pers 7:31–44. <a href=https://doi.org/10.1016/0092-6566(73)90030-5>https://doi.org/10.1016/0092-6566(73)90030-5</a><p class=c-reading-companion__reference-links><a href=https://doi.org/10.1016%2F0092-6566%2873%2990030-5 data-track=click data-track-action="article reference" data-track-label=link>Article</a>&nbsp;<a href="http://scholar.google.com/scholar_lookup?&amp;title=The%20voice%20of%20confidence%3A%20Paralinguistic%20cues%20and%20audience%20evaluation&amp;journal=J%20Res%20Pers&amp;doi=10.1016%2F0092-6566%2873%2990030-5&amp;volume=7&amp;pages=31-44&amp;publication_year=1973&amp;author=Scherer%2CKR&amp;author=London%2CH&amp;author=Wolf%2CJ" data-track=click data-track-action="google scholar reference" data-track-label=link>
 Google Scholar</a>&nbsp;</p><li class=c-reading-companion__reference-item><p class="c-reading-companion__reference-citation u-font-family-serif" id=rc-ref-CR86>Schirmer A (2010) Mark my words: tone of voice changes affective word representations in memory. PLoS ONE 5(2):e9080. <a href=https://doi.org/10.1371/journal.pone.0009080>https://doi.org/10.1371/journal.pone.0009080</a><p class=c-reading-companion__reference-links><a href="http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT&amp;bibcode=2010PLoSO...5.9080S" data-track=click data-track-action="ads reference" data-track-label=link>ADS</a>&nbsp;<a href=https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BC3cXitFGns7g%3D data-track=click data-track-action="cas reference" data-track-label=link>CAS</a>&nbsp;<a href=https://doi.org/10.1371%2Fjournal.pone.0009080 data-track=click data-track-action="article reference" data-track-label=link>Article</a>&nbsp;<a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=20169154" data-track=click data-track-action="pubmed reference" data-track-label=link>PubMed</a>&nbsp;<a href=http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2821399 data-track=click data-track-action="pubmed central reference" data-track-label=link>PubMed Central</a>&nbsp;<a href="http://scholar.google.com/scholar_lookup?&amp;title=Mark%20my%20words%3A%20tone%20of%20voice%20changes%20affective%20word%20representations%20in%20memory&amp;journal=PLoS%20ONE&amp;doi=10.1371%2Fjournal.pone.0009080&amp;volume=5&amp;issue=2&amp;publication_year=2010&amp;author=Schirmer%2CA" data-track=click data-track-action="google scholar reference" data-track-label=link>
 Google Scholar</a>&nbsp;</p><li class=c-reading-companion__reference-item><p class="c-reading-companion__reference-citation u-font-family-serif" id=rc-ref-CR87>Schirmer A, Kotz SA (2003) ERP evidence for a sex-specific Stroop effect in emotional speech. J Cogn Neurosci 15:1135–1148. <a href=https://doi.org/10.1162/089892903322598102>https://doi.org/10.1162/089892903322598102</a><p class=c-reading-companion__reference-links><a href=https://doi.org/10.1162%2F089892903322598102 data-track=click data-track-action="article reference" data-track-label=link>Article</a>&nbsp;<a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=14709232" data-track=click data-track-action="pubmed reference" data-track-label=link>PubMed</a>&nbsp;<a href="http://scholar.google.com/scholar_lookup?&amp;title=ERP%20evidence%20for%20a%20sex-specific%20Stroop%20effect%20in%20emotional%20speech&amp;journal=J%20Cogn%20Neurosci&amp;doi=10.1162%2F089892903322598102&amp;volume=15&amp;pages=1135-1148&amp;publication_year=2003&amp;author=Schirmer%2CA&amp;author=Kotz%2CSA" data-track=click data-track-action="google scholar reference" data-track-label=link>
 Google Scholar</a>&nbsp;</p><li class=c-reading-companion__reference-item><p class="c-reading-companion__reference-citation u-font-family-serif" id=rc-ref-CR88>Scott SK, Sauter D, McGettigan C (2010) Brain mechanisms for processing perceived emotional vocalizations in humans. In: Brudzynski SM (ed), Handbook of behavioral neuroscience, Elsevier, pp. 187–197</p><li class=c-reading-companion__reference-item><p class="c-reading-companion__reference-citation u-font-family-serif" id=rc-ref-CR89>Seber GAF (1984) Multivariate observations. John Wiley &amp; Sons</p><li class=c-reading-companion__reference-item><p class="c-reading-companion__reference-citation u-font-family-serif" id=rc-ref-CR91>Thompson WF, Balkwill LL (2009) Cross-cultural similarities and differences. In: Juslin PN, Sloboda JA (eds) Handbook of music and emotion: theory, research, applications, 1st edn. Oxford University Press, New York, pp. 755–791. <a href=https://doi.org/10.1093/acprof:oso/9780199230143.003.0027>https://doi.org/10.1093/acprof:oso/9780199230143.003.0027</a></p><li class=c-reading-companion__reference-item><p class="c-reading-companion__reference-citation u-font-family-serif" id=rc-ref-CR92>Toivanen J, Väyrynen E, Sepännen T (2004) Automatic discrimination of emotion from spoken Finnish. Lang Speech 47:383–412. <a href=https://doi.org/10.1177/00238309040470040301>https://doi.org/10.1177/00238309040470040301</a><p class=c-reading-companion__reference-links><a href=https://doi.org/10.1177%2F00238309040470040301 data-track=click data-track-action="article reference" data-track-label=link>Article</a>&nbsp;<a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=16038449" data-track=click data-track-action="pubmed reference" data-track-label=link>PubMed</a>&nbsp;<a href="http://scholar.google.com/scholar_lookup?&amp;title=Automatic%20discrimination%20of%20emotion%20from%20spoken%20Finnish&amp;journal=Lang%20Speech&amp;doi=10.1177%2F00238309040470040301&amp;volume=47&amp;pages=383-412&amp;publication_year=2004&amp;author=Toivanen%2CJ&amp;author=V%C3%A4yrynen%2CE&amp;author=Sep%C3%A4nnen%2CT" data-track=click data-track-action="google scholar reference" data-track-label=link>
 Google Scholar</a>&nbsp;</p><li class=c-reading-companion__reference-item><p class="c-reading-companion__reference-citation u-font-family-serif" id=rc-ref-CR93>Wagner HL (1993) On measuring performance in category judgement studies of nonverbal behaviour. J Nonverbal Behav 17:3–28. <a href=https://doi.org/10.1007/BF00987006>https://doi.org/10.1007/BF00987006</a><p class=c-reading-companion__reference-links><a href=https://doi.org/10.1007%2FBF00987006 data-track=click data-track-action="article reference" data-track-label=link>Article</a>&nbsp;<a href="http://scholar.google.com/scholar_lookup?&amp;title=On%20measuring%20performance%20in%20category%20judgement%20studies%20of%20nonverbal%20behaviour&amp;journal=J%20Nonverbal%20Behav&amp;doi=10.1007%2FBF00987006&amp;volume=17&amp;pages=3-28&amp;publication_year=1993&amp;author=Wagner%2CHL" data-track=click data-track-action="google scholar reference" data-track-label=link>
 Google Scholar</a>&nbsp;</p><li class=c-reading-companion__reference-item><p class="c-reading-companion__reference-citation u-font-family-serif" id=rc-ref-CR94>Wendt B, Scheich H (2002) The “Magdeburger Prosodie Korpus”—a spoken language corpus for fMRI-Studies. In: Bel B, Marlien I (eds) Speech prosody. Aix-en-Provence, SproSIG, pp. 699–701</p><li class=c-reading-companion__reference-item><p class="c-reading-companion__reference-citation u-font-family-serif" id=rc-ref-CR95>Wilson TD, Gilbert DT (2008) Explaining away: a model of affective adaptation. Perspect Psychol Sci 3:370–386. <a href=https://doi.org/10.1111/j.1745-6924.2008.00085.x>https://doi.org/10.1111/j.1745-6924.2008.00085.x</a><p class=c-reading-companion__reference-links><a href=https://doi.org/10.1111%2Fj.1745-6924.2008.00085.x data-track=click data-track-action="article reference" data-track-label=link>Article</a>&nbsp;<a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=26158955" data-track=click data-track-action="pubmed reference" data-track-label=link>PubMed</a>&nbsp;<a href="http://scholar.google.com/scholar_lookup?&amp;title=Explaining%20away%3A%20a%20model%20of%20affective%20adaptation&amp;journal=Perspect%20Psychol%20Sci&amp;doi=10.1111%2Fj.1745-6924.2008.00085.x&amp;volume=3&amp;pages=370-386&amp;publication_year=2008&amp;author=Wilson%2CTD&amp;author=Gilbert%2CDT" data-track=click data-track-action="google scholar reference" data-track-label=link>
 Google Scholar</a>&nbsp;</p><li class=c-reading-companion__reference-item><p class="c-reading-companion__reference-citation u-font-family-serif" id=rc-ref-CR96>World Medical Association (2013) World Medical Association Declaration of Helsinki: ethical principles form medical research involving human subjects. JAMA 310:2191–2194. <a href=https://doi.org/10.1001/jama.2013.281053>https://doi.org/10.1001/jama.2013.281053</a><p class=c-reading-companion__reference-links><a href=https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BC2cXmtVOmsA%3D%3D data-track=click data-track-action="cas reference" data-track-label=link>CAS</a>&nbsp;<a href=https://doi.org/10.1001%2Fjama.2013.281053 data-track=click data-track-action="article reference" data-track-label=link>Article</a>&nbsp;<a href="http://scholar.google.com/scholar_lookup?&amp;title=World%20Medical%20Association%20Declaration%20of%20Helsinki%3A%20ethical%20principles%20form%20medical%20research%20involving%20human%20subjects&amp;journal=JAMA&amp;doi=10.1001%2Fjama.2013.281053&amp;volume=310&amp;pages=2191-2194&amp;publication_year=2013" data-track=click data-track-action="google scholar reference" data-track-label=link>
 Google Scholar</a>&nbsp;</p></ol></div></div>
 </div>
 </div>
</aside>
</div>
 
 
 
 
 
 
 
 
 
 
 
<footer class=composite-layer itemscope itemtype=http://schema.org/Periodical>
 
 
 <div class="u-mt-16 u-mb-16">
 <div class=u-container>
 <div class="u-display-flex u-flex-wrap u-justify-content-space-between">
 
 <p class="c-meta u-ma-0 u-flex-shrink">
 <span class=c-meta__item>
 Humanities and Social Sciences Communications (<i>Humanit Soc Sci Commun</i>)
 </span>
 
 
 <span class=c-meta__item>
 <abbr title="International Standard Serial Number">ISSN</abbr> <span itemprop=onlineIssn>2662-9992</span> (online)
 </span>
 
 
 
 </p>
 </div>
 </div>
</div>
 <div>
 <div class=c-footer>
 <div class=u-container>
 <div class=u-hide-print data-track-component=footer>
 <h2 class=u-visually-hidden>nature.com sitemap</h2>
 <div class=c-footer__header>
 <div class=c-footer__logo>
 <img alt="Nature portfolio" src="data:image/svg+xml;base64,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" loading=lazy width=200 height=31>
 </div>
 <ul class="c-menu c-menu--inherit u-mr-32">
 <li class=c-menu__item><a class=c-menu__link href=https://www.nature.com/npg_/company_info/index.html data-track=click data-track-action="about us" data-track-label=link>About us</a></li>
 <li class=c-menu__item><a class=c-menu__link href=https://www.nature.com/npg_/press_room/press_releases.html data-track=click data-track-action="press releases" data-track-label=link>Press releases</a></li>
 <li class=c-menu__item><a class=c-menu__link href=https://press.nature.com/ data-track=click data-track-action="press office" data-track-label=link>Press office</a></li>
 <li class=c-menu__item><a class=c-menu__link href=https://support.nature.com/support/home data-track=click data-track-action="contact us" data-track-label=link>Contact us</a></li>
 </ul>
 <ul class="c-menu c-menu--inherit">
 <li class=c-menu__item>
 <a class=c-menu__link href=https://www.facebook.com/NaturePortfolioJournals/ aria-label=Facebook data-track=click data-track-action=facebook data-track-label=link>
 <svg class=u-icon role=img aria-hidden=true focusable=false xmlns=http://www.w3.org/2000/svg width=16 height=16 viewBox="0 0 20 20"><path d="M2.5 20C1.1 20 0 18.9 0 17.5v-15C0 1.1 1.1 0 2.5 0h15C18.9 0 20 1.1 20 2.5v15c0 1.4-1.1 2.5-2.5 2.5h-3.7v-7.7h2.6l.4-3h-3v-2c0-.9.2-1.5 1.5-1.5h1.6V3.1c-.3 0-1.2-.1-2.3-.1-2.3 0-3.9 1.4-3.9 4v2.2H8.1v3h2.6V20H2.5z"></path></svg>
 </a>
 </li>
 <li class=c-menu__item>
 <a class=c-menu__link href="https://twitter.com/NaturePortfolio?lang=en" aria-label=Twitter data-track=click data-track-action=twitter data-track-label=link>
 <svg class=u-icon role=img aria-hidden=true focusable=false xmlns=http://www.w3.org/2000/svg width=20 height=20 viewBox="0 0 20 20"><path d="M17.6 4.1c.8-.5 1.5-1.4 1.8-2.4-.8.5-1.7.9-2.6 1-.7-.8-1.8-1.4-3-1.4-2.3 0-4.1 1.9-4.1 4.3 0 .3 0 .7.1 1-3.4 0-6.4-1.8-8.4-4.4C1 2.9.8 3.6.8 4.4c0 1.5.7 2.8 1.8 3.6C2 8 1.4 7.8.8 7.5v.1c0 2.1 1.4 3.8 3.3 4.2-.3.1-.7.2-1.1.2-.3 0-.5 0-.8-.1.5 1.7 2 3 3.8 3-1.3 1.1-3.1 1.8-5 1.8-.3 0-.7 0-1-.1 1.8 1.2 4 1.9 6.3 1.9C13.8 18.6 18 12 18 6.3v-.6c.8-.6 1.5-1.4 2-2.2-.7.3-1.5.5-2.4.6z"></path></svg>
 </a>
 </li>
 <li class=c-menu__item>
 <a class=c-menu__link href=https://www.youtube.com/channel/UCvCLdSgYdSTpWcOgEJgi-ng aria-label=YouTube data-track=click data-track-action=youtube data-track-label=link>
 <svg class=u-icon role=img aria-hidden=true focusable=false xmlns=http://www.w3.org/2000/svg width=20 height=20 viewBox="0 0 20 20"><path d="M7.9 12.6V6.9l5.4 2.8c0 .1-5.4 2.9-5.4 2.9zM19.8 6s-.2-1.4-.8-2c-.8-.8-1.6-.8-2-.9-2.8-.2-7-.2-7-.2s-4.2 0-7 .2c-.4 0-1.2 0-2 .9-.6.6-.8 2-.8 2S0 7.6 0 9.2v1.5c0 1.7.2 3.3.2 3.3s.2 1.4.8 2c.8.8 1.8.8 2.2.9 1.6.1 6.8.2 6.8.2s4.2 0 7-.2c.4 0 1.2-.1 2-.9.6-.6.8-2 .8-2s.2-1.6.2-3.3V9.2c0-1.6-.2-3.2-.2-3.2z"></path></svg>
 </a>
 </li>
 </ul>
 </div>
 <div class=c-footer__grid>
 <div class=c-footer__group>
 <h3 class=c-footer__heading>Discover content</h3>
 <ul class=c-footer__list>
 <li class=c-footer__item><a href=https://www.nature.com/siteindex data-track=click data-track-action="journals a-z" data-track-label=link>Journals A-Z</a></li>
 <li class=c-footer__item><a href=https://www.nature.com/subjects/ data-track=click data-track-action="article by subject" data-track-label=link>Articles by subject</a></li>
 <li class=c-footer__item><a href=https://nano.nature.com/ data-track=click data-track-action=nano data-track-label=link>Nano</a></li>
 <li class=c-footer__item><a href=https://www.nature.com/protocolexchange/ data-track=click data-track-action="protocol exchange" data-track-label=link>Protocol Exchange</a></li>
 <li class=c-footer__item><a href=https://www.natureindex.com/ data-track=click data-track-action="nature index" data-track-label=link>Nature Index</a></li>
 </ul>
 </div>
 <div class=c-footer__group>
 <h3 class=c-footer__heading>Publishing policies</h3>
 <ul class=c-footer__list>
 <li class=c-footer__item><a href=https://www.nature.com/authors/editorial_policies/ data-track=click data-track-action="Nature portfolio policies" data-track-label=link>Nature portfolio policies</a></li>
 <li class=c-footer__item><a href=https://www.nature.com/nature-research/open-access data-track=click data-track-action="open access" data-track-label=link>Open access</a></li>
 </ul>
 </div>
 <div class=c-footer__group>
 <h3 class=c-footer__heading>Author &amp; Researcher services</h3>
 <ul class=c-footer__list>
 <li class=c-footer__item><a href=https://www.nature.com/reprints/ data-track=click data-track-action="reprints and permissions" data-track-label=link>Reprints &amp; permissions</a></li>
 <li class=c-footer__item><a href=https://www.springernature.com/gp/authors/research-data data-track=click data-track-action="data research service" data-track-label=link>Research data</a></li>
 <li class=c-footer__item><a href=https://authorservices.springernature.com/language-editing/ data-track=click data-track-action="language editing" data-track-label=link>Language editing</a></li>
 <li class=c-footer__item><a href=https://authorservices.springernature.com/scientific-editing/ data-track=click data-track-action="scientific editing" data-track-label=link>Scientific editing</a></li>
 <li class=c-footer__item><a href=https://masterclasses.nature.com/ data-track=click data-track-action="nature masterclasses" data-track-label=link>Nature Masterclasses</a></li>
 <li class=c-footer__item><a href=https://partnerships.nature.com/product/researcher-training/ data-track=click data-track-action="nature research academies" data-track-label=link>Nature Research Academies</a></li>
 <li class=c-footer__item><a href=https://solutions.springernature.com/ data-track=click data-track-action="research solutions" data-track-label=link>Research Solutions</a></li>
 </ul>
 </div>
 <div class=c-footer__group>
 <h3 class=c-footer__heading>Libraries &amp; institutions</h3>
 <ul class=c-footer__list>
 <li class=c-footer__item><a href=https://www.springernature.com/gp/librarians/tools-services data-track=click data-track-action="librarian service and tools" data-track-label=link>Librarian service &amp; tools</a></li>
 <li class=c-footer__item><a href=https://www.springernature.com/gp/librarians/manage-your-account/librarianportal data-track=click data-track-action="librarian portal" data-track-label=link>Librarian portal</a></li>
 <li class=c-footer__item><a href=https://www.nature.com/openresearch/about-open-access/information-for-institutions/ data-track=click data-track-action="open research" data-track-label=link>Open research</a></li>
 <li class=c-footer__item><a href=https://www.springernature.com/gp/librarians/recommend-to-your-library data-track=click data-track-action="Recommend to library" data-track-label=link>Recommend to library</a></li>
 </ul>
 </div>
 <div class=c-footer__group>
 <h3 class=c-footer__heading>Advertising &amp; partnerships</h3>
 <ul class=c-footer__list>
 <li class=c-footer__item><a href=https://partnerships.nature.com/product/digital-advertising/ data-track=click data-track-action=advertising data-track-label=link>Advertising</a></li>
 <li class=c-footer__item><a href=https://partnerships.nature.com/ data-track=click data-track-action="partnerships and services" data-track-label=link>Partnerships &amp; Services</a></li>
 <li class=c-footer__item><a href=https://partnerships.nature.com/media-kits/ data-track=click data-track-action="media kits" data-track-label=link>Media kits</a></li>
 <li class=c-footer__item><a href=https://partnerships.nature.com/product/branded-content-native-advertising/ data-track-action="branded content" data-track-label=link>Branded content</a></li>
 </ul>
 </div>
 <div class=c-footer__group>
 <h3 class=c-footer__heading>Career development</h3>
 <ul class=c-footer__list>
 <li class=c-footer__item><a href=https://www.nature.com/naturecareers data-track=click data-track-action="nature careers" data-track-label=link>Nature Careers</a></li>
 <li class=c-footer__item><a href=https://conferences.nature.com/ data-track=click data-track-action="nature conferences" data-track-label=link>Nature<span class=u-visually-hidden> </span> Conferences</a></li>
 <li class=c-footer__item><a href=https://www.nature.com/natureevents/ data-track=click data-track-action="nature events" data-track-label=link>Nature<span class=u-visually-hidden> </span> events</a></li>
 </ul>
 </div>
 <div class=c-footer__group>
 <h3 class=c-footer__heading>Regional websites</h3>
 <ul class=c-footer__list>
 <li class=c-footer__item><a href=https://www.nature.com/natafrica data-track=click data-track-action="nature africa" data-track-label=link>Nature Africa</a></li>
 <li class=c-footer__item><a href=http://www.naturechina.com/ data-track=click data-track-action="nature china" data-track-label=link>Nature China</a></li>
 <li class=c-footer__item><a href=https://www.nature.com/nindia data-track=click data-track-action="nature india" data-track-label=link>Nature India</a></li>
 <li class=c-footer__item><a href=https://www.nature.com/natitaly data-track=click data-track-action="nature Italy" data-track-label=link>Nature Italy</a></li>
 <li class=c-footer__item><a href=https://www.natureasia.com/ja-jp/ data-track=click data-track-action="nature japan" data-track-label=link>Nature Japan</a></li>
 <li class=c-footer__item><a href=https://www.natureasia.com/ko-kr/ data-track=click data-track-action="nature korea" data-track-label=link>Nature Korea</a></li>
 <li class=c-footer__item><a href=https://www.nature.com/nmiddleeast/ data-track=click data-track-action="nature middle east" data-track-label=link>Nature Middle East</a></li>
 </ul>
 </div>
 <div class=c-footer__group>
 <h3 class=c-footer__heading>Legal &amp; Privacy</h3>
 <ul class=c-footer__list>
 <li class=c-footer__item><a href=https://www.nature.com/info/privacy data-track=click data-track-action="privacy policy" data-track-label=link>Privacy Policy</a></li>
 <li class=c-footer__item><a href=https://www.nature.com/info/cookies data-track=click data-track-action="use of cookies" data-track-label=link>Use of cookies</a></li>
 <li class=c-footer__item><a class=optanon-toggle-display href=https://www.nature.com/articles/s41599-020-0499-z data-cc-action=preferences data-track=click data-track-action="manage cookies" data-track-label=link>Manage cookies/Do not sell my data</a></li>
 <li class=c-footer__item><a href=https://www.nature.com/info/legal-notice data-track=click data-track-action="legal notice" data-track-label=link>Legal notice</a></li>
 <li class=c-footer__item><a href=https://www.nature.com/info/accessibility-statement data-track=click data-track-action="accessibility statement" data-track-label=link>Accessibility statement</a></li>
 <li class=c-footer__item><a href=https://www.nature.com/info/terms-and-conditions data-track=click data-track-action="terms and conditions" data-track-label=link>Terms &amp; Conditions</a></li>
 <li class=c-footer__item><a href=https://www.springernature.com/ccpa data-track=click data-track-action="california privacy statement" data-track-label=link>California Privacy Statement</a></li>
 
 </ul>
 </div>
 </div>
</div>
 </div>
 </div>
 </div>
 <div class=c-corporate-footer>
 <div class=u-container>
 <img src="data:image/svg+xml;base64,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" alt="Springer Nature" loading=lazy width=140 height=14>
 <p class=c-corporate-footer__legal data-test=copyright>© 2022 Springer Nature Limited</p>
 </div>
</div>
 
 <svg class="u-hide hide">
 <symbol id=global-icon-chevron-right viewBox="0 0 10 10" xmlns=http://www.w3.org/2000/svg>
 <path d="m5.96738168 4.70639573 2.39518594-2.41447274c.37913917-.38219212.98637524-.38972225 1.35419292-.01894278.37750606.38054586.37784436.99719163-.00013556 1.37821513l-4.03074001 4.06319683c-.37758093.38062133-.98937525.38100976-1.367372-.00003075l-4.03091981-4.06337806c-.37759778-.38063832-.38381821-.99150444-.01600053-1.3622839.37750607-.38054587.98772445-.38240057 1.37006824.00302197l2.39538588 2.4146743.96295325.98624457z" fill=currentColor fill-rule=evenodd transform="matrix(0 -1 1 0 0 10)"></path>
 </symbol>
 <symbol id=global-icon-download viewBox="0 0 16 16">
 <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule=evenodd></path>
 </symbol>
 <symbol id=global-icon-email viewBox="0 0 18 18">
 <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule=evenodd></path>
 </symbol>
 <symbol id=global-icon-institution viewBox="0 0 18 18">
 <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule=evenodd></path>
 </symbol>
 <symbol id=global-icon-search viewBox="0 0 22 22">
 <path fill-rule=evenodd d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z"></path>
 </symbol>
 <symbol id=icon-info viewBox="0 0 18 18">
 <path d="m9 0c4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9zm0 7h-1.5l-.11662113.00672773c-.49733868.05776511-.88337887.48043643-.88337887.99327227 0 .47338693.32893365.86994729.77070917.97358929l.1126697.01968298.11662113.00672773h.5v3h-.5l-.11662113.0067277c-.42082504.0488782-.76196299.3590206-.85696816.7639815l-.01968298.1126697-.00672773.1166211.00672773.1166211c.04887817.4208251.35902055.761963.76398144.8569682l.1126697.019683.11662113.0067277h3l.1166211-.0067277c.4973387-.0577651.8833789-.4804365.8833789-.9932723 0-.4733869-.3289337-.8699473-.7707092-.9735893l-.1126697-.019683-.1166211-.0067277h-.5v-4l-.00672773-.11662113c-.04887817-.42082504-.35902055-.76196299-.76398144-.85696816l-.1126697-.01968298zm0-3.25c-.69035594 0-1.25.55964406-1.25 1.25s.55964406 1.25 1.25 1.25 1.25-.55964406 1.25-1.25-.55964406-1.25-1.25-1.25z" fill-rule=evenodd></path>
 </symbol>
 <symbol id=icon-success viewBox="0 0 18 18">
 <path d="M9 0a9 9 0 110 18A9 9 0 019 0zm3.486 4.982l-4.718 5.506L5.14 8.465a.991.991 0 00-1.423.133 1.06 1.06 0 00.13 1.463l3.407 2.733a1 1 0 001.387-.133l5.385-6.334a1.06 1.06 0 00-.116-1.464.991.991 0 00-1.424.119z" fill-rule=evenodd></path>
 </symbol>
 <symbol id=icon-chevron-down viewBox="0 0 16 16">
 <path d="m5.58578644 3-3.29289322-3.29289322c-.39052429-.39052429-.39052429-1.02368927 0-1.41421356s1.02368927-.39052429 1.41421356 0l4 4c.39052429.39052429.39052429 1.02368927 0 1.41421356l-4 4c-.39052429.39052429-1.02368927.39052429-1.41421356 0s-.39052429-1.02368927 0-1.41421356z" fill-rule=evenodd transform="matrix(0 1 -1 0 11 1)"></path>
 </symbol>
 <symbol id=icon-warning viewBox="0 0 18 18">
 <path d="m9 11.75c.69035594 0 1.25.5596441 1.25 1.25s-.55964406 1.25-1.25 1.25-1.25-.5596441-1.25-1.25.55964406-1.25 1.25-1.25zm.41320045-7.75c.55228475 0 1.00000005.44771525 1.00000005 1l-.0034543.08304548-.3333333 4c-.043191.51829212-.47645714.91695452-.99654578.91695452h-.15973424c-.52008864 0-.95335475-.3986624-.99654576-.91695452l-.33333333-4c-.04586475-.55037702.36312325-1.03372649.91350028-1.07959124l.04148683-.00259031zm-.41320045 14c-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9 4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9z" fill-rule=evenodd></path>
 </symbol>
 <symbol id=icon-plus viewBox="0 0 16 16">
 <path d="m2.00087166 7h4.99912834v-4.99912834c0-.55276616.44386482-1.00087166 1-1.00087166.55228475 0 1 .44463086 1 1.00087166v4.99912834h4.9991283c.5527662 0 1.0008717.44386482 1.0008717 1 0 .55228475-.4446309 1-1.0008717 1h-4.9991283v4.9991283c0 .5527662-.44386482 1.0008717-1 1.0008717-.55228475 0-1-.4446309-1-1.0008717v-4.9991283h-4.99912834c-.55276616 0-1.00087166-.44386482-1.00087166-1 0-.55228475.44463086-1 1.00087166-1z" fill-rule=evenodd></path>
 </symbol>
 <symbol id=icon-minus viewBox="0 0 16 16">
 <path d="m2.00087166 7h11.99825664c.5527662 0 1.0008717.44386482 1.0008717 1 0 .55228475-.4446309 1-1.0008717 1h-11.99825664c-.55276616 0-1.00087166-.44386482-1.00087166-1 0-.55228475.44463086-1 1.00087166-1z" fill-rule=evenodd></path>
 </symbol>
 <symbol id=icon-error viewBox="0 0 18 18">
 <path d="m9 0c4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9zm2.8630343 4.71100931-2.8630343 2.86303426-2.86303426-2.86303426c-.39658757-.39658757-1.03281091-.39438847-1.4265779-.00062147-.39651227.39651226-.39348876 1.03246767.00062147 1.4265779l2.86303426 2.86303426-2.86303426 2.8630343c-.39658757.3965875-.39438847 1.0328109-.00062147 1.4265779.39651226.3965122 1.03246767.3934887 1.4265779-.0006215l2.86303426-2.8630343 2.8630343 2.8630343c.3965875.3965876 1.0328109.3943885 1.4265779.0006215.3965122-.3965123.3934887-1.0324677-.0006215-1.4265779l-2.8630343-2.8630343 2.8630343-2.86303426c.3965876-.39658757.3943885-1.03281091.0006215-1.4265779-.3965123-.39651227-1.0324677-.39348876-1.4265779.00062147z" fill=currentColor fill-rule=evenodd></path>
 </symbol>
 <symbol id=icon-springer-arrow-left>
 <path d="M15 7a1 1 0 000-2H3.385l2.482-2.482a.994.994 0 00.02-1.403 1.001 1.001 0 00-1.417 0L.294 5.292a1.001 1.001 0 000 1.416l4.176 4.177a.991.991 0 001.4.016 1 1 0 00-.003-1.42L3.385 7H15z"></path>
 </symbol>
 <symbol id=icon-springer-arrow-right>
 <path d="M1 7a1 1 0 010-2h11.615l-2.482-2.482a.994.994 0 01-.02-1.403 1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L12.615 7H1z"></path>
 </symbol>
 <symbol id=icon-arrow-up viewBox="0 0 16 16">
 <path d="m12.716625 4.46975946-4.03074003-4.17620792c-.37758093-.39120768-.98937525-.39160691-1.367372.0000316l-4.03091981 4.1763942c-.37759778.39122514-.38381821 1.01908149-.01600053 1.40017357.37750607.39113012.98772445.3930364 1.37006824-.00310603l2.39538588-2.48183446v11.61478958l.00649339.1166211c.055753.4973387.46370161.8833789.95867408.8833789.49497246 0 .90292107-.3860402.95867408-.8833789l.00649338-.1166211v-11.61478958l2.39518592 2.4816273c.3791392.39282216.9863753.40056173 1.3541929.01946965.3775061-.39113012.3778444-1.02492687-.0001355-1.41654791z" fill-rule=evenodd></path>
 </symbol>
 <symbol id=icon-tick viewBox="0 0 24 24">
 <path d="M12,24 C5.372583,24 0,18.627417 0,12 C0,5.372583 5.372583,0 12,0 C18.627417,0 24,5.372583 24,12 C24,18.627417 18.627417,24 12,24 Z M7.657,10.79 C7.45285634,10.6137568 7.18569967,10.5283283 6.91717333,10.5534259 C6.648647,10.5785236 6.40194824,10.7119794 6.234,10.923 C5.87705269,11.3666969 5.93445559,12.0131419 6.364,12.387 L10.261,15.754 C10.6765468,16.112859 11.3037113,16.0695601 11.666,15.657 L17.759,8.713 C18.120307,8.27302248 18.0695334,7.62621189 17.644,7.248 C17.4414817,7.06995024 17.1751516,6.9821166 16.9064461,7.00476032 C16.6377406,7.02740404 16.3898655,7.15856958 16.22,7.368 L10.768,13.489 L7.657,10.79 Z"></path>
 </symbol>
 <symbol id=icon-expand-image viewBox="0 0 18 18">
 <path d="m7.49754099 11.9178212c.38955542-.3895554.38761957-1.0207846-.00290473-1.4113089-.39324695-.3932469-1.02238878-.3918247-1.41130883-.0029047l-4.10273549 4.1027355.00055454-3.5103985c.00008852-.5603185-.44832171-1.006032-1.00155062-1.0059446-.53903074.0000852-.97857527.4487442-.97866268 1.0021075l-.00093318 5.9072465c-.00008751.553948.44841131 1.001882 1.00174994 1.0017946l5.906983-.0009331c.5539233-.0000875 1.00197907-.4486389 1.00206646-1.0018679.00008515-.5390307-.45026621-.9784332-1.00588841-.9783454l-3.51010549.0005545zm3.00571741-5.83449376c-.3895554.38955541-.3876196 1.02078454.0029047 1.41130883.393247.39324696 1.0223888.39182478 1.4113089.00290473l4.1027355-4.10273549-.0005546 3.5103985c-.0000885.56031852.4483217 1.006032 1.0015506 1.00594461.5390308-.00008516.9785753-.44874418.9786627-1.00210749l.0009332-5.9072465c.0000875-.553948-.4484113-1.00188204-1.0017499-1.00179463l-5.906983.00093313c-.5539233.00008751-1.0019791.44863892-1.0020665 1.00186784-.0000852.53903074.4502662.97843325 1.0058884.97834547l3.5101055-.00055449z" fill=currentColor fill-rule=evenodd></path>
 </symbol>
 </svg>
</footer>
 
 <noscript>
        <img hidden src="https://verify.nature.com/verify/nature.png" width="0" height="0" style="display: none" alt="">
    </noscript>
 
<img src="data:*/*;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR4nGP6zwAAAgcBApocMXEAAAAASUVORK5CYII=" width=1 height=1 alt class=u-visually-hidden>
<noscript><img height="1" width="1" style="display:none" src="https://www.facebook.com/tr?id=1229240860577415&amp;ev=PageView&amp;noscript=1"></noscript>
<img src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" height=1 width=1 style=display:none><img src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" height=1 width=1 style=display:none><img src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" height=1 width=1 style=display:none><img src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" height=1 width=1 style=display:none><img src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" height=1 width=1 style=display:none><img src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" height=1 width=1 style=display:none><div style=width:0px;height:0px;display:none;visibility:hidden id=batBeacon929602619706></div><div id=criteo-tags-div style=display:none></div>
<div id=ZN_0CcSRG9GrqrxATc></div>
<img src=data:null;base64,><img src=data:null;base64,><div id=___tcf___container___ style=position:absolute;width:100%;line-height:1;top:0px;left:0px;height:100vh;overflow:visible;pointer-events:none;max-height:43842.5px></div>